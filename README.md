# CS224n (NLP) -- Spring 2019
Deep Learning Based NLP

https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/

## Syllabus
https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/index.html#schedule

## Assignments
https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/assignments/a1.pdf
https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/assignments/a2.pdf
https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/assignments/a3.pdf
https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/assignments/a4.pdf
https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/assignments/a5.pdf

## Project: Question-Answering Using SQuAD 2.0 Dataset
https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/project/default-final-project-handout.pdf <br />

Experimented with:
1. Bi-directional Attention Flow (BiDAF) with and without character level embeddings (in addition to word level embeddings)
2. BiDAF with soft output labelling for lower variance
3. Question-Answer Network (QANet)
4. QANet with Stochastic Dropout Layer inside the contextual embeddings layer
5. Bi-directional Embedding Representation for Transformers (BERT) based architecture for Question Answering tasks
