{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"A4_Tensorflow.ipynb","provenance":[],"collapsed_sections":["7F5bnMcFMjhW","TNM-nXNJ09ZN","hPKkWeQV9uax"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"us85j4BFR9vx","colab_type":"text"},"source":["#CS224N Assignment 4 (using TF 2.0)"]},{"cell_type":"markdown","metadata":{"id":"njl33jpji8SE","colab_type":"text"},"source":["Good tutorials on TF: https://riptutorial.com/tensorflow/example/29069/how-to-use-tf-gather-nd\n","\n","colab timeout:\n","\n","https://stackoverflow.com/questions/57113226/how-to-prevent-google-colab-from-disconnecting/57114793\n","\n","https://www.reddit.com/r/datascience/comments/bkrzah/google_colab_how_to_avoid_timeoutdisconnect_issues/"]},{"cell_type":"code","metadata":{"id":"3k6TXPUdSMwa","colab_type":"code","colab":{}},"source":["# !pip install tensorflow-gpu #no need to do this anymore as it's available by default in colab"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CMT9P4W3ao5o","colab_type":"code","outputId":"2b3995bf-5b0f-470d-bf60-6ba47ae5487f","executionInfo":{"status":"ok","timestamp":1576438489460,"user_tz":480,"elapsed":3988,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","print(tf.__version__) #use TF vesion 2.0"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n","2.0.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ywntWaZNI1d7","colab_type":"text"},"source":["## 1. UTILITY FUNCTIONS"]},{"cell_type":"code","metadata":{"id":"Dr_WZ7olRx4R","colab_type":"code","colab":{}},"source":["#utils.py\n","\n","\"\"\"\n","CS224N 2018-19: Homework 4\n","nmt.py: NMT Model\n","Pencheng Yin <pcyin@cs.cmu.edu>\n","Sahil Chopra <schopra8@stanford.edu>\n","Implemented in TF 2.0 by Amit Patel\n","\"\"\"\n","\n","import math\n","from typing import List\n","\n","import numpy as np\n","import tensorflow as tf\n","\n","\n","\n","def pad_sents(sents, pad_token):\n","    \"\"\" Pad list of sentences according to the longest sentence in the batch.\n","    @param sents (list[list[str]]): list of sentences, where each sentence\n","                                    is represented as a list of words\n","    @param pad_token (str): padding token\n","    @returns sents_padded (list[list[str]]): list of sentences where sentences shorter\n","        than the max length sentence are padded out with the pad_token, such that\n","        each sentences in the batch now has equal length.\n","    \"\"\"\n","    sents_padded = []\n","\n","    ### YOUR CODE HERE (~6 Lines)\n","    max_len = 0\n","    for s in sents:\n","        max_len = max(len(s), max_len)\n","        \n","    for s in sents:\n","        temp = s[:] #deep copy\n","        n = len(temp)\n","        for i in range(n,max_len,1):\n","            temp.append(pad_token)\n","        sents_padded.append(temp)\n","    ### END YOUR CODE\n","\n","    return sents_padded\n","\n","\n","\n","def read_corpus(file_path, source):\n","    \"\"\" Read file, where each sentence is dilineated by a `\\n`.\n","    @param file_path (str): path to file containing corpus\n","    @param source (str): \"tgt\" or \"src\" indicating whether text\n","        is of the source language or target language\n","    \"\"\"\n","    data = []\n","    for line in open(file_path):\n","        sent = line.strip().split(' ')\n","        # only append <s> and </s> to the target sentence\n","        if source == 'tgt':\n","            sent = ['<s>'] + sent + ['</s>']\n","        data.append(sent)\n","\n","    return data\n","\n","\n","def batch_iter(data, batch_size, shuffle=False):\n","    \"\"\" Yield batches of source and target sentences reverse sorted by source length (largest to smallest).\n","    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n","    @param batch_size (int): batch size\n","    @param shuffle (boolean): whether to randomly shuffle the dataset\n","    \"\"\"\n","    batch_num = math.ceil(len(data) / batch_size)\n","    index_array = list(range(len(data)))\n","\n","    if shuffle:\n","        np.random.shuffle(index_array)\n","\n","    for i in range(batch_num):\n","        indices = index_array[i * batch_size: (i + 1) * batch_size]\n","        examples = [data[idx] for idx in indices]\n","\n","        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n","        src_sents = [e[0] for e in examples]\n","        tgt_sents = [e[1] for e in examples]\n","\n","        yield src_sents, tgt_sents\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LewIYM88Ij7l","colab_type":"text"},"source":["## 2. VOCABULARY\n","Will generate the word2id and id2word dictionaries for source and target languages."]},{"cell_type":"code","metadata":{"id":"NgnkrDr9dcHt","colab_type":"code","colab":{}},"source":["#vocab.py\n","\"\"\"\n","CS224N 2018-19: Homework 4\n","vocab.py: Vocabulary Generation\n","Pencheng Yin <pcyin@cs.cmu.edu>\n","Sahil Chopra <schopra8@stanford.edu>\n","Amit Patel: Changed to TF 2.0\n","\n","Usage:\n","    vocab.py --train-src=<file> --train-tgt=<file> [options] VOCAB_FILE\n","\n","Options:\n","    -h --help                  Show this screen.\n","    --train-src=<file>         File of training source sentences\n","    --train-tgt=<file>         File of training target sentences\n","    --size=<int>               vocab size [default: 50000]\n","    --freq-cutoff=<int>        frequency cutoff [default: 2]\n","\"\"\"\n","\n","from collections import Counter\n","from docopt import docopt\n","from itertools import chain\n","import json\n","from typing import List\n","\n","\n","class VocabEntry(object):\n","    \"\"\" Vocabulary Entry, i.e. structure containing either\n","    src or tgt language terms.\n","    \"\"\"\n","    def __init__(self, word2id=None):\n","        \"\"\" Init VocabEntry Instance.\n","        @param word2id (dict): dictionary mapping words 2 indices\n","        \"\"\"\n","        if word2id:\n","            self.word2id = word2id\n","        else:\n","            self.word2id = dict()\n","            self.word2id['<pad>'] = 0   # Pad Token\n","            self.word2id['<s>'] = 1 # Start Token\n","            self.word2id['</s>'] = 2    # End Token\n","            self.word2id['<unk>'] = 3   # Unknown Token\n","        self.unk_id = self.word2id['<unk>']\n","        self.id2word = {v: k for k, v in self.word2id.items()}\n","\n","    def __getitem__(self, word):\n","        \"\"\" Retrieve word's index. Return the index for the unk\n","        token if the word is out of vocabulary.\n","        @param word (str): word to look up.\n","        @returns index (int): index of word \n","        \"\"\"\n","        return self.word2id.get(word, self.unk_id)\n","\n","    def __contains__(self, word):\n","        \"\"\" Check if word is captured by VocabEntry.\n","        @param word (str): word to look up\n","        @returns contains (bool): whether word is contained    \n","        \"\"\"\n","        return word in self.word2id\n","\n","    def __setitem__(self, key, value):\n","        \"\"\" Raise error, if one tries to edit the VocabEntry.\n","        \"\"\"\n","        raise ValueError('vocabulary is readonly')\n","\n","    def __len__(self):\n","        \"\"\" Compute number of words in VocabEntry.\n","        @returns len (int): number of words in VocabEntry\n","        \"\"\"\n","        return len(self.word2id)\n","\n","    def __repr__(self):\n","        \"\"\" Representation of VocabEntry to be used\n","        when printing the object.\n","        \"\"\"\n","        return 'Vocabulary[size=%d]' % len(self)\n","\n","    def id2word(self, wid):\n","        \"\"\" Return mapping of index to word.\n","        @param wid (int): word index\n","        @returns word (str): word corresponding to index\n","        \"\"\"\n","        return self.id2word[wid]\n","\n","    def add(self, word):\n","        \"\"\" Add word to VocabEntry, if it is previously unseen.\n","        @param word (str): word to add to VocabEntry\n","        @return index (int): index that the word has been assigned\n","        \"\"\"\n","        if word not in self:\n","            wid = self.word2id[word] = len(self)\n","            self.id2word[wid] = word\n","            return wid\n","        else:\n","            return self[word]\n","\n","    def words2indices(self, sents):\n","        \"\"\" Convert list of words or list of sentences of words\n","        into list or list of list of indices.\n","        @param sents (list[str] or list[list[str]]): sentence(s) in words\n","        @return word_ids (list[int] or list[list[int]]): sentence(s) in indices\n","        \"\"\"\n","        if type(sents[0]) == list:\n","            return [[self[w] for w in s] for s in sents]\n","        else:\n","            return [self[w] for w in sents]\n","\n","    def indices2words(self, word_ids):\n","        \"\"\" Convert list of indices into words.\n","        @param word_ids (list[int]): list of word ids\n","        @return sents (list[str]): list of words\n","        \"\"\"\n","        return [self.id2word[w_id] for w_id in word_ids]\n","\n","    def to_input_tensor(self, sents, device):\n","        \"\"\" Convert list of sentences (words) into tensor with necessary padding for \n","        shorter sentences.\n","\n","        @param sents (List[List[str]]): list of sentences (words)\n","        @param device: device on which to load the tesnor, i.e. CPU or GPU\n","\n","        @returns sents_var: tensor of (batch_size, max_sentence_length)\n","        \"\"\"\n","        word_ids = self.words2indices(sents)\n","        sents_t = pad_sents(word_ids, self.word2id['<pad>'])\n","\n","        with tf.device(device):\n","            sents_var = tf.constant(sents_t, dtype=tf.int64) #there's a bug in TF2.0 RC0 that prevents tf.int32 tensor to be placed on GPU (but tf.int16/64 or tf.float32 is fine)\n","        #print(sents_var.device)\n","        \n","        #sents_var = tf.linalg.matrix_transpose(sents_var) #Don't do this for TF as we want batch to be the first dimension (unlike Pytorch)\n","        return sents_var \n","\n","    @staticmethod\n","    def from_corpus(corpus, size, freq_cutoff=2):\n","        \"\"\" Given a corpus construct a Vocab Entry.\n","        @param corpus (list[str]): corpus of text produced by read_corpus function\n","        @param size (int): # of words in vocabulary\n","        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word\n","        @returns vocab_entry (VocabEntry): VocabEntry instance produced from provided corpus\n","        \"\"\"\n","        vocab_entry = VocabEntry()\n","        word_freq = Counter(chain(*corpus))\n","        valid_words = [w for w, v in word_freq.items() if v >= freq_cutoff]\n","        print('number of word types: {}, number of word types w/ frequency >= {}: {}'\n","              .format(len(word_freq), freq_cutoff, len(valid_words)))\n","        top_k_words = sorted(valid_words, key=lambda w: word_freq[w], reverse=True)[:size]\n","        for word in top_k_words:\n","            vocab_entry.add(word)\n","        return vocab_entry\n","\n","\n","class Vocab(object):\n","    \"\"\" Vocab encapsulating src and target langauges.\n","    \"\"\"\n","    def __init__(self, src_vocab, tgt_vocab):\n","        \"\"\" Init Vocab.\n","        @param src_vocab (VocabEntry): VocabEntry for source language\n","        @param tgt_vocab (VocabEntry): VocabEntry for target language\n","        \"\"\"\n","        self.src = src_vocab\n","        self.tgt = tgt_vocab\n","\n","    @staticmethod\n","    def build(src_sents, tgt_sents, vocab_size, freq_cutoff) -> 'Vocab':\n","        \"\"\" Build Vocabulary.\n","        @param src_sents (list[str]): Source sentences provided by read_corpus() function\n","        @param tgt_sents (list[str]): Target sentences provided by read_corpus() function (will have beginning and end of sentence tokens)\n","        @param vocab_size (int): Size of vocabulary for both source and target languages\n","        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word.\n","        \"\"\"\n","        assert len(src_sents) == len(tgt_sents)\n","\n","        print('initialize source vocabulary ..')\n","        src = VocabEntry.from_corpus(src_sents, vocab_size, freq_cutoff)\n","\n","        print('initialize target vocabulary ..')\n","        tgt = VocabEntry.from_corpus(tgt_sents, vocab_size, freq_cutoff)\n","\n","        return Vocab(src, tgt)\n","\n","    def save(self, file_path):\n","        \"\"\" Save Vocab to file as JSON dump.\n","        @param file_path (str): file path to vocab file\n","        \"\"\"\n","        json.dump(dict(src_word2id=self.src.word2id, tgt_word2id=self.tgt.word2id), open(file_path, 'w'), indent=2)\n","\n","    @staticmethod\n","    def load(file_path):\n","        \"\"\" Load vocabulary from JSON dump.\n","        @param file_path (str): file path to vocab file\n","        @returns Vocab object loaded from JSON dump\n","        \"\"\"\n","        entry = json.load(open(file_path, 'r'))\n","        src_word2id = entry['src_word2id']\n","        tgt_word2id = entry['tgt_word2id']\n","\n","        return Vocab(VocabEntry(src_word2id), VocabEntry(tgt_word2id))\n","\n","    def __repr__(self):\n","        \"\"\" Representation of Vocab to be used\n","        when printing the object.\n","        \"\"\"\n","        return 'Vocab(source %d words, target %d words)' % (len(self.src), len(self.tgt))\n","\n","\n","\n","def main_vocabulory(train_src, train_tgt, size=50000, freq_cutoff=2, VOCAB_FILE=None, create=False, readin=True):\n","    ''' \n","    Main function for managing the vocabulory (i.e. create or readin):\n","    @param train-src=<file>: File of training source sentences\n","    @param train-tgt=<file>: File of training target sentences\n","    @param size=<int>: vocab size [default: 50000]\n","    @param freq-cutoff=<int>: frequency cutoff [default: 2]\n","    @param VOCAB_FILE=<file>: File to store the vocabulory\n","    @param create=<True/False>: A flag to create the vocabulory\n","    @param readin=<True/False>: A flag to readin the already created vocabulory file\n","    '''\n","    \n","    if create == True:\n","        print('read in source sentences: %s' % train_src)\n","        print('read in target sentences: %s' % train_tgt)\n","\n","        src_sents = read_corpus(train_src, source='src')\n","        tgt_sents = read_corpus(train_tgt, source='tgt') #(will have beginning and end of sentence tokens)\n","        print('\\nExample of source and target sentences: \\nSource: {} \\nTarget: {}\\n'.format(src_sents[0], tgt_sents[0]))\n","\n","        vocab = Vocab.build(src_sents, tgt_sents, int(size), int(freq_cutoff))\n","        print('generated vocabulary, source %d words, target %d words' % (len(vocab.src), len(vocab.tgt)))\n","\n","        vocab.save(VOCAB_FILE)\n","        print('vocabulary saved to %s' % VOCAB_FILE)\n","    \n","    if readin == True:\n","        return Vocab.load(VOCAB_FILE) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_wOLoDhjJsqw","colab_type":"code","outputId":"4fce981a-b776-46aa-e780-2a828d1badd4","executionInfo":{"status":"ok","timestamp":1576438533464,"user_tz":480,"elapsed":33289,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"}},"colab":{"base_uri":"https://localhost:8080/","height":255}},"source":["#Create/readin the Vocabulary file\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","import os\n","os.chdir('/content/gdrive/My Drive/Colab Notebooks/Deep_Learning/CS224N_2019/a4')\n","!pwd\n","!ls\n","print()\n","train_src = './a4/en_es_data/train.es'\n","train_tgt = './a4/en_es_data/train.en'\n","VOCAB_FILE = './VOCAB_FILE.json'\n","VOCABULARY = None #global variable to store the vocabulary\n","VOCABULARY = main_vocabulory(train_src, train_tgt, VOCAB_FILE=VOCAB_FILE, create=False, readin=True)\n","print(VOCABULARY)\n","#VOCABULARY.src.id2word[1] #/tgt.word2id/id2word #for the source vocab dictionary\n","# print(VOCABULARY.src.id2word(1))\n","!ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n","/content/gdrive/My Drive/Colab Notebooks/Deep_Learning/CS224N_2019/a4\n","a4\tA4_Tensorflow.ipynb\t    __MACOSX   __pycache__  VOCAB_FILE.json\n","a4.pdf\tKeras_RNN_Playground.ipynb  model.png  saved_model\n","\n","Vocab(source 50004 words, target 50002 words)\n","a4\tA4_Tensorflow.ipynb\t    __MACOSX   __pycache__  VOCAB_FILE.json\n","a4.pdf\tKeras_RNN_Playground.ipynb  model.png  saved_model\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7F5bnMcFMjhW","colab_type":"text"},"source":["##VOCAB Playground"]},{"cell_type":"code","metadata":{"id":"O4kYIHETCOJV","colab_type":"code","outputId":"9e6bea35-64e2-4e93-b7c2-d6d4e995cc09","executionInfo":{"status":"ok","timestamp":1573535237027,"user_tz":480,"elapsed":6467,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"}},"colab":{"base_uri":"https://localhost:8080/","height":728}},"source":["#Vocabulary playground\n","print(VOCABULARY)\n","print(len(VOCABULARY.tgt))\n","tgt_corpus = read_corpus(train_tgt, 'tgt')[0:4]\n","print(tgt_corpus)\n","target_padded = VOCABULARY.tgt.to_input_tensor(tgt_corpus, 'GPU:0')\n","print('target padded', target_padded, target_padded[:,:-1])\n","target_masks = tf.cast(target_padded != VOCABULARY.tgt['<pad>'], tf.float32)\n","print(target_masks)\n","\n","'''\n","#There's a bug in TF2.0 RC0 that prevents tf.int32 tensor to be placed on GPU (but tf.int16/64 or tf.float32 is fine)\n","dtype = tf.int64\n","var1 = np.array([[1,2,3],[11,22,33]])\n","with tf.device(\"GPU:0\"):\n","    var1 = tf.constant(var1, dtype=dtype)\n","print(var1.device)\n","\n","var1 = np.array([[1,2,3],[11,22,33]])\n","with tf.device(\"CPU:0\"):\n","    var1 = tf.constant(var1, dtype=dtype)\n","print(var1.device)\n","'''"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Vocab(source 50004 words, target 50002 words)\n","50002\n","[['<s>', 'Thank', 'you', 'so', 'much,', 'Chris.', 'And', \"it's\", 'truly', 'a', 'great', 'honor', 'to', 'have', 'the', 'opportunity', 'to', 'come', 'to', 'this', 'stage', 'twice;', \"I'm\", 'extremely', 'grateful.', '</s>'], ['<s>', 'I', 'have', 'been', 'blown', 'away', 'by', 'this', 'conference,', 'and', 'I', 'want', 'to', 'thank', 'all', 'of', 'you', 'for', 'the', 'many', 'nice', 'comments', 'about', 'what', 'I', 'had', 'to', 'say', 'the', 'other', 'night.', '</s>'], ['<s>', 'And', 'I', 'say', 'that', 'sincerely,', 'partly', 'because', '(Mock', 'sob)', 'I', 'need', 'that.', '', 'Put', 'yourselves', 'in', 'my', 'position.', '</s>'], ['<s>', 'I', 'flew', 'on', 'Air', 'Force', 'Two', 'for', 'eight', 'years.', '</s>']]\n","target padded tf.Tensor(\n","[[    1   183    13    39  1587  6916    15    38  1094     7   199  2917\n","      5    21     4   796     5   149     5    16  1262 45903    77   926\n","  16104     2     0     0     0     0     0     0]\n"," [    1    11    21    94  6031   359    54    16  5821     8    11    93\n","      5   787    34     6    13    19     4   128   883  6759    26    28\n","     11    57     5   180     4    96  1993     2]\n"," [    1    15    11   180     9     3  4097    58 45904 45905    11   122\n","    198   151  4500  7092    10    35  4876     2     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [    1    11  2628    24  4578  6760  1562    19   695   392     2     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]], shape=(4, 32), dtype=int64) tf.Tensor(\n","[[    1   183    13    39  1587  6916    15    38  1094     7   199  2917\n","      5    21     4   796     5   149     5    16  1262 45903    77   926\n","  16104     2     0     0     0     0     0]\n"," [    1    11    21    94  6031   359    54    16  5821     8    11    93\n","      5   787    34     6    13    19     4   128   883  6759    26    28\n","     11    57     5   180     4    96  1993]\n"," [    1    15    11   180     9     3  4097    58 45904 45905    11   122\n","    198   151  4500  7092    10    35  4876     2     0     0     0     0\n","      0     0     0     0     0     0     0]\n"," [    1    11  2628    24  4578  6760  1562    19   695   392     2     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0]], shape=(4, 31), dtype=int64)\n","tf.Tensor(\n","[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n","  1. 1. 0. 0. 0. 0. 0. 0.]\n"," [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n","  1. 1. 1. 1. 1. 1. 1. 1.]\n"," [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n","  0. 0. 0. 0. 0. 0. 0. 0.]\n"," [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","  0. 0. 0. 0. 0. 0. 0. 0.]], shape=(4, 32), dtype=float32)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'\\n#There\\'s a bug in TF2.0 RC0 that prevents tf.int32 tensor to be placed on GPU (but tf.int16/64 or tf.float32 is fine)\\ndtype = tf.int64\\nvar1 = np.array([[1,2,3],[11,22,33]])\\nwith tf.device(\"GPU:0\"):\\n    var1 = tf.constant(var1, dtype=dtype)\\nprint(var1.device)\\n\\nvar1 = np.array([[1,2,3],[11,22,33]])\\nwith tf.device(\"CPU:0\"):\\n    var1 = tf.constant(var1, dtype=dtype)\\nprint(var1.device)\\n'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"kV5rreup1qQL","colab_type":"code","outputId":"dd03a2ef-bc88-4280-ee7c-7b53f312fe4d","executionInfo":{"status":"ok","timestamp":1572131822448,"user_tz":420,"elapsed":5564,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"}},"colab":{"base_uri":"https://localhost:8080/","height":292}},"source":["# Playground\n","#1. Pad sentences (pad an uneven list of lists into a 2d matrix)\n","x = [[1,2,3], [1,21], [1], [11,22,1,3]]\n","x_padded = tf.keras.preprocessing.sequence.pad_sequences(x, padding='post', value=0.0)\n","print(x_padded)\n","\n","#Constants and Variables\n","a = tf.constant([1,2,3])\n","a2 = tf.constant(a)\n","b = tf.ones((3,1))\n","\n","c = tf.Variable(initial_value=a)\n","print(a, a2, b, c)\n","print(c[2])\n","print()\n","\n","print(a[0], a2[0], b[0], c[0])\n","#a[0] = 11 #error as constant does not support item assignment\n","#b[0] = 11 #error as constant does not support item assignment\n","#c[0] = 11 #error as variable does not support item assignment but it does have a method to update the value\n","c[0].assign(11)\n","print(c)\n","print()\n","\n","# Good read on pack padded sentences for RNN\n","# 1. Pytoch: https://suzyahyah.github.io/pytorch/2019/07/01/DataLoader-Pad-Pack-Sequence.html\n","'''\n","        The whole sequence is:\n","        pad\n","        embed\n","        pack_padded\n","        – [rnn] -->\n","        pad_packed\n","        eval\n","'''\n","# 2. TF: https://danijar.com/variable-sequence-lengths-in-tensorflow/\n","\n","# Don't need to do pack padding in TF 2.0 due to masking operation in embeddings layer and other layers that support masking."],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[ 1  2  3  0]\n"," [ 1 21  0  0]\n"," [ 1  0  0  0]\n"," [11 22  1  3]]\n","tf.Tensor([1 2 3], shape=(3,), dtype=int32) tf.Tensor([1 2 3], shape=(3,), dtype=int32) tf.Tensor(\n","[[1.]\n"," [1.]\n"," [1.]], shape=(3, 1), dtype=float32) <tf.Variable 'Variable:0' shape=(3,) dtype=int32, numpy=array([1, 2, 3], dtype=int32)>\n","tf.Tensor(3, shape=(), dtype=int32)\n","\n","tf.Tensor(1, shape=(), dtype=int32) tf.Tensor(1, shape=(), dtype=int32) tf.Tensor([1.], shape=(1,), dtype=float32) tf.Tensor(1, shape=(), dtype=int32)\n","<tf.Variable 'Variable:0' shape=(3,) dtype=int32, numpy=array([11,  2,  3], dtype=int32)>\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'\\n        The whole sequence is:\\n        pad\\n        embed\\n        pack_padded\\n        – [rnn] -->\\n        pad_packed\\n        eval\\n'"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"DmVktMpgJD4M","colab_type":"text"},"source":["## 3. MODEL"]},{"cell_type":"code","metadata":{"id":"GPoFGv2Wc2R2","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import tensorflow.keras as K"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x5GGttGbSQvq","colab_type":"code","outputId":"f5010a49-c172-4ee6-e700-5154ca3e1a2d","executionInfo":{"status":"ok","timestamp":1576438539270,"user_tz":480,"elapsed":1512,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["#Playground\n","tf.random.set_seed(0)\n","\n","'''\n","#Embeddings\n","print('1. EMBEDDINGS')\n","tf.random.set_seed(1)\n","embd = ModelEmbeddings(3, VOCABULARY)\n","x = tf.constant([[1,0,3,0]], dtype=tf.int32)\n","# x = tf.constant([1], dtype=tf.int32)\n","print(embd.source(x))\n","print(embd.source.variables)\n","\n","in_ = K.layers.Input(shape=(4,))\n","y = embd.source(in_)\n","x = tf.constant([[1,0,3,0]], dtype=tf.int32)\n","model = K.Model(inputs=in_, outputs=y)\n","print(y)\n","print(model(x))\n","print()\n","\n","\n","#TF Basics\n","a = tf.Variable([1,2])\n","print(a)\n","a = tf.cast(a, dtype=tf.float32)\n","print(a)\n","\n","a = tf.Variable([[1.0,2,3], [11,22,33]])\n","print(a)\n","b = tf.nn.softmax(a, axis=-1)\n","print(b)\n","c = tf.nn.log_softmax(a, axis=-1)\n","print(c)\n","\n","print('zzz')\n","a = tf.ones((5,4,2))\n","b = tf.split(a, a.shape[1], 1)\n","print(b)\n","print(a[:,1,:])\n","\n","\n","device_dict = {'cpu': \"CPU:0\", 'gpu': \"GPU:0\"}\n","with tf.device(device_dict['gpu']):\n","    #Embeddings layer\n","    model_embeddings = K.layers.Embedding(4, 3)\n","    print(model_embeddings.trainable_variables)\n","    x = tf.constant([[1,0,3,0]], dtype=tf.int64)\n","    print(model_embeddings(x))\n","    print(model_embeddings.trainable_variables)\n","    print(model_embeddings.trainable_variables[0].device)\n","    print(x.device)\n","\n","\n","# TF Boolean mask\n","\n","# This doesn't work\n","a = tf.Variable([[1,2],[11,22],[1,3]])\n","b = tf.Variable([[1,0],[1,1],[1,0]])\n","print(a>2)\n","print(tf.boolean_mask(a,a>2))\n","# a[b].assign(1)\n","# a\n","\n","#This works\n","a = tf.Variable([[1,2],[11,22],[1,3]])\n","b = tf.Variable([[1,0],[1,1],[1,0]])\n","b = tf.cast(b, tf.bool)\n","print('A: ', a>2)\n","print('B: ', tf.where(a>2, 1, 0))\n","print('C: ', tf.where(b, a, 0))\n","# a[b].assign(1)\n","# a\n","\n","#Numpy Boolean mask\n","a = np.array([[1,2],[11,22],[1,3]])\n","b = np.array([[1,0],[1,1],[1,0]], dtype=np.bool) #boolean mask\n","a[b] = 10\n","print(a)\n","a[a==2] = 11\n","print(a)\n","\n","# Reshape vs Transpose (In most cases, use transpose and not reshape, esp when using matrix multiplication!)\n","a = tf.random.uniform([2,3])\n","print(a,'\\n')\n","print(tf.transpose(a, [1,0]),'\\n')\n","print(tf.reshape(a, [3,2]),'\\n')\n","\n","#tf.gather\n","b,t,v = 2,3,2\n","params = tf.random.uniform([b,t,v])\n","# params = tf.transpose(params)\n","print(params, '\\n')\n","print()\n","#print(params[:,[1,2]]) #doesn't work\n","indices = [0,1,1,1,0,0]\n","gthr = tf.gather(params, indices, axis=0)\n","print(gthr, '\\n')\n","# indices = tf.constant([[1],[0]])\n","#gthr = tf.gather_nd(params, indices)\n","#print(gthr, '\\n')\n","\n","\n","params = tf.random.uniform([b,t,v])\n","# indices = [1,0] #tf.constant([[1],[2]])\n","indices = [[0,1,0], [1,1,1]]\n","indices = tf.constant([[1],[0],[1],[0],[1],[0]])\n","print(params, '\\n')\n","print(indices, '\\n')\n","gthr = tf.gather(params, indices, axis=2)\n","print(gthr, '\\n')\n","gthr = tf.gather_nd(params, indices)\n","print(gthr, '\\n')\n","params = tf.reshape(params, [-1,v])\n","print(params, '\\n')\n","params = tf.reshape(params, [b,t,v])\n","print(params, '\\n')\n","\n","indices = [[0, 0], [1, 1]]\n","params = [['a', 'b'], ['c', 'd']]\n","#output = ['a', 'd']\n","gthr = tf.gather_nd(params, indices)\n","print(gthr, '\\n')\n","\n","\n","params = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n","# indices = tf.constant([[0,0], [2,1]])\n","indices = [0,1]\n","print(tf.gather(params, indices, axis=0))\n","'''\n","\n","\n","'''\n","#TF vs Pytroch gather function comparison\n","\n","import time\n","import numpy as np\n","import tensorflow as tf\n","from numba import jit\n","import torch\n","\n","tf.gather_nd([[1,2,3]], [[1]]) #do this first as tf.gather_nd() takes a lot of time the first time its called\n","\n","# b, t, v = 2, 4, 5\n","b, t, v = 64, 300, 20000\n","params = np.arange(b*t*v).reshape([b,t,v])\n","indices = np.random.randint(0,v, [b,t])\n","\n","def func_pythn(indices, params):\n","    #Using for Python loop\n","    def get_indices(indices):\n","        #indices = [[[0,0,0]],[[1,1,0]]]\n","        indices = [[[i,j,indices[i,j]] for j in range(indices.shape[1])] for i in range(indices.shape[0])] #this is slow\n","        return indices\n","    indices = get_indices(indices)\n","    indices = tf.Variable(indices)\n","    selected = tf.gather_nd(params, indices)\n","    return selected\n","\n","@jit #(nopython=True, parallel=True) # about 3x faster\n","def func_pythn_numba(indices, params):\n","    #using python for loop but then optimized using Numba\n","    def get_indices(indices):\n","        #indices = [[[0,0,0]],[[1,1,0]]]\n","        indices = [[[i,j,indices[i,j]] for j in range(indices.shape[1])] for i in range(indices.shape[0])] #this is slow\n","        return indices\n","    indices = get_indices(indices)\n","    indices = tf.Variable(indices)\n","    selected = tf.gather_nd(params, indices)\n","    return selected\n","\n","def func_tf_with_reshape(indices, params):\n","    #purely TF with reshape\n","    (b,t,v) = params.shape #indices: (b,t)\n","    params = tf.reshape(params, [-1, v])\n","    indices = tf.reshape(indices, [-1,1])\n","    indices = tf.stack([tf.range(b*t, dtype=tf.int64),indices[:,0]],axis=-1)\n","    selected = tf.gather_nd(params, indices)\n","    selected = tf.reshape(selected, [b,t])\n","    return selected\n","\n","def func_tf(indices, params):\n","    #purely TF without reshape\n","    (b,t,v) = params.shape #indices: (b,t)\n","    B,T = tf.meshgrid(tf.range(b), tf.range(t), indexing='ij')\n","    indices = tf.stack([B,T,indices], axis=-1) #(b,t,3)\n","    selected = tf.gather_nd(params, indices)\n","    return selected\n","\n","\n","\n","def func_trch(indices, params):\n","    #purely Pytroch\n","    #params = torch.tensor(params)\n","    #indices = torch.tensor(indices)\n","    indices = torch.unsqueeze(indices, dim=-1)\n","    selected = torch.gather(params, 2, indices)\n","    return selected\n","\n","\n","pp = tf.Variable(params, dtype=tf.float32)\n","strt = time.time()\n","func_pythn(indices, pp)\n","print('func_pythn w/ TF: ', time.time()-strt)\n","\n","pp = tf.Variable(params, dtype=tf.float32)\n","strt = time.time()\n","func_pythn_numba(indices, pp)\n","print('func_pythn_numba w/ TF: ', time.time()-strt)\n","\n","pp = tf.Variable(params, dtype=tf.float32)\n","strt = time.time()\n","func_pythn_numba(indices, pp)\n","print('func_pythn_numba w/ TF: ', time.time()-strt)\n","\n","\n","pp = tf.Variable(params, dtype=tf.float32)\n","strt = time.time()\n","func_tf_with_reshape(indices, pp)\n","print('func_tf_with_reshape: ', time.time()-strt)\n","\n","\n","pp = tf.Variable(params, dtype=tf.float32)\n","strt = time.time()\n","func_tf(indices, pp)\n","print('func_tf: ', time.time()-strt)\n","\n","\n","pp = torch.tensor(params, dtype=torch.float32)\n","ii = torch.tensor(indices)\n","strt = time.time()\n","func_trch(ii, pp)\n","print('func_trch: ', time.time()-strt)\n","\n","print('\\nTF Gradient')\n","with tf.GradientTape() as t:\n","    params_tf = tf.Variable(params, dtype=tf.float32)\n","    y_tf = func_tf(indices, params_tf)\n","    y = tf.reduce_sum(y_tf)\n","# print(y_tf, '\\n')\n","print(y, '\\n')\n","g_tf = t.gradient(y, params_tf)\n","# print(g_tf, '\\n')\n","\n","print('\\nPytorch Gradient')\n","params_trch = torch.tensor(params, dtype=torch.float32)\n","params_trch.requires_grad = True\n","indices_trch = torch.tensor(indices)\n","y_trch = func_trch(indices_trch, params_trch)\n","y = torch.sum(y_trch)\n","y.backward()\n","# print(y_trch, '\\n')\n","print(y, '\\n')\n","g_trch = params_trch.grad\n","# print(g_trch, '\\n')\n","\n","print('TF vs Pytroch Gradient Comparison')\n","print(np.all(g_tf.numpy() == g_trch.numpy()))\n","\n","\n","#TF Gradient Tape\n","a = tf.Variable(1.0)\n","b = tf.Variable(2.0)\n","b2 = tf.Variable(3.0)\n","a2 = tf.stop_gradient(a)\n","with tf.GradientTape() as t:\n","    c = 2*a2 + b + a\n","grad = t.gradient(c, a)\n","print(grad)\n","'''"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n#TF vs Pytroch gather function comparison\\n\\nimport time\\nimport numpy as np\\nimport tensorflow as tf\\nfrom numba import jit\\nimport torch\\n\\ntf.gather_nd([[1,2,3]], [[1]]) #do this first as tf.gather_nd() takes a lot of time the first time its called\\n\\n# b, t, v = 2, 4, 5\\nb, t, v = 64, 300, 20000\\nparams = np.arange(b*t*v).reshape([b,t,v])\\nindices = np.random.randint(0,v, [b,t])\\n\\ndef func_pythn(indices, params):\\n    #Using for Python loop\\n    def get_indices(indices):\\n        #indices = [[[0,0,0]],[[1,1,0]]]\\n        indices = [[[i,j,indices[i,j]] for j in range(indices.shape[1])] for i in range(indices.shape[0])] #this is slow\\n        return indices\\n    indices = get_indices(indices)\\n    indices = tf.Variable(indices)\\n    selected = tf.gather_nd(params, indices)\\n    return selected\\n\\n@jit #(nopython=True, parallel=True) # about 3x faster\\ndef func_pythn_numba(indices, params):\\n    #using python for loop but then optimized using Numba\\n    def get_indices(indices):\\n        #indices = [[[0,0,0]],[[1,1,0]]]\\n        indices = [[[i,j,indices[i,j]] for j in range(indices.shape[1])] for i in range(indices.shape[0])] #this is slow\\n        return indices\\n    indices = get_indices(indices)\\n    indices = tf.Variable(indices)\\n    selected = tf.gather_nd(params, indices)\\n    return selected\\n\\ndef func_tf_with_reshape(indices, params):\\n    #purely TF with reshape\\n    (b,t,v) = params.shape #indices: (b,t)\\n    params = tf.reshape(params, [-1, v])\\n    indices = tf.reshape(indices, [-1,1])\\n    indices = tf.stack([tf.range(b*t, dtype=tf.int64),indices[:,0]],axis=-1)\\n    selected = tf.gather_nd(params, indices)\\n    selected = tf.reshape(selected, [b,t])\\n    return selected\\n\\ndef func_tf(indices, params):\\n    #purely TF without reshape\\n    (b,t,v) = params.shape #indices: (b,t)\\n    B,T = tf.meshgrid(tf.range(b), tf.range(t), indexing='ij')\\n    indices = tf.stack([B,T,indices], axis=-1) #(b,t,3)\\n    selected = tf.gather_nd(params, indices)\\n    return selected\\n\\n\\n\\ndef func_trch(indices, params):\\n    #purely Pytroch\\n    #params = torch.tensor(params)\\n    #indices = torch.tensor(indices)\\n    indices = torch.unsqueeze(indices, dim=-1)\\n    selected = torch.gather(params, 2, indices)\\n    return selected\\n\\n\\npp = tf.Variable(params, dtype=tf.float32)\\nstrt = time.time()\\nfunc_pythn(indices, pp)\\nprint('func_pythn w/ TF: ', time.time()-strt)\\n\\npp = tf.Variable(params, dtype=tf.float32)\\nstrt = time.time()\\nfunc_pythn_numba(indices, pp)\\nprint('func_pythn_numba w/ TF: ', time.time()-strt)\\n\\npp = tf.Variable(params, dtype=tf.float32)\\nstrt = time.time()\\nfunc_pythn_numba(indices, pp)\\nprint('func_pythn_numba w/ TF: ', time.time()-strt)\\n\\n\\npp = tf.Variable(params, dtype=tf.float32)\\nstrt = time.time()\\nfunc_tf_with_reshape(indices, pp)\\nprint('func_tf_with_reshape: ', time.time()-strt)\\n\\n\\npp = tf.Variable(params, dtype=tf.float32)\\nstrt = time.time()\\nfunc_tf(indices, pp)\\nprint('func_tf: ', time.time()-strt)\\n\\n\\npp = torch.tensor(params, dtype=torch.float32)\\nii = torch.tensor(indices)\\nstrt = time.time()\\nfunc_trch(ii, pp)\\nprint('func_trch: ', time.time()-strt)\\n\\nprint('\\nTF Gradient')\\nwith tf.GradientTape() as t:\\n    params_tf = tf.Variable(params, dtype=tf.float32)\\n    y_tf = func_tf(indices, params_tf)\\n    y = tf.reduce_sum(y_tf)\\n# print(y_tf, '\\n')\\nprint(y, '\\n')\\ng_tf = t.gradient(y, params_tf)\\n# print(g_tf, '\\n')\\n\\nprint('\\nPytorch Gradient')\\nparams_trch = torch.tensor(params, dtype=torch.float32)\\nparams_trch.requires_grad = True\\nindices_trch = torch.tensor(indices)\\ny_trch = func_trch(indices_trch, params_trch)\\ny = torch.sum(y_trch)\\ny.backward()\\n# print(y_trch, '\\n')\\nprint(y, '\\n')\\ng_trch = params_trch.grad\\n# print(g_trch, '\\n')\\n\\nprint('TF vs Pytroch Gradient Comparison')\\nprint(np.all(g_tf.numpy() == g_trch.numpy()))\\n\\n\\n#TF Gradient Tape\\na = tf.Variable(1.0)\\nb = tf.Variable(2.0)\\nb2 = tf.Variable(3.0)\\na2 = tf.stop_gradient(a)\\nwith tf.GradientTape() as t:\\n    c = 2*a2 + b + a\\ngrad = t.gradient(c, a)\\nprint(grad)\\n\""]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"Rj3ywV7nXmiU","colab_type":"code","colab":{}},"source":["#model_embeddings.py\n","\n","\"\"\"\n","CS224N 2018-19: Homework 4\n","model_embeddings.py: Embeddings for the NMT model\n","Pencheng Yin <pcyin@cs.cmu.edu>\n","Sahil Chopra <schopra8@stanford.edu>\n","Anand Dhoot <anandd@stanford.edu>\n","Implemented in TF 2.0 by Amit Patel\n","\n","\"\"\"\n","#Embeddings Layer\n","class ModelEmbeddings(tf.keras.layers.Layer): \n","    \"\"\"\n","    Class that converts input words to their embeddings.\n","    \"\"\"\n","    def __init__(self, embed_size, vocab):\n","        \"\"\"\n","        Init the Embedding layers.\n","\n","        @param embed_size (int): Embedding size (dimensionality)\n","        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n","                              See vocab.py for documentation.\n","        \"\"\"\n","        super(ModelEmbeddings, self).__init__()\n","        self.embed_size = embed_size\n","\n","        # default values\n","        self.source = None\n","        self.target = None\n","\n","        src_pad_token_idx = vocab.src['<pad>'] #idx is 0 for the pad token\n","        tgt_pad_token_idx = vocab.tgt['<pad>'] #idx is 0 for the pad token\n","        #print(src_pad_token_idx)\n","        #print(tgt_pad_token_idx)\n","\n","        ### YOUR CODE HERE (~2 Lines)\n","        ### TODO - Initialize the following variables:\n","        ###     self.source (Embedding Layer for source language)\n","        ###     self.target (Embedding Layer for target langauge)\n","        ###\n","        ### Note:\n","        ###     1. `vocab` object contains two vocabularies:\n","        ###            `vocab.src` for source\n","        ###            `vocab.tgt` for target\n","        ###     2. You can get the length of a specific vocabulary by running:\n","        ###             `len(vocab.<specific_vocabulary>)`\n","        ###     3. Remember to include the padding token for the specific vocabulary\n","        ###        when creating your Embedding.\n","        ###\n","        ### Use the following docs to properly initialize these variables:\n","        ###     Embedding Layer:\n","        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding\n","        ###         https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Embedding\n","        \n","        #Don't do len(vocab.src)+1 because 'pad' token is already in the vocabulary\n","        self.source = K.layers.Embedding(len(vocab.src), self.embed_size,  mask_zero=True)\n","        self.target = K.layers.Embedding(len(vocab.tgt), self.embed_size,  mask_zero=True) #not necessary to use mask_zero as decoder uses a for loop\n","        ### END YOUR CODE\n","    \n","    def build(self, input_shape):\n","        self.source.build(input_shape)\n","        self.target.build(input_shape)\n","        self.built = True\n","\n","    '''\n","    def __call__(self, X):\n","        in_shape = X.shape\n","        self.build(in_shape)\n","        return self.call(X)\n","    '''"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ha74nVVz6eb","colab_type":"code","colab":{}},"source":["#nmt_model.py\n","\n","\"\"\"\n","CS224N 2018-19: Homework 4\n","nmt_model.py: NMT Model\n","Pencheng Yin <pcyin@cs.cmu.edu>\n","Sahil Chopra <schopra8@stanford.edu>\n","Amit Patel: Implemented in TF 2.04/sAEboJhx2nVdbGNmAjoESGlyKXs0Tk__ssNz-EM_MPGjMgwUv-kNeBw\n","\"\"\"\n","from collections import namedtuple\n","import sys\n","from typing import List, Tuple, Dict, Set, Union\n","\n","Hypothesis = namedtuple('Hypothesis', ['value', 'score'])\n","\n","#NMT Model\n","class NMT(tf.keras.Model):\n","    \"\"\" Simple Neural Machine Translation Model:\n","        - Bidrectional LSTM Encoder\n","        - Unidirection LSTM Decoder\n","        - Global Attention Model (Luong, et al. 2015)\n","    \"\"\"\n","    def __init__(self, embed_size, hidden_size, vocab, dropout_rate=0.2, device='gpu', training=True):\n","        \"\"\" Init NMT Model.\n","\n","        @param embed_size (int): Embedding size (dimensionality)\n","        @param hidden_size (int): Hidden Size (dimensionality)\n","        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n","                              See vocab.py for documentation.\n","        @param dropout_rate (float): Dropout probability, for attention\n","        @param device (string): gpu or cpu\n","        @param training: True or False used for the dropout layer\n","        \"\"\"\n","        super(NMT, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.dropout_rate = dropout_rate\n","        self.vocab = vocab\n","        self.training = training\n","\n","\n","        # default values\n","        self.encoder = None \n","        self.decoder = None\n","        self.h_projection = None\n","        self.c_projection = None\n","        self.att_projection = None\n","        self.combined_output_projection = None\n","        self.target_vocab_projection = None\n","        self.dropout = None\n","\n","\n","        ### YOUR CODE HERE (~8 Lines)\n","        ### TODO - Initialize the following variables:\n","        ###     self.encoder (Bidirectional LSTM with bias)\n","        ###     self.decoder (LSTM Cell with bias)\n","        ###     self.h_projection (Linear Layer with no bias), called W_{h} in the PDF.\n","        ###     self.c_projection (Linear Layer with no bias), called W_{c} in the PDF.\n","        ###     self.att_projection (Linear Layer with no bias), called W_{attProj} in the PDF.\n","        ###     self.combined_output_projection (Linear Layer with no bias), called W_{u} in the PDF.\n","        ###     self.target_vocab_projection (Linear Layer with no bias), called W_{vocab} in the PDF.\n","        ###     self.dropout (Dropout Layer)\n","        ###\n","        ### Use the following docs to properly initialize these variables:\n","        ###     LSTM:\n","        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM\n","        ###         https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/LSTM\n","        ###     LSTM Cell:\n","        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell\n","        ###     Linear Layer:\n","        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Linear\n","        ###     Dropout Layer:\n","        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout\n","\n","        #cpu or gpu\n","        device_dict = {'cpu': \"CPU:0\", 'gpu': \"GPU:0\"}\n","        self.tf_device = device_dict[device]\n","        \n","        #Embeddings layer\n","        self.model_embeddings = ModelEmbeddings(embed_size, vocab)\n","        #encoder layer\n","        encd_fwd = K.layers.LSTM(hidden_size, return_sequences=True, return_state=True)\n","        encd_bck = K.layers.LSTM(hidden_size, return_sequences=True, return_state=True, go_backwards=True)\n","        self.encoder = K.layers.Bidirectional(layer=encd_fwd, backward_layer=encd_bck)\n","        #decoder layer\n","        self.decoder = K.layers.LSTMCell(hidden_size, implementation=1) #fails with implementation=2\n","        #w_h\n","        self.h_projection = K.layers.Dense(hidden_size, input_shape=(2*hidden_size,), use_bias=False) #no need to specify the input shape. Will figure it out when build() us called.\n","        #w_c\n","        self.c_projection = K.layers.Dense(hidden_size, input_shape=(2*hidden_size,), use_bias=False) #no need to specify the input shape. Will figure it out when build() us called.\n","        #w_att\n","        self.att_projection = K.layers.Dense(hidden_size, input_shape=(2*hidden_size,), use_bias=False) #no need to specify the input shape. Will figure it out when build() us called.\n","        #w_u\n","        self.combined_output_projection = K.layers.Dense(hidden_size, input_shape=(3*hidden_size,), use_bias=False) #no need to specify the input shape. Will figure it out when build() us called.\n","        #w_vocab\n","        self.target_vocab_projection = K.layers.Dense(len(vocab.tgt), input_shape=(hidden_size,), use_bias=False) #no need to specify the input shape. Will figure it out when build() us called.\n","        #Dropout Layer\n","        self.dropout = K.layers.Dropout(dropout_rate)\n","\n","     ### END YOUR CODE\n","\n","\n","    def call(self, source, target):\n","        \"\"\" Take a mini-batch of source and target sentences, compute the log-likelihood of\n","        target sentences under the language models learned by the NMT system.\n","\n","        @param source (List[List[str]]): list of source sentence tokens\n","        @param target (List[List[str]]): list of target sentence tokens, wrapped by `<s>` and `</s>`\n","\n","        @returns scores (Tensor): a variable/tensor of shape (b, ) representing the\n","                                    log-likelihood of generating the gold-standard target sentence for\n","                                    each example in the input batch. Here b = batch size.\n","        \"\"\"\n","        # Compute sentence lengths\n","        source_lengths = [len(s) for s in source]\n","\n","        # Convert list of lists into tensors\n","        source_padded = self.vocab.src.to_input_tensor(source, device=self.device)   # Tensor: (b, src_len)\n","        target_padded = self.vocab.tgt.to_input_tensor(target, device=self.device)   # Tensor: (b, tgt_len)\n","\n","        ###     Run the network forward:\n","        ###     1. Apply the encoder to `source_padded` by calling `self.encode()`\n","        ###     2. Generate sentence masks for `source_padded` by calling `self.generate_sent_masks()`\n","        ###     3. Apply the decoder to compute combined-output by calling `self.decode()`\n","        ###     4. Compute log probability distribution over the target vocabulary using the\n","        ###        combined_outputs returned by the `self.decode()` function.\n","\n","        enc_hiddens, dec_init_state = self.encode(source_padded, source_lengths) #(b,src_len,2h), ((b,2h),(b,2h))\n","        enc_masks = self.generate_sent_masks(enc_hiddens, source_lengths) #(b,src_len)\n","        combined_outputs, dec_state = self.decode(enc_hiddens, enc_masks, dec_init_state, target_padded) #(b, tgt_len-1, h),\n","        P = tf.nn.log_softmax(self.target_vocab_projection(combined_outputs), axis=-1) #(b, tgt_len, Vt)\n","        # Zero out, probabilities for which we have nothing in the target text (i.e. padded tokens)\n","        target_masks = tf.cast(target_padded != self.vocab.tgt['<pad>'], tf.float32) #(b, tgt_len)\n","        \n","        # Compute log probability of generating true target words\n","        #Doing idx=target_padded[:,1:] and target_masks[:,1:], as we are predicting the next word for each input word.\n","        ###P = tf.transpose(P, perm=[1,0,2]) #(b, tgt_len, Vt). NB: don't use reshape! (no need to transpose in TF as it is in proper shape)\n","        (b,tgt_len,_) = P.shape\n","        B,T = tf.meshgrid(tf.range(b, dtype=tf.int64), tf.range(tgt_len, dtype=tf.int64), indexing='ij')\n","\n","        idx = tf.stack([B, T, target_padded[:,1:]], axis=-1) #(b, tgt_len-1, 3)\n","        target_gold_words_log_prob = tf.gather_nd(P, idx) #(b,tgt_len-1)\n","        target_gold_words_log_prob = target_gold_words_log_prob * target_masks[:,1:] #(b,tgt_len-1)\n","        scores = tf.reduce_sum(target_gold_words_log_prob, axis=1) #sum across the time axis. Shape is (b,) (each number corresponds to the log probability of each sentence)\n","        \n","        #Pytorch code for reference\n","        #target_gold_words_log_prob = torch.gather(P, index=target_padded[1:].unsqueeze(-1), dim=-1).squeeze(-1) * target_masks[1:]\n","        #scores = target_gold_words_log_prob.sum(dim=0) #dim 0 is tgt_len\n","\n","        return scores\n","\n","\n","\n","    def encode(self, source_padded, source_lengths):\n","        \"\"\" Apply the encoder to source sentences to obtain encoder hidden states.\n","            Additionally, take the final states of the encoder and project them to obtain initial states for decoder.\n","\n","        @param source_padded (Tensor): Tensor of padded source sentences with shape (b, src_len), where\n","                                        b = batch_size, src_len = maximum source sentence length. Note that \n","                                       these have already been sorted in order of longest to shortest sentence.\n","        @param source_lengths (List[int]): List of actual lengths for each of the source sentences in the batch (not being used in TF version as no need to do 'pack_padded_sequence' etc)\n","        @returns enc_hiddens (Tensor): Tensor of hidden units with shape (b, src_len, h*2), where\n","                                        b = batch size, src_len = maximum source sentence length, h = hidden size.\n","        @returns dec_init_state (tuple(Tensor, Tensor)): Tuple of tensors representing the decoder's initial\n","                                                hidden state and cell.\n","        \"\"\"\n","        enc_hiddens, dec_init_state = None, None\n","\n","        ### YOUR CODE HERE (~ 8 Lines)\n","        ### TODO:\n","        ###     1. Construct Tensor `embd` of source sentences with shape (b, src_len, e) using the source model embeddings.\n","        ###         src_len = maximum source sentence length, b = batch size, e = embedding size. Note\n","        ###         that there is no initial hidden state or cell for the decoder.\n","        ###     2. Compute `enc_hiddens`, `last_hidden`, `last_cell` by applying the encoder to `embd`.\n","        ###         - Before you can apply the encoder, you need to apply the `pack_padded_sequence` function to embd.\n","        ###         - After you apply the encoder, you need to apply the `pad_packed_sequence` function to enc_hiddens.\n","        ###         - Note that the shape of the tensor returned by the encoder is (src_len b, h*2) and we want to\n","        ###           return a tensor of shape (b, src_len, h*2) as `enc_hiddens`.\n","        ###     3. Compute `dec_init_state` = (init_decoder_hidden, init_decoder_cell):\n","        ###         - `init_decoder_hidden`:\n","        ###             `last_hidden` is a tensor shape (2, b, h). The first dimension corresponds to forwards and backwards.\n","        ###             Concatenate the forwards and backwards tensors to obtain a tensor shape (b, 2*h).\n","        ###             Apply the h_projection layer to this in order to compute init_decoder_hidden.\n","        ###             This is h_0^{dec} in the PDF. Here b = batch size, h = hidden size\n","        ###         - `init_decoder_cell`:\n","        ###             `last_cell` is a tensor shape (2, b, h). The first dimension corresponds to forwards and backwards.\n","        ###             Concatenate the forwards and backwards tensors to obtain a tensor shape (b, 2*h).\n","        ###             Apply the c_projection layer to this in order to compute init_decoder_cell.\n","        ###             This is c_0^{dec} in the PDF. Here b = batch size, h = hidden size\n","        ###\n","        ### See the following docs, as you may need to use some of the following functions in your implementation:\n","        ###     Pack the padded sequence embd before passing to the encoder:\n","        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence\n","        ###     Pad the packed sequence, enc_hiddens, returned by the encoder:\n","        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_packed_sequence\n","        ###     Tensor Concatenation:\n","        ###         https://pytorch.org/docs/stable/torch.html#torch.cat\n","        ###     Tensor Permute:\n","        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute\n","\n","        #AMIT TODO VERIFY THIS: DUE TO MASKING OPERATION SUPPORTED BY EMBEDDINGS AND LSTM LAYERS in TF 2.0, \n","        #DON'T NEED TO 'pack_padded_sequence' AND 'pad_packed_sequence'\n","\n","        #1 Embed the source input/sentences\n","        embd = self.model_embeddings.source(source_padded) #b x sen_len x embd_dim\n","\n","        #2 Apply the encoder layer\n","        enc_hiddens, last_hidden_fwd, last_cell_fwd, last_hidden_bck, last_cell_bck  = self.encoder(embd, initial_state=None) \n","        #(enc_hiddens: b x sen_len x 2*hidden_size, last_hidden_fwd: b x hidden_size, last_cell_fwd: b x hidden_size, last_hidden_bck: b x hidden_size, last_cell_bck: b x hidden_size)\n","        mask = tf.cast(embd._keras_mask, tf.float32) #b x sen_len\n","        mask = tf.broadcast_to(mask, shape=[2*self.hidden_size, embd.shape[0], embd.shape[1]]) #2h x b x sen_len\n","        mask = tf.transpose(mask, perm=[1,2,0]) #b x sen_len x 2h\n","        enc_hiddens = mask * enc_hiddens #so that padded inputs have zero output and proper gradients (#version B of masking where we take output from every time step (see notes for details on masking))\n","\n","        #3 Compute dec_init_state\n","        h0_dec = tf.concat([last_hidden_fwd, last_hidden_bck], axis=1) #b x 2*hidden_size\n","        h0_dec = self.h_projection(h0_dec) #b x hidden_size\n","        c0_dec = tf.concat([last_cell_fwd, last_cell_bck], axis=1) #b x 2*hidden_size\n","        c0_dec = self.c_projection(c0_dec) #b x hidden_size\n","        dec_init_state = (h0_dec, c0_dec)\n","        ### END YOUR CODE\n","\n","        return enc_hiddens, dec_init_state\n","\n","\n","    def decode(self, enc_hiddens, enc_masks, dec_init_state, target_padded):\n","        \"\"\"Compute combined output vectors for a batch.\n","\n","        @param enc_hiddens (Tensor): Hidden states (b, src_len, h*2), where\n","                                     b = batch size, src_len = maximum source sentence length, h = hidden size.\n","        @param enc_masks (Tensor): Tensor of sentence masks (b, src_len), where\n","                                     b = batch size, src_len = maximum source sentence length.\n","        @param dec_init_state (tuple(Tensor, Tensor)): Initial state and cell for decoder\n","        @param target_padded (Tensor): Gold-standard padded target sentences (b, tgt_len), where\n","                                       tgt_len = maximum target sentence length, b = batch size. \n","        \n","        @returns combined_outputs (Tensor): combined output tensor  (b, tgt_len,  h), where\n","                                        tgt_len = maximum target sentence length, b = batch_size,  h = hidden size\n","        \"\"\"\n","        # Chop off the <END> token for max length sentences. As the last input is 'end_of_sentence' token.\n","        # For shorter sentences in the batch, they will have an 'end_of_sentence' token followed by \n","        # many 'pad' tokens to mask the input. An input of 'pad' token will return the previous output with no gradient flow (see below code).\n","        # But for 'end_of_sentence' token, the model will learn to predict the 'pad' token\n","        # As in the source input, for shorter target sentences, after 'end_of_sentence' token, they are padded with 'pad' tokens.\n","        target_padded = target_padded[:,:-1] #(b, tgt_len-1)\n","\n","        # Initialize the decoder state (hidden and cell)\n","        dec_state = dec_init_state\n","\n","        # Initialize previous combined output vector o_{t-1} as zero\n","        batch_size = enc_hiddens.shape[0]\n","        with tf.device(self.tf_device):\n","            o_prev = tf.zeros((batch_size, self.hidden_size), dtype=tf.float32)\n","        #print(o_prev.device)\n","\n","        # Initialize a list we will use to collect the combined output o_t on each step\n","        combined_outputs = []\n","\n","        ### YOUR CODE HERE (~9 Lines)\n","        ### TODO:\n","        ###     1. Apply the attention projection layer to `enc_hiddens` to obtain `enc_hiddens_proj`,\n","        ###         which should be shape (b, src_len, h),\n","        ###         where b = batch size, src_len = maximum source length, h = hidden size.\n","        ###         This is applying W_{attProj} to h^enc, as described in the PDF.\n","        ###     2. Construct tensor `Y` of target sentences with shape (b, tgt_len, e) using the target model embeddings.\n","        ###         where tgt_len = maximum target sentence length, b = batch size, e = embedding size.\n","        ###     3. Use the torch.split function to iterate over the time dimension of Y.\n","        ###         Within the loop, this will give you Y_t of shape (1, b, e) where b = batch size, e = embedding size.\n","        ###             - Squeeze Y_t into a tensor of dimension (b, e). \n","        ###             - Construct Ybar_t by concatenating Y_t with o_prev.\n","        ###             - Use the step function to compute the the Decoder's next (cell, state) values\n","        ###               as well as the new combined output o_t.\n","        ###             - Append o_t to combined_outputs\n","        ###             - Update o_prev to the new o_t.\n","        ###     4. Use torch.stack to convert combined_outputs from a list length tgt_len of\n","        ###         tensors shape (b, h), to a single tensor shape (b, tgt_len, h)\n","        ###         where tgt_len = maximum target sentence length, b = batch size, h = hidden size.\n","        ###\n","        ### Note:\n","        ###    - When using the squeeze() function make sure to specify the dimension you want to squeeze\n","        ###      over. Otherwise, you will remove the batch dimension accidentally, if batch_size = 1.\n","        ###   \n","        ### Use the following docs to implement this functionality:\n","        ###     Zeros Tensor:\n","        ###         https://pytorch.org/docs/stable/torch.html#torch.zeros\n","        ###     Tensor Splitting (iteration):\n","        ###         https://pytorch.org/docs/stable/torch.html#torch.split\n","        ###     Tensor Dimension Squeezing:\n","        ###         https://pytorch.org/docs/stable/torch.html#torch.squeeze\n","        ###     Tensor Concatenation:\n","        ###         https://pytorch.org/docs/stable/torch.html#torch.cat\n","        ###     Tensor Stacking:\n","        ###         https://pytorch.org/docs/stable/torch.html#torch.stack\n","\n","        \n","        #1. Apply attention projection\n","        #enc_hiddens is b x sen_len x 2*hidden_size\n","        #self.att_projection matrix is hidden_size x 2*hidden_size\n","        #it will first reshape enc_hiddens to b*sen_len x 2*hidden_size (i.e. maintain the last dimension)\n","        #then apply att_projection's matrix multiplication\n","        #the reshape the output to b x sen_len x hidden_size\n","        enc_hiddens_proj = self.att_projection(enc_hiddens) #b x sen_len x hidden_size\n","\n","        #2 Embed the target input/sentences\n","        Y = self.model_embeddings.target(target_padded) #b x tgt_len x embd_dim\n","\n","        #3 Iterate over the time dimension\n","        #no need to use tf.split\n","        #Amit TODO: make sure for padded inputs, no gradients are passed.\n","\n","        tgt_len = Y.shape[1]\n","        prev_dec_state = (tf.zeros_like(dec_state[0]), tf.zeros_like(dec_state[1]))\n","        dec_masks = tf.cast(Y._keras_mask, tf.float32) #b x t\n","        #o_prev is initialized to zeros and is of shape b x h (see above)\n","        for t in range(tgt_len):\n","            #padded inputs mask to zero-out gradients\n","            mask_t = dec_masks[:,t:t+1] #b x 1\n","            mask_t = tf.stop_gradient(mask_t) #Not necessary actually, just doing it for my paranoia!\n","            mask_t_b = 1-mask_t #inverse of mask_t\n","\n","            #do the actual computation using step method\n","            Y_t = Y[:,t,:]\n","            Ybar_t = tf.concat([Y_t,o_prev], axis=1)\n","            dec_state, o_t, _ = self.step(Ybar_t, dec_state, enc_hiddens, enc_hiddens_proj, enc_masks)\n","\n","            #apply the mask (if comment out the below masking, then the results pass sanity check with rtol 1e-3. But commenting it out will be wrong, I think.)\n","            ##o_t = mask_t*o_t #version B of masking where we take output from every time step (see notes for details on masking)\n","            o_t = mask_t*o_t + mask_t_b*tf.stop_gradient(o_prev) #which type of masking is this? It is version A but with gradient flow of verison B\n","            dec_state[0] = mask_t*dec_state[0] + mask_t_b*prev_dec_state[0] #version A of masking where we want the gradient flow not to be corrupted by masked inputs (see notes for details on masking)\n","            dec_state[1] = mask_t*dec_state[1] + mask_t_b*prev_dec_state[1] #version A of masking where we want the gradient flow not to be corrupted by masked inputs (see notes for details on masking)\n","\n","            #update the respective variables\n","            o_prev = o_t #(b,h)\n","            prev_dec_state = dec_state\n","            combined_outputs.append(o_t)\n","\n","        #4. Combine the outputs from all time steps\n","        combined_outputs = tf.stack(combined_outputs, axis=1) #(b, tgt_len, h)\n","        ### END YOUR CODE\n","\n","        return combined_outputs, dec_state\n","\n","\n","    def step(self, Ybar_t, dec_state, enc_hiddens, enc_hiddens_proj, enc_masks=None):\n","        \"\"\" Compute one forward step of the LSTM decoder, including the attention computation.\n","\n","        @param Ybar_t (Tensor): Concatenated Tensor of [Y_t o_prev], with shape (b, e + h). The input for the decoder,\n","                                where b = batch size, e = embedding size, h = hidden size.\n","        @param dec_state (tuple(Tensor, Tensor)): Tuple of tensors both with shape (b, h), where b = batch size, h = hidden size.\n","                First tensor is decoder's prev hidden state, second tensor is decoder's prev cell.\n","        @param enc_hiddens (Tensor): Encoder hidden states Tensor, with shape (b, src_len, h * 2), where b = batch size,\n","                                    src_len = maximum source length, h = hidden size.\n","        @param enc_hiddens_proj (Tensor): Encoder hidden states Tensor, projected from (h * 2) to h. Tensor is with shape (b, src_len, h),\n","                                    where b = batch size, src_len = maximum source length, h = hidden size.\n","        @param enc_masks (Tensor): Tensor of sentence masks shape (b, src_len),\n","                                    where b = batch size, src_len is maximum source length.\n","\n","        @returns dec_state (tuple (Tensor, Tensor)): Tuple of tensors both shape (b, h), where b = batch size, h = hidden size.\n","                First tensor is decoder's new hidden state, second tensor is decoder's new cell.\n","        @returns combined_output (Tensor): Combined output Tensor at timestep t, shape (b, h), where b = batch size, h = hidden size.\n","        @returns e_t (Tensor): Tensor of shape (b, src_len). It is attention scores distribution.\n","                                Note: You will not use this outside of this function.\n","                                      We are simply returning this value so that we can sanity check\n","                                      your implementation.\n","        \"\"\"\n","\n","        combined_output = None\n","\n","        ### YOUR CODE HERE (~3 Lines)\n","        ### TODO:\n","        ###     1. Apply the decoder to `Ybar_t` and `dec_state`to obtain the new dec_state.\n","        ###     2. Split dec_state into its two parts (dec_hidden, dec_cell)\n","        ###     3. Compute the attention scores e_t, a Tensor shape (b, src_len). \n","        ###        Note: b = batch_size, src_len = maximum source length, h = hidden size.\n","        ###\n","        ###       Hints:\n","        ###         - dec_hidden is shape (b, h) and corresponds to h^dec_t in the PDF (batched)\n","        ###         - enc_hiddens_proj is shape (b, src_len, h) and corresponds to W_{attProj} h^enc (batched).\n","        ###         - Use batched matrix multiplication (torch.bmm) to compute e_t.\n","        ###         - To get the tensors into the right shapes for bmm, you will need to do some squeezing and unsqueezing.\n","        ###         - When using the squeeze() function make sure to specify the dimension you want to squeeze\n","        ###             over. Otherwise, you will remove the batch dimension accidentally, if batch_size = 1.\n","        ###\n","        ### Use the following docs to implement this functionality:\n","        ###     Batch Multiplication:\n","        ###        https://pytorch.org/docs/stable/torch.html#torch.bmm\n","        ###     Tensor Unsqueeze:\n","        ###         https://pytorch.org/docs/stable/torch.html#torch.unsqueeze\n","        ###     Tensor Squeeze:\n","        ###         https://pytorch.org/docs/stable/torch.html#torch.squeeze\n","\n","        #1 Apply decoder layer\n","        _, dec_state = self.decoder(Ybar_t, dec_state)\n","        \n","        #2 Split dec_state\n","        dec_hidden, dec_cell = dec_state #both tensors are b x h\n","\n","        #3. Attention scores\n","        #enc_hiddens_proj is shape (b, src_len, h)\n","        #A. using batch_matrix multiplication\n","        b_dim, src_len_dim, h_dim = enc_hiddens_proj.shape\n","        var_a = tf.expand_dims(dec_hidden, axis=1) #b x 1 x h\n","        ###var_b = tf.reshape(enc_hiddens_proj, [-1,h_dim,src_len_dim]) #b x h x src_len\n","        var_b = tf.transpose(enc_hiddens_proj, perm=[0,2,1]) #b x h x src_len (DON'T USE RESHAPE AS IT WILL GIVE WRONG RESULTS WHEN COMBINED WITH MATRIX MULTIPLICATION)\n","        e_t = tf.linalg.matmul(var_a, var_b) #b x 1 x src_len (will do matrix multiplication for the last 2 dimensions of both tensors, i.e. 1 x h by h x src_len and keep the other dimensions i.e. b)\n","        e_t = tf.squeeze(e_t, axis=1) #b x src_len\n","        '''\n","        #B. using a for loop for sanity check\n","        e_t = []\n","        for i in range(enc_hiddens_proj.shape[0]):\n","            var_a = dec_hidden[i:i+1,:] #1xh\n","            var_b = enc_hiddens_proj[i,:,:] #src_len x h\n","            e_t_tmp = tf.linalg.matmul(var_a, var_b, transpose_b=True) #1 x src_len\n","            e_t.append(e_t_tmp)\n","        e_t = tf.concat(e_t, axis=0) #b x src_len\n","        '''      \n","        ### END YOUR CODE\n","\n","        #e_t is b x src_len\n","        # Set e_t to -inf where enc_masks has True else keep as is\n","        if enc_masks is not None:\n","            e_t = tf.where(enc_masks, -float('inf'), e_t)\n","\n","        ### YOUR CODE HERE (~6 Lines)\n","        ### TODO:\n","        ###     1. Apply softmax to e_t to yield alpha_t\n","        ###     2. Use batched matrix multiplication between alpha_t and enc_hiddens to obtain the\n","        ###         attention output vector, a_t.\n","        #$$     Hints:\n","        ###           - alpha_t is shape (b, src_len)\n","        ###           - enc_hiddens is shape (b, src_len, 2h)\n","        ###           - a_t should be shape (b, 2h)\n","        ###           - You will need to do some squeezing and unsqueezing.\n","        ###     Note: b = batch size, src_len = maximum source length, h = hidden size.\n","        ###\n","        ###     3. Concatenate dec_hidden with a_t to compute tensor U_t\n","        ###     4. Apply the combined output projection layer to U_t to compute tensor V_t\n","        ###     5. Compute tensor O_t by first applying the Tanh function and then the dropout layer.\n","        ###\n","        ### Use the following docs to implement this functionality:\n","        ###     Softmax:\n","        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.functional.softmax\n","        ###     Batch Multiplication:\n","        ###        https://pytorch.org/docs/stable/torch.html#torch.bmm\n","        ###     Tensor View:\n","        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n","        ###     Tensor Concatenation:\n","        ###         https://pytorch.org/docs/stable/torch.html#torch.cat\n","        ###     Tanh:\n","        ###         https://pytorch.org/docs/stable/torch.html#torch.tanh\n","\n","\n","        #1. Apply softmax (to get attention weights)\n","        alpha_t = K.activations.softmax(e_t, axis=-1) #b x src_len\n","\n","        #2. Obtain attention output vector a_t\n","        #enc_hiddens is of shape (b, src_len, 2h)\n","        var_a = tf.transpose(enc_hiddens, perm=[0,2,1]) #(b, 2h, src_len) (DON'T USE RESHAPE AS IT WILL GIVE WRONG RESULTS WHEN COMBINED WITH MATRIX MULTIPLICATION)\n","        alpha_t = tf.expand_dims(alpha_t, axis=2) #(b, src_len, 1)\n","        a_t = tf.linalg.matmul(var_a, alpha_t) #(b, 2h, 1)\n","        a_t = tf.squeeze(a_t, axis=2) #(b, 2h)\n","\n","        #3. compute ut\n","        #dec_hidden is (b, h)\n","        u_t = tf.concat([a_t, dec_hidden], axis=1) #(b, 3h)\n","\n","        #4. Compute V_t by applying the combined output projection layer\n","        V_t = self.combined_output_projection(u_t) #(b, h)\n","\n","        #5. Compute O_t\n","        O_t = tf.nn.tanh(V_t)\n","        O_t = self.dropout(O_t, training=self.training) #(b,h)\n","\n","        ### END YOUR CODE\n","\n","        combined_output = O_t\n","        return dec_state, combined_output, e_t\n","\n","\n","    def generate_sent_masks(self, enc_hiddens, source_lengths):\n","        \"\"\" Generate sentence masks for encoder hidden states.\n","\n","        @param enc_hiddens (Tensor): encodings of shape (b, src_len, 2*h), where b = batch size,\n","                                     src_len = max source length, h = hidden size. \n","        @param source_lengths (List[int]): List of actual lengths for each of the sentences in the batch.\n","        \n","        @returns enc_masks (Tensor): Tensor of sentence masks of shape (b, src_len),\n","                                    where src_len = max source length, h = hidden size.\n","        \"\"\"\n","\n","        enc_masks = tf.zeros(shape=enc_hiddens.shape[0:2], dtype=tf.float32) #b x src_len\n","        enc_masks = tf.Variable(enc_masks)\n","        for e_id, src_len in enumerate(source_lengths):\n","            #enc_masks[e_id, src_len:] = 1 #can't do this as variable tensors don't support assignment operator as well as broadcating (see below)\n","            enc_masks[e_id, src_len:].assign(tf.broadcast_to(1.0, shape=[enc_hiddens.shape[1]-src_len,]))\n","\n","        with tf.device(self.tf_device):\n","            enc_masks = tf.constant(enc_masks.numpy()) #tf.constant is more memory efficient\n","            #enc_masks = tf.Variable(enc_masks) #tf.Variable takes more memory\n","            enc_masks = tf.cast(enc_masks, dtype=tf.bool)\n","        \n","        #print(enc_masks.device)\n","        return enc_masks #(b,src_len)\n","\n","\n","    def beam_search(self, src_sent, beam_size, max_decoding_time_step=50):\n","        #code provided with the assignment (converted to TF from Pytorch)\n","        \"\"\" Given a single source sentence, perform beam search, yielding translations in the target language.\n","        @param src_sent (List[str]): a single source sentence (words)\n","        @param beam_size (int): beam size\n","        @param max_decoding_time_step (int): maximum number of time steps to unroll the decoding RNN\n","        @returns hypotheses (List[Hypothesis]): a list of hypothesis, each hypothesis has two fields:\n","                value: List[str]: the decoded target sentence, represented as a list of words\n","                score: float: the log-likelihood of the target sentence\n","        \"\"\"\n","        src_sents_var = self.vocab.src.to_input_tensor([src_sent], self.device) #(1,sen_len)\n","\n","        src_encodings, dec_init_vec = self.encode(src_sents_var, [len(src_sent)]) #src_encoddings = (1,sen_len,h)\n","        src_encodings_att_linear = self.att_projection(src_encodings) #(1,sen_len,h)\n","\n","        h_tm1 = dec_init_vec #tuple of 1xh\n","        with tf.device(self.device):\n","            att_tm1 = tf.zeros([1, self.hidden_size])\n","\n","        eos_id = self.vocab.tgt['</s>']\n","\n","        hypotheses = [['<s>']]\n","        with tf.device(self.device):\n","            hyp_scores = tf.zeros(len(hypotheses), dtype=tf.float32)\n","        completed_hypotheses = []\n","\n","        t = 0\n","        while len(completed_hypotheses) < beam_size and t < max_decoding_time_step:\n","            t += 1\n","            hyp_num = len(hypotheses)\n","\n","            exp_src_encodings = tf.broadcast_to(src_encodings, shape=[hyp_num] + src_encodings.shape[1:]) #(hyp_num,sen_len,2h)\n","            exp_src_encodings_att_linear = tf.broadcast_to(src_encodings_att_linear, [hyp_num] + src_encodings_att_linear.shape[1:]) #(hyp_num, sen_len, h)\n","\n","            y_tm1 = tf.constant([self.vocab.tgt[hyp[-1]] for hyp in hypotheses], dtype=tf.float32)\n","            y_t_embed = self.model_embeddings.target(y_tm1)\n","\n","            x = tf.concat([y_t_embed, att_tm1], axis=-1)\n","\n","            (h_t, cell_t), att_t, _  = self.step(x, h_tm1,\n","                                                      exp_src_encodings, exp_src_encodings_att_linear, enc_masks=None)\n","\n","            # log probabilities over target words\n","            log_p_t = tf.nn.log_softmax(self.target_vocab_projection(att_t), axis=-1)\n","\n","            live_hyp_num = beam_size - len(completed_hypotheses)\n","            #contiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand_as(log_p_t) + log_p_t).view(-1)\n","            contiuating_hyp_scores = tf.reshape(tf.broadcast_to(tf.expand_dims(hyp_scores, 1), log_p_t.shape) + \n","                                                 log_p_t, [-1])\n","\n","            #top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(contiuating_hyp_scores, k=live_hyp_num)\n","            top_cand_hyp_pos = tf.argsort(contiuating_hyp_scores, axis=-1, direction='DESCENDING')[0:live_hyp_num].numpy()\n","            top_cand_hyp_scores = contiuating_hyp_scores.numpy()[top_cand_hyp_pos]\n","\n","            prev_hyp_ids = top_cand_hyp_pos // len(self.vocab.tgt)\n","            hyp_word_ids = top_cand_hyp_pos % len(self.vocab.tgt)\n","\n","            new_hypotheses = []\n","            live_hyp_ids = []\n","            new_hyp_scores = []\n","\n","            for prev_hyp_id, hyp_word_id, cand_new_hyp_score in zip(prev_hyp_ids, hyp_word_ids, top_cand_hyp_scores):\n","                prev_hyp_id = prev_hyp_id.item()\n","                hyp_word_id = hyp_word_id.item()\n","                cand_new_hyp_score = cand_new_hyp_score.item()\n","\n","                hyp_word = self.vocab.tgt.id2word[hyp_word_id]\n","                new_hyp_sent = hypotheses[prev_hyp_id] + [hyp_word]\n","                if hyp_word == '</s>':\n","                    completed_hypotheses.append(Hypothesis(value=new_hyp_sent[1:-1],\n","                                                           score=cand_new_hyp_score))\n","                else:\n","                    new_hypotheses.append(new_hyp_sent)\n","                    live_hyp_ids.append(prev_hyp_id)\n","                    new_hyp_scores.append(cand_new_hyp_score)\n","\n","            if len(completed_hypotheses) == beam_size:\n","                break\n","\n","            h_tm1 = (tf.constant(h_t.numpy()[live_hyp_ids]), tf.constant(cell_t.numpy()[live_hyp_ids]))\n","            att_tm1 = tf.constant(att_t.numpy()[live_hyp_ids])\n","\n","            hypotheses = new_hypotheses\n","            with tf.device(self.device):\n","                hyp_scores = tf.constant(new_hyp_scores, dtype=tf.float32).numpy()\n","\n","        if len(completed_hypotheses) == 0:\n","            completed_hypotheses.append(Hypothesis(value=hypotheses[0][1:],\n","                                                   score=hyp_scores[0].item()))\n","\n","        completed_hypotheses.sort(key=lambda hyp: hyp.score, reverse=True)\n","\n","        return completed_hypotheses\n","\n","\n","    def uniformly_initialize_layers(self, uniform_init):\n","        \"\"\" Reinitialize the Layer Weights for Sanity Checks.\n","        Amit added this for TF\n","        \"\"\"\n","        def initialize_layer(l):\n","            for i in range(len(l.variables)):\n","                init_val = tf.random.uniform(l.variables[i].shape, -uniform_init, uniform_init)\n","                l.variables[i].assign(init_val)\n","\n","        e, h = self.model_embeddings.embed_size, self.hidden_size\n","        input_dims = [(1,1), (1,1,e), (1,1,(e+h)), (1,2*h), (1,2*h), (1,2*h), (1,3*h), (1,h), None]\n","        for l_num,l in enumerate(self.layers):\n","            l.build(input_dims[l_num])\n","            initialize_layer(l)\n","\n","\n","    @property\n","    def device(self):\n","        \"\"\" Determine which device to place the Tensors upon, CPU or GPU.\n","        \"\"\"\n","        return self.tf_device\n","\n","\n","    @staticmethod\n","    def load(path, vocab=VOCABULARY):\n","        #Loads the model weights and model hyper parameters\n","        \"\"\" Load the model from a file.\n","        @param path (str): path to model\n","        \"\"\"\n","        #1. build model using the saved hyper parameters\n","        print('restore model hyper parameters', file=sys.stderr)\n","        with open(path + '_model_hyper_params.json', 'r') as f:\n","            hyper_params = json.load(f)\n","        args = hyper_params['args']\n","        model = NMT(vocab=vocab, **args)\n","        model.uniformly_initialize_layers(0.1) #to build it\n","\n","        #2. load model weights\n","        print('restore model weights', file=sys.stderr)\n","        model.load_weights(path)\n","\n","        return model\n","\n","    def save(self, path):\n","        #Saves the model weights and model hyper parameters\n","        \"\"\" Save the odel to a file.\n","        @param path (str): path to the model\n","        \"\"\"\n","\n","        #1. Save model weights\n","        print('save model parameters to [%s]' % path, file=sys.stderr)\n","        self.save_weights(path)\n","\n","        #2. Save the model_hyper_params\n","        hyper_params = {\n","            'args': dict(embed_size=self.model_embeddings.embed_size, hidden_size=self.hidden_size, dropout_rate=self.dropout_rate),\n","        }\n","        with open(path + '_model_hyper_params.json', 'w') as f:\n","            json.dump(hyper_params, f)\n","\n","    #Amit ToDo\n","    def no_beam_search(self, src_sent, max_decoding_time_step=50):\n","        #Done by Amit\n","        \"\"\" Given a single source sentence, perform beam search, yielding translations in the target language.\n","        @param src_sent (List[str]): a single source sentence (words)\n","        @param max_decoding_time_step (int): maximum number of time steps to unroll the decoding RNN\n","        @returns hypotheses (List[Hypothesis]): a list of hypothesis, each hypothesis has two fields:\n","                value: List[str]: the decoded target sentence, represented as a list of words\n","                score: float: the log-likelihood of the target sentence\n","        \"\"\"\n","        self.training = False\n","        src_sents_var = self.vocab.src.to_input_tensor([src_sent], self.device) #(1,src_len)\n","\n","        src_encodings, dec_init_vec = self.encode(src_sents_var, [len(src_sent)]) #(1,src_len,2h), ((1,h),(1,h))\n","        src_encodings_prj = self.att_projection(src_encodings) #(1,src_len,h)\n","\n","        h_tm1 = dec_init_vec #tuple of 1xh\n","        with tf.device(self.device):\n","            o_tm1 = tf.zeros([1, self.hidden_size])\n","        bos = tf.constant([self.vocab.tgt['<s>']]) #(1,1)\n","        y_t = self.model_embeddings.target(bos) #(1,embed_dim)\n","        y_t = tf.concat([y_t, o_tm1], axis=-1) #(1,embed_dim+h)\n","        res_tgt = []\n","        eos_id = self.vocab.tgt['</s>']\n","        #print(eos_id)\n","\n","        for t in range(max_decoding_time_step):\n","            '''\n","            #self.step() does this\n","            _, h_t = self.decoder(y_t, h_tm1) #h_t = tuple of 1xh\n","            e_t = tf.linalg.matmul(h_t[0], tf.transpose(src_encodings_prj[0,:,:])) #(1,src_len)\n","            alpha_t = tf.keras.activations.softmax(e_t, axis=-1) #(1,src_len)\n","            a_t = tf.linalg.matmul(alpha_t, src_encodings[0,:,:]) #(1,2h)\n","            u_t = tf.concat([a_t, h_t[0]], axis=-1) #(1,3h)\n","            v_t = self.combined_output_projection(u_t) #(1,h)\n","            o_t = tf.nn.tanh(v_t)\n","            o_t = self.dropout(o_t, training=self.training) #(1,h)\n","            '''\n","\n","            h_t, o_t, _ = self.step(y_t, h_tm1, src_encodings, src_encodings_prj)\n","\n","            p_t = self.target_vocab_projection(o_t) #(1,vocab)\n","            p_t = tf.nn.log_softmax(p_t, axis=-1) #(1,vocab)\n","            p_t_idx_sorted = tf.argsort(p_t, -1, 'DESCENDING')\n","            if p_t_idx_sorted[0][0] == eos_id:\n","                break\n","            idx_t = p_t_idx_sorted[0][0].numpy()\n","            res_tgt.append(self.vocab.tgt.id2word[idx_t])           \n","\n","            h_tm1 = h_t\n","            y_t = tf.constant([idx_t]) #(1,1)\n","            y_t = self.model_embeddings.target(y_t) #(1,embed_dim)\n","            y_t = tf.concat([y_t, o_t], axis=-1) #(1,embed_dim+h)\n","\n","        #res_tgt = \" \".join(res_tgt)\n","        #print(res_tgt)\n","        #print(t)\n","        res_tgt = Hypothesis(value=res_tgt, score=0.0)\n","\n","        return [res_tgt]\n","\n","    #Amit ToDo\n","    def amit_beam_search(self, src_sent, beam_size=3, max_decoding_time_step=50):\n","        #Done by Amit (Note: this is much slower than the verison provided with the assignment i.e. beam_search() but the they both yield the same results)\n","        \"\"\" Given a single source sentence, perform beam search, yielding translations in the target language.\n","        @param src_sent (List[str]): a single source sentence (words)\n","        @param beam_size (int): beam size for beam search\n","        @param max_decoding_time_step (int): maximum number of time steps to unroll the decoding RNN\n","        @returns hypotheses (List[Hypothesis]): a list of hypothesis, each hypothesis has two fields:\n","                value: List[str]: the decoded target sentence, represented as a list of words\n","                score: float: the log-likelihood of the target sentence\n","        \"\"\"\n","        self.training = False\n","        src_sents_var = self.vocab.src.to_input_tensor([src_sent], self.device) #(1,src_len)\n","\n","        src_encodings, dec_init_vec = self.encode(src_sents_var, [len(src_sent)]) #(1,src_len,2h), ((1,h),(1,h))\n","        src_encodings_prj = self.att_projection(src_encodings) #(1,src_len,h)\n","\n","        h_tm1 = dec_init_vec #tuple of 1xh\n","        with tf.device(self.device):\n","            o_tm1 = tf.zeros([1, self.hidden_size])\n","        bos = tf.constant([self.vocab.tgt['<s>']]) #(1,1)\n","        y_t = self.model_embeddings.target(bos) #(1,embed_dim)\n","        y_t = tf.concat([y_t, o_tm1], axis=-1) #(1,embed_dim+h)\n","        eos_tag = '</s>'\n","        eos_id = self.vocab.tgt[eos_tag]\n","        #print(eos_id)\n","        hyp = {'prob':0, 'h_tm1':h_tm1, 'y_t':y_t, 'tgt_output': []}\n","        hypotheses = [hyp]\n","        for t in range(max_decoding_time_step):\n","            temp_hypotheses = [] #will have a max size of beam_size^2\n","            for hyp in hypotheses:\n","                if len(hyp['tgt_output']) > 0 and hyp['tgt_output'][-1] == eos_tag:\n","                    temp_hypotheses.append(hyp)\n","                    continue #go to next hyp\n","                h_t, o_t, _ = self.step(hyp['y_t'], hyp['h_tm1'], src_encodings, src_encodings_prj)\n","\n","                p_t = self.target_vocab_projection(o_t) #(1,vocab)\n","                p_t = tf.nn.log_softmax(p_t, axis=-1) #(1,vocab)\n","                p_t_idx_sorted = tf.argsort(p_t, -1, 'DESCENDING')\n","\n","                for k in range(beam_size):\n","                    idx_t = p_t_idx_sorted[0][k].numpy()\n","                    new_hyp = {}\n","                    new_hyp['prob'] = hyp['prob']+p_t[0][idx_t].numpy()\n","                    new_hyp['h_tm1'] = h_t\n","                    y_t = tf.constant([idx_t]) #(1,1)\n","                    y_t = self.model_embeddings.target(y_t) #(1,embed_dim)\n","                    y_t = tf.concat([y_t, o_t], axis=-1) #(1,embed_dim+h)\n","                    new_hyp['y_t'] = y_t\n","                    new_hyp['tgt_output'] = hyp['tgt_output'][:] #deep copy\n","                    new_hyp['tgt_output'].append(self.vocab.tgt.id2word[idx_t])\n","                    temp_hypotheses.append(new_hyp)\n","\n","            temp_hypotheses = sorted(temp_hypotheses, reverse=True, key=lambda x: x['prob'])\n","            hypotheses = temp_hypotheses[0:beam_size]\n","            if hypotheses[0]['tgt_output'][-1] == eos_tag:\n","                res_tgt = Hypothesis(value=hypotheses[0]['tgt_output'][0:-1], score=hypotheses[0]['prob']) #ignore eos_tag\n","                break\n","            \n","        if t == max_decoding_time_step-1:\n","            res_tgt = Hypothesis(value=hypotheses[0]['tgt_output'], score=hypotheses[0]['prob']) #no eos_tag\n","\n","\n","        #print(t)\n","        #res_tgt = \" \".join(res_tgt)\n","        #print(res_tgt)\n","        return [res_tgt]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xP-Jtwd8TSkf","colab_type":"code","colab":{}},"source":["def dummy_decode(test_source_file, test_target_file, model_save_path='./saved_model/model'):\n","    #load the best saved model\n","    print('load the previously best saved model', file=sys.stderr)\n","    model = NMT.load(model_save_path)\n","\n","    print(\"load test source sentences from [{}]\".format(test_source_file))\n","    test_data_src = read_corpus(test_source_file, source='src')\n","    print(\"load test target sentences from [{}]\".format(test_target_file))\n","    test_data_tgt = read_corpus(test_target_file, source='tgt')\n","    print(len(test_data_src), len(test_data_tgt))\n","\n","    for idx, sent in enumerate(test_data_src[0:1]):\n","        #print(sent)\n","        with tf.device(model.device):\n","            print(model.no_beam_search(sent))\n","            print(model.amit_beam_search(sent, beam_size=10))\n","            print(model.beam_search(sent, beam_size=10))\n","            print(test_data_tgt[idx])\n","\n","#dummy_decode('./a4/en_es_data/dev.es', './a4/en_es_data/dev.en')\n","#print()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TNM-nXNJ09ZN","colab_type":"text"},"source":["##4. SANITY CHECK"]},{"cell_type":"code","metadata":{"id":"_vPx6fTDt8YH","colab_type":"code","outputId":"bd4a6e7e-afd3-42c3-cf47-115febc2b982","executionInfo":{"status":"ok","timestamp":1573583987119,"user_tz":480,"elapsed":12489,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"}},"colab":{"base_uri":"https://localhost:8080/","height":150}},"source":["#Pytorch Model for Debug Only and initialize the weights for the LSTMCell and Bidirectional LSTM in Tensorflow (as it is randomly initialized)\n","#Amit added this\n","\n","#----------\n","# CONSTANTS\n","#----------\n","BATCH_SIZE = 5\n","EMBED_SIZE = 3\n","HIDDEN_SIZE = 3\n","DROPOUT_RATE = 0.0\n","NUM_LAYERS = 1\n","\n","import torch\n","class Pytorch_NMT(torch.nn.Module):\n","    def __init__(self, embed_size, hidden_size, vocab, dropout_rate=0.2, device='cpu:0'):\n","        super(Pytorch_NMT, self).__init__()\n","        self.embed_src = torch.nn.Embedding(len(vocab.src), embed_size, padding_idx=0)\n","        self.embed_tgt = torch.nn.Embedding(len(vocab.tgt), embed_size, padding_idx=0)\n","        self.encoder = torch.nn.LSTM(embed_size, hidden_size, num_layers=1, batch_first=True, bidirectional=True)\n","        #self.encoder = torch.nn.LSTM(embed_size, hidden_size, num_layers=1, batch_first=True)\n","        self.decoder = torch.nn.LSTMCell(embed_size+hidden_size, hidden_size)\n","        self.h_projection = torch.nn.Linear(2*hidden_size, hidden_size, bias=False)\n","        self.vocab = vocab\n","        self.device = device\n","\n","    def encode(self, source_padded, source_lengths):\n","        embed = self.embed_src(source_padded)\n","        pack_source_padded = torch.nn.utils.rnn.pack_padded_sequence(embed, source_lengths, batch_first=True, enforce_sorted=False)\n","        s_h = torch.zeros((NUM_LAYERS*2, BATCH_SIZE, HIDDEN_SIZE))\n","        s_c = torch.zeros((NUM_LAYERS*2, BATCH_SIZE, HIDDEN_SIZE))        \n","        out, state = self.encoder(pack_source_padded, [s_h,s_c])\n","        out = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)[0]\n","\n","        last_hidden_fwd = state[0][0,:,:] #bxh\n","        last_hidden_bck = state[0][1,:,:] #bxh\n","        last_cell_fwd = state[1][0,:,:] #bxh\n","        last_cell_bck = state[1][1,:,:] #bxh\n","        h0_dec = torch.cat([last_hidden_fwd, last_hidden_bck], axis=1) #b x 2*hidden_size\n","        h0_dec = self.h_projection(h0_dec) #b x hidden_size\n","        c0_dec = torch.cat([last_cell_fwd, last_cell_bck], axis=1) #b x 2*hidden_size\n","        c0_dec = self.h_projection(c0_dec) #b x hidden_size\n","        dec_init_state = (h0_dec, c0_dec)\n","\n","        # print(out.shape)\n","        # print(state[0].shape)\n","        # print(state[1].shape)\n","        return out, dec_init_state\n","\n","\n","def reinitialize_layers(model):\n","    \"\"\" Reinitialize the Layer Weights for Sanity Checks.\n","    \"\"\"\n","    def init_weights(m):\n","        if type(m) == torch.nn.Linear:\n","            m.weight.data.fill_(0.3)\n","            if m.bias is not None:\n","                m.bias.data.fill_(0.1)\n","        elif type(m) == torch.nn.Embedding:\n","            m.weight.data.fill_(0.15)\n","        elif type(m) == torch.nn.Dropout:\n","            torch.nn.Dropout(DROPOUT_RATE)\n","    with torch.no_grad():\n","        model.apply(init_weights)\n","\n","\n","def question_1d_sanity_check(model, src_sents, tgt_sents, vocab):\n","    \"\"\" Sanity check for question 1d. \n","        Compares student output to that of model with dummy data.\n","    \"\"\"\n","    print(\"Running Sanity Check for Question 1d: Encode\")\n","    print (\"-\"*80)\n","\n","    # Configure for Testing\n","    reinitialize_layers(model)\n","    source_lengths = [len(s) for s in src_sents]\n","    source_padded = model.vocab.src.to_input_tensor(src_sents, device=model.device)\n","    source_padded = torch.tensor(source_padded.numpy())\n","\n","    # Load Outputs\n","    enc_hiddens_target = torch.load('./a4/sanity_check_en_es_data/enc_hiddens.pkl')\n","    dec_init_state_target = torch.load('./a4/sanity_check_en_es_data/dec_init_state.pkl')\n","\n","    # Test\n","    with torch.no_grad():\n","        enc_hiddens_pred, dec_init_state_pred = model.encode(source_padded, source_lengths)\n","    \n","    assert(np.allclose(enc_hiddens_target.numpy(), enc_hiddens_pred.numpy())), \"enc_hiddens is incorrect: it should be:\\n {} but is:\\n{}\".format(enc_hiddens_target, enc_hiddens_pred)\n","    print(\"enc_hiddens Sanity Checks Passed!\")\n","    assert(np.allclose(dec_init_state_target[0].numpy(), dec_init_state_pred[0].numpy())), \"dec_init_state[0] is incorrect: it should be:\\n {} but is:\\n{}\".format(dec_init_state_target[0], dec_init_state_pred[0])\n","    print(\"dec_init_state[0] Sanity Checks Passed!\")\n","    assert(np.allclose(dec_init_state_target[1].numpy(), dec_init_state_pred[1].numpy())), \"dec_init_state[1] is incorrect: it should be:\\n {} but is:\\n{}\".format(dec_init_state_target[1], dec_init_state_pred[1])\n","    print(\"dec_init_state[1] Sanity Checks Passed!\")\n","    print (\"-\"*80)\n","    print(\"All Sanity Checks Passed for Question 1d: Encode!\")\n","    print (\"-\"*80)\n","\n","\n","def generate_outputs(model, source, target, vocab):\n","    \"\"\" Generate outputs.\n","    \"\"\"\n","    print (\"-\"*80)\n","    print(\"Generating Comparison Outputs\")\n","    reinitialize_layers(model)\n","\n","    # Compute sentence lengths\n","    source_lengths = [len(s) for s in source]\n","\n","    # Convert list of lists into tensors\n","    source_padded = model.vocab.src.to_input_tensor(source, device=model.device)\n","    source_padded = torch.tensor(source_padded.numpy())\n","    target_padded = model.vocab.tgt.to_input_tensor(target, device=model.device)\n","    target_padded = torch.tensor(target_padded.numpy())\n","\n","    # Run the model forward\n","    enc_hiddens, dec_init_state = model.encode(source_padded, source_lengths)\n","    #enc_masks = model.generate_sent_masks(enc_hiddens, source_lengths)\n","    #combined_outputs = model.decode(enc_hiddens, enc_masks, dec_init_state, targe\n","\n","\n","def main_sanity_check():\n","    \"\"\" Main function for sanity checking\n","    \"\"\"\n","    # Seed the Random Number Generators\n","    seed = 1234\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    np.random.seed(seed * 13 // 7)\n","\n","    # Load training data & vocabulary\n","    train_data_src = read_corpus('./a4/sanity_check_en_es_data/train_sanity_check.es', 'src')\n","    train_data_tgt = read_corpus('./a4/sanity_check_en_es_data/train_sanity_check.en', 'tgt')\n","    train_data = list(zip(train_data_src, train_data_tgt))\n","\n","    for src_sents, tgt_sents in batch_iter(train_data, batch_size=BATCH_SIZE, shuffle=True):\n","        src_sents = src_sents\n","        tgt_sents = tgt_sents\n","        break\n","    vocab = Vocab.load('./a4/sanity_check_en_es_data/vocab_sanity_check.json') \n","\n","    # Create NMT Model\n","    model_pytorch = Pytorch_NMT(EMBED_SIZE, HIDDEN_SIZE, vocab)\n","\n","    question_1d_sanity_check(model_pytorch, src_sents, tgt_sents, vocab)\n","    #print(model_pytorch.decoder.state_dict())\n","    #question_1e_sanity_check(model_pytorch, src_sents, tgt_sents, vocab)\n","    #question_1f_sanity_check(model_pytorch, src_sents, tgt_sents, vocab)\n","    #generate_outputs(model_pytorch, src_sents, tgt_sents, vocab)\n","\n","    return model_pytorch\n","\n","model_pytorch = main_sanity_check() #used to initialize the weights for the LSTMCell and BidirectionalLSTM in Tensorflow (as it is randomly initialized)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Running Sanity Check for Question 1d: Encode\n","--------------------------------------------------------------------------------\n","enc_hiddens Sanity Checks Passed!\n","dec_init_state[0] Sanity Checks Passed!\n","dec_init_state[1] Sanity Checks Passed!\n","--------------------------------------------------------------------------------\n","All Sanity Checks Passed for Question 1d: Encode!\n","--------------------------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zq09bChi1BK-","colab_type":"code","outputId":"9d74d3eb-dfe8-443b-ace9-8de345e612a2","executionInfo":{"status":"ok","timestamp":1573584018498,"user_tz":480,"elapsed":30175,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"}},"colab":{"base_uri":"https://localhost:8080/","height":435}},"source":["#sanity_check.py\n","\n","\"\"\"\n","CS224N 2018-19: Homework 4\n","sanity_check.py: sanity checks for assignment 4\n","Sahil Chopra <schopra8@stanford.edu>\n","Michael Hahn <>\n","Amit modified it for TF\n","\n","Usage:\n","    sanity_check.py 1d\n","    sanity_check.py 1e\n","    sanity_check.py 1f\n","\n","\"\"\"\n","import numpy as np\n","import tensorflow as tf\n","import torch #to load the target tensors\n","\n","\n","#----------\n","# CONSTANTS\n","#----------\n","BATCH_SIZE = 5\n","EMBED_SIZE = 3\n","HIDDEN_SIZE = 3\n","DROPOUT_RATE = 0.0\n","\n","def save_tf_vars(vars, path, use_list):\n","    #saving tf variables (assume the variables are numpy arrays or Python native numeric types. But can be inside a list.)\n","    if use_list:\n","        chkpt = tf.train.Checkpoint()\n","        chkpt.listed = [tf.Variable(v) for v in vars]\n","        tf.saved_model.save(chkpt, path)\n","    else:\n","        tf.saved_model.save(tf.Variable(vars), path)\n","\n","def load_tf_vars(path, use_list):\n","    #loading saved tf variables\n","    restored = tf.saved_model.load(path)\n","    if use_list:\n","        restored = list(restored.listed)\n","    return restored\n","\n","\n","def reinitialize_layers(model):\n","    \"\"\" Reinitialize the Layer Weights for Sanity Checks.\n","    \"\"\"\n","    def init_dense(l):\n","        for i in range(len(l.variables)):\n","            if len(l.variables[i].shape) == 1: #bias kernel\n","                init_val = tf.fill(l.variables[i].shape, 0.1)\n","                l.variables[i].assign(init_val)\n","            else:\n","                init_val = tf.fill(l.variables[i].shape, 0.3) #weights kernel\n","                l.variables[i].assign(init_val)\n","\n","    e, h = EMBED_SIZE, HIDDEN_SIZE\n","    input_dims = [(1,1), (1,1,e), (1,1,(e+h)), (1,2*h), (1,2*h), (1,2*h), (1,3*h), (1,h), None]\n","    for l_num,l in enumerate(model.layers):\n","        if type(l) == tf.keras.layers.Dense:\n","            l.build(input_dims[l_num])\n","            init_dense(l)\n","        elif type(l) == ModelEmbeddings:\n","            l.source.build(input_dims[l_num])\n","            init_val = tf.fill(l.source.variables[0].shape, 0.15) #has only 1 variable i.e. embeddings matrix\n","            l.source.variables[0].assign(init_val)\n","            #\n","            l.target.build(input_dims[l_num])\n","            init_val = tf.fill(l.target.variables[0].shape, 0.15) #has only 1 variable i.e. embeddings matrix\n","            l.target.variables[0].assign(init_val)\n","        elif type(l) == tf.keras.layers.Dropout:\n","            l = tf.keras.layers.Dropout(DROPOUT_RATE) #works because lists are mutable\n","        elif type(l) == tf.keras.layers.Bidirectional:\n","            l.build(input_dims[l_num])\n","            tf_vars = l.variables \n","            py_trch = model_pytorch.encoder.state_dict()\n","            tf_vars[0].assign(tf.Variable(np.transpose(py_trch['weight_ih_l0'].numpy())))\n","            tf_vars[1].assign(tf.Variable(np.transpose(py_trch['weight_hh_l0'].numpy())))\n","            tf_vars[2].assign(tf.Variable((py_trch['bias_ih_l0'] + py_trch['bias_hh_l0']).numpy()))\n","            tf_vars[3].assign(tf.Variable(np.transpose(py_trch['weight_ih_l0_reverse'].numpy())))\n","            tf_vars[4].assign(tf.Variable(np.transpose(py_trch['weight_hh_l0_reverse'].numpy())))\n","            tf_vars[5].assign(tf.Variable((py_trch['bias_ih_l0_reverse'] + py_trch['bias_hh_l0_reverse']).numpy()))\n","        elif type(l) == tf.keras.layers.LSTMCell:\n","            l.build(input_dims[l_num])\n","            tf_vars = l.variables \n","            py_trch = model_pytorch.decoder.state_dict()\n","            tf_vars[0].assign(tf.Variable(np.transpose(py_trch['weight_ih'].numpy())))\n","            tf_vars[1].assign(tf.Variable(np.transpose(py_trch['weight_hh'].numpy())))\n","            tf_vars[2].assign(tf.Variable((py_trch['bias_ih'] + py_trch['bias_hh']).numpy()))\n","        else:\n","            pass #don't do anything\n","\n","\n","def generate_outputs(model, source, target, vocab):\n","    \"\"\" Generate outputs.\n","    \"\"\"\n","    print (\"-\"*80)\n","    print(\"Generating Comparison Outputs\")\n","    reinitialize_layers(model)\n","\n","    # Compute sentence lengths\n","    source_lengths = [len(s) for s in source]\n","\n","    # Convert list of lists into tensors\n","    source_padded = model.vocab.src.to_input_tensor(source, device=model.device)\n","    target_padded = model.vocab.tgt.to_input_tensor(target, device=model.device)\n","\n","    # Run the model forward\n","    enc_hiddens, dec_init_state = model.encode(source_padded, source_lengths)\n","    enc_masks = model.generate_sent_masks(enc_hiddens, source_lengths)\n","    combined_outputs = model.decode(enc_hiddens, enc_masks, dec_init_state, target_padded)\n","\n","    # Save Tensors to disk\n","    save_tf_vars(enc_hiddens, './a4/sanity_check_en_es_data/Amit_output/enc_hiddens', use_list=False)\n","    save_tf_vars(dec_init_state, './a4/sanity_check_en_es_data/Amit_output/dec_init_state', use_list=True)\n","    save_tf_vars(enc_masks, './a4/sanity_check_en_es_data/Amit_output/enc_masks', use_list=False)\n","    save_tf_vars(combined_outputs, './a4/sanity_check_en_es_data/Amit_output/combined_outputs', use_list=False)\n","\n","    # Load Tensors from disk\n","    # print(load_tf_vars('./a4/sanity_check_en_es_data/Amit_output/enc_hiddens', use_list=False))\n","    # print(load_tf_vars('./a4/sanity_check_en_es_data/Amit_output/dec_init_state', use_list=True))\n","    # print(load_tf_vars('./a4/sanity_check_en_es_data/Amit_output/enc_masks', use_list=False))\n","    # print(load_tf_vars('./a4/sanity_check_en_es_data/Amit_output/combined_outputs', use_list=False))\n","\n","\n","def question_1d_sanity_check(model, src_sents, tgt_sents, vocab):\n","    \"\"\" Sanity check for question 1d. \n","        Compares student output to that of model with dummy data.\n","    \"\"\"\n","    print(\"Running Sanity Check for Question 1d: Encode\")\n","    print (\"-\"*80)\n","\n","    # Configure for Testing\n","    reinitialize_layers(model)\n","    source_lengths = [len(s) for s in src_sents]\n","    source_padded = model.vocab.src.to_input_tensor(src_sents, device=model.device)\n","\n","    # print(np.allclose(model.layers[1].variables[0].numpy(), \n","    #                           np.transpose(model_pytorch.encoder.state_dict()['weight_ih_l0'].numpy())))\n","            \n","    # Load Target Outputs (in PyTorch, as that's what it's saved as)\n","    enc_hiddens_target = torch.load('./a4/sanity_check_en_es_data/enc_hiddens.pkl')\n","    dec_init_state_target = torch.load('./a4/sanity_check_en_es_data/dec_init_state.pkl')\n","    #print(enc_hiddens_target.shape)\n","    #print(dec_init_state_target[0].shape)\n","    #print(dec_init_state_target[1].shape)\n","\n","    # Test\n","    enc_hiddens_pred, dec_init_state_pred = model.encode(source_padded, source_lengths)\n","    assert(np.allclose(enc_hiddens_target.numpy(), enc_hiddens_pred.numpy())), \"enc_hiddens is incorrect: it should be:\\n {} but is:\\n{}\".format(enc_hiddens_target, enc_hiddens_pred)\n","    print(\"enc_hiddens Sanity Checks Passed!\")\n","    assert(np.allclose(dec_init_state_target[0].numpy(), dec_init_state_pred[0].numpy())), \"dec_init_state[0] is incorrect: it should be:\\n {} but is:\\n{}\".format(dec_init_state_target[0], dec_init_state_pred[0])\n","    print(\"dec_init_state[0] Sanity Checks Passed!\")\n","    assert(np.allclose(dec_init_state_target[1].numpy(), dec_init_state_pred[1].numpy())), \"dec_init_state[1] is incorrect: it should be:\\n {} but is:\\n{}\".format(dec_init_state_target[1], dec_init_state_pred[1])\n","    print(\"dec_init_state[1] Sanity Checks Passed!\")\n","    print (\"-\"*80)\n","    print(\"All Sanity Checks Passed for Question 1d: Encode!\")\n","    print (\"-\"*80)\n","\n","\n","def question_1e_sanity_check(model, src_sents, tgt_sents, vocab):\n","    \"\"\" Sanity check for question 1e. \n","        Compares student output to that of model with dummy data.\n","    \"\"\"\n","    print (\"-\"*80)\n","    print(\"Running Sanity Check for Question 1e: Decode\")\n","    print (\"-\"*80)\n","\n","    # Load Inputs\n","    dec_init_state = torch.load('./a4/sanity_check_en_es_data/dec_init_state.pkl')\n","    dec_init_state = (tf.constant(dec_init_state[0].numpy()), tf.constant(dec_init_state[1].numpy()))\n","    enc_hiddens = torch.load('./a4/sanity_check_en_es_data/enc_hiddens.pkl')\n","    enc_hiddens = tf.constant(enc_hiddens.numpy())\n","    enc_masks = torch.load('./a4/sanity_check_en_es_data/enc_masks.pkl')\n","    enc_masks = tf.constant(enc_masks.numpy(), dtype=tf.bool)\n","    target_padded = torch.load('./a4/sanity_check_en_es_data/target_padded.pkl')\n","    target_padded = tf.constant(np.transpose(target_padded.numpy())) #so it's batch first\n","    # print(dec_init_state[0].shape, dec_init_state[1].shape)\n","    # print(enc_hiddens.shape)\n","    # print(enc_masks.shape)\n","    # print(target_padded.shape)\n","\n","    # Load Outputs\n","    combined_outputs_target = torch.load('./a4/sanity_check_en_es_data/combined_outputs.pkl').permute(1,0,2)\n","    #print(combined_outputs_target.shape)\n","\n","    # Configure for Testing\n","    reinitialize_layers(model)\n","    COUNTER = [0]\n","    def stepFunction(Ybar_t, dec_state, enc_hiddens, enc_hiddens_proj, enc_masks):\n","       dec_state = torch.load('./a4/sanity_check_en_es_data/step_dec_state_{}.pkl'.format(COUNTER[0]))\n","       dec_state = [tf.constant(dec_state[0].numpy()), tf.constant(dec_state[1].numpy())]\n","       o_t = torch.load('./a4/sanity_check_en_es_data/step_o_t_{}.pkl'.format(COUNTER[0]))\n","       o_t = tf.constant(o_t.numpy())\n","       COUNTER[0]+=1\n","       return dec_state, o_t, None\n","    temp = model.step\n","    model.step = stepFunction\n","\n","    # Run Tests\n","    combined_outputs_pred, _ = model.decode(enc_hiddens, enc_masks, dec_init_state, target_padded)\n","    model.step = temp\n","    #print(combined_outputs_pred[3], '\\n')\n","    #print(combined_outputs_target[3].numpy(), '\\n')\n","    #print(combined_outputs_pred[3].numpy()/combined_outputs_target[3].numpy(), '\\n')\n","    assert(np.allclose(combined_outputs_pred.numpy(), combined_outputs_target.numpy(), rtol=1e-1)), \"combined_outputs is incorrect: it should be:\\n {} but is:\\n{}\".format(combined_outputs_target, combined_outputs_pred)\n","    print(\"combined_outputs Sanity Checks Passed!\")\n","    print (\"-\"*80)\n","    print(\"All Sanity Checks Passed for Question 1e: Decode!\")\n","    print (\"-\"*80)\n","\n","def question_1f_sanity_check(model, src_sents, tgt_sents, vocab):\n","    \"\"\" Sanity check for question 1f. \n","        Compares student output to that of model with dummy data.\n","    \"\"\"\n","    print (\"-\"*80)\n","    print(\"Running Sanity Check for Question 1f: Step\")\n","    print (\"-\"*80)\n","    reinitialize_layers(model)\n","\n","    # Inputs\n","    Ybar_t = torch.load('./a4/sanity_check_en_es_data/Ybar_t.pkl')\n","    Ybar_t = tf.constant(Ybar_t.numpy())\n","    dec_init_state = torch.load('./a4/sanity_check_en_es_data/dec_init_state.pkl')\n","    dec_init_state = (tf.constant(dec_init_state[0].numpy()), tf.constant(dec_init_state[1].numpy()))\n","    enc_hiddens = torch.load('./a4/sanity_check_en_es_data/enc_hiddens.pkl')\n","    enc_hiddens = tf.constant(enc_hiddens.numpy())\n","    enc_masks = torch.load('./a4/sanity_check_en_es_data/enc_masks.pkl')\n","    enc_masks = tf.constant(enc_masks.numpy(), dtype=tf.bool)\n","    enc_hiddens_proj = torch.load('./a4/sanity_check_en_es_data/enc_hiddens_proj.pkl')\n","    enc_hiddens_proj = tf.constant(enc_hiddens_proj.numpy())\n","    # print(Ybar_t.shape)\n","    # print(dec_init_state[0].shape)\n","    # print(dec_init_state[1].shape)\n","    # print(enc_hiddens.shape)\n","    # print(enc_masks.shape)\n","    # print(enc_hiddens_proj.shape)\n","\n","    # Output\n","    dec_state_target = torch.load('./a4/sanity_check_en_es_data/dec_state.pkl')\n","    o_t_target = torch.load('./a4/sanity_check_en_es_data/o_t.pkl')\n","    e_t_target = torch.load('./a4/sanity_check_en_es_data/e_t.pkl')\n","    \n","    # Run Tests\n","    dec_state_pred, o_t_pred, e_t_pred= model.step(Ybar_t, dec_init_state, enc_hiddens, enc_hiddens_proj, enc_masks)\n","    # print(dec_state_pred[0].shape, dec_state_pred[1].shape)\n","    # print(o_t_pred.shape)\n","    # print(e_t_pred.shape)\n","    assert(np.allclose(dec_state_target[0].numpy(), dec_state_pred[0].numpy())), \"decoder_state[0] is incorrect: it should be:\\n {} but is:\\n{}\".format(dec_state_target[0], dec_state_pred[0])\n","    print(\"dec_state[0] Sanity Checks Passed!\")\n","    assert(np.allclose(dec_state_target[1].numpy(), dec_state_pred[1].numpy())), \"decoder_state[1] is incorrect: it should be:\\n {} but is:\\n{}\".format(dec_state_target[1], dec_state_pred[1])\n","    print(\"dec_state[1] Sanity Checks Passed!\")\n","    assert(np.allclose(o_t_target.numpy(), o_t_pred.numpy())), \"combined_output is incorrect: it should be:\\n {} but is:\\n{}\".format(o_t_target, o_t_pred)\n","    print(\"combined_output  Sanity Checks Passed!\")\n","    assert(np.allclose(e_t_target.numpy(), e_t_pred.numpy())), \"e_t is incorrect: it should be:\\n {} but is:\\n{}\".format(e_t_target, e_t_pred)\n","    print(\"e_t Sanity Checks Passed!\")\n","    print (\"-\"*80)    \n","    print(\"All Sanity Checks Passed for Question 1f: Step!\")\n","    print (\"-\"*80)\n","\n","\n","def main_sanity_check():\n","    \"\"\" Main function for sanity checking\n","    \"\"\"\n","    # Seed the Random Number Generators\n","    seed = 1234\n","    tf.random.set_seed(seed)\n","    np.random.seed(seed * 13 // 7)\n","\n","    # Load training data & vocabulary\n","    train_data_src = read_corpus('./a4/sanity_check_en_es_data/train_sanity_check.es', 'src')\n","    train_data_tgt = read_corpus('./a4/sanity_check_en_es_data/train_sanity_check.en', 'tgt')\n","    train_data = list(zip(train_data_src, train_data_tgt))\n","\n","    for src_sents, tgt_sents in batch_iter(train_data, batch_size=BATCH_SIZE, shuffle=True):\n","        src_sents = src_sents\n","        tgt_sents = tgt_sents\n","        break\n","    vocab = Vocab.load('./a4/sanity_check_en_es_data/vocab_sanity_check.json') \n","\n","    # Create NMT Model\n","    model = NMT(embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, \n","                dropout_rate=DROPOUT_RATE, vocab=vocab, device='cpu', training=True)\n","\n","    question_1d_sanity_check(model, src_sents, tgt_sents, vocab)\n","    question_1e_sanity_check(model, src_sents, tgt_sents, vocab)\n","    question_1f_sanity_check(model, src_sents, tgt_sents, vocab)\n","    #generate_outputs(model, src_sents, tgt_sents, vocab)\n","\n","main_sanity_check()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Running Sanity Check for Question 1d: Encode\n","--------------------------------------------------------------------------------\n","enc_hiddens Sanity Checks Passed!\n","dec_init_state[0] Sanity Checks Passed!\n","dec_init_state[1] Sanity Checks Passed!\n","--------------------------------------------------------------------------------\n","All Sanity Checks Passed for Question 1d: Encode!\n","--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------\n","Running Sanity Check for Question 1e: Decode\n","--------------------------------------------------------------------------------\n","combined_outputs Sanity Checks Passed!\n","--------------------------------------------------------------------------------\n","All Sanity Checks Passed for Question 1e: Decode!\n","--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------\n","Running Sanity Check for Question 1f: Step\n","--------------------------------------------------------------------------------\n","dec_state[0] Sanity Checks Passed!\n","dec_state[1] Sanity Checks Passed!\n","combined_output  Sanity Checks Passed!\n","e_t Sanity Checks Passed!\n","--------------------------------------------------------------------------------\n","All Sanity Checks Passed for Question 1f: Step!\n","--------------------------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qW4cXEjxDIYS","colab_type":"text"},"source":["##5. RUN"]},{"cell_type":"code","metadata":{"id":"RH7QmGVZDCFl","colab_type":"code","outputId":"006ee89e-a592-43bb-a3cc-b231c943edb7","executionInfo":{"status":"ok","timestamp":1576366543398,"user_tz":480,"elapsed":87608,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"}},"colab":{"base_uri":"https://localhost:8080/","height":183}},"source":["#run.py\n","\n","#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\n","\"\"\"\n","CS224N 2018-19: Homework 4\n","run.py: Run Script for Simple NMT Model\n","Pencheng Yin <pcyin@cs.cmu.edu>\n","Sahil Chopra <schopra8@stanford.edu>\n","Updated by Amit for TF 2.0\n","\n","Usage:\n","    run.py train --train-src=<file> --train-tgt=<file> --dev-src=<file> --dev-tgt=<file> --vocab=<file> [options]\n","    run.py decode [options] MODEL_PATH TEST_SOURCE_FILE OUTPUT_FILE\n","    run.py decode [options] MODEL_PATH TEST_SOURCE_FILE TEST_TARGET_FILE OUTPUT_FILE\n","\n","Options:\n","    --train-src=<file>                      train source file\n","    --train-tgt=<file>                      train target file\n","    --dev-src=<file>                        dev source file\n","    --dev-tgt=<file>                        dev target file\n","    --vocab=<file>                          vocab file\n","    --seed=<int>                            seed [default: 0]\n","    --batch-size=<int>                      batch size [default: 32]\n","    --embed-size=<int>                      embedding size [default: 256]\n","    --hidden-size=<int>                     hidden size [default: 256]\n","    --clip-grad=<float>                     gradient clipping [default: 5.0]\n","    --log-every=<int>                       log every [default: 10]\n","    --max-epoch=<int>                       max epoch [default: 30]\n","    --input-feed                            use input feeding\n","    --patience=<int>                        wait for how many iterations to decay learning rate [default: 5]\n","    --max-num-trial=<int>                   terminate training after how many trials [default: 5]\n","    --lr-decay=<float>                      learning rate decay [default: 0.5]\n","    --beam-size=<int>                       beam size [default: 5]\n","    --sample-size=<int>                     sample size [default: 5]\n","    --lr=<float>                            learning rate [default: 0.001]\n","    --uniform-init=<float>                  uniformly initialize all parameters [default: 0.1]\n","    --save-to=<file>                        model save path [default: model.bin]\n","    --valid-niter=<int>                     perform validation after how many iterations [default: 2000]\n","    --dropout=<float>                       dropout [default: 0.3]\n","    --max-decoding-time-step=<int>          maximum number of decoding time steps [default: 70]\n","\"\"\"\n","import math\n","import sys\n","import json\n","import time\n","from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n","import numpy as np\n","from typing import List, Tuple, Dict, Set, Union\n","from tqdm import tqdm\n","\n","\n","def evaluate_ppl(model, dev_data, batch_size=32):\n","    \"\"\" Evaluate perplexity on dev sentences\n","    @param model (NMT): NMT Model\n","    @param dev_data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n","    @param batch_size (batch size)\n","    @returns ppl (perplixty on dev sentences)\n","    \"\"\"\n","    model.training = False\n","\n","    cum_loss = 0.\n","    cum_tgt_words = 0.\n","\n","    for src_sents, tgt_sents in batch_iter(dev_data, batch_size):\n","        loss = -tf.reduce_sum(model(src_sents, tgt_sents))\n","\n","        cum_loss += loss\n","        tgt_word_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>` i.e. 'start sentence' tag\n","        cum_tgt_words += tgt_word_num_to_predict\n","\n","    ppl = np.exp(cum_loss / cum_tgt_words)\n","\n","    model.training=True\n","\n","    return ppl\n","\n","\n","def compute_corpus_level_bleu_score(references, hypotheses):\n","    \"\"\" Given decoding results and reference sentences, compute corpus-level BLEU score.\n","    @param references (List[List[str]]): a list of gold-standard reference target sentences\n","    @param hypotheses (List[Hypothesis]): a list of hypotheses, one for each reference\n","    @returns bleu_score (float): corpus-level BLEU score\n","    \"\"\"\n","    if references[0][0] == '<s>':\n","        references = [ref[1:-1] for ref in references]\n","    bleu_score = corpus_bleu([[ref] for ref in references],\n","                             [hyp.value for hyp in hypotheses])\n","    return bleu_score\n","\n","\n","def train(train_src, train_tgt, dev_src, dev_tgt, train_batch_size=32, clip_grad=5.0, valid_niter=2000, log_every=50, \n","          model_save_path='./saved_model/model', uniform_init=0.1, lr=1.e-3, lr_decay=0.5, max_epoch=30, patience_lr_decay=5, max_num_trial=5, continue_training=False):\n","    \"\"\" Train the NMT Model.\n","    @param train_src (string): train source file\n","    @param train_tgt (string): train target file\n","    @param dev_src (string): dev source file\n","    @param dev_tgt (string): dev target file\n","    @param train_batch_size (int): batch size [default: 32]\n","    @param clip_grad (string): gradient clipping [default: 5.0]\n","    @param valid_niter (int): perform validation after how many iterations [default: 2000]\n","    @param log_every (int): log every [default: 10]\n","    @param model_save_path (string): model save path [default: model.bin]\n","    @param uniform_init (float): uniformly initialize all parameters [default: 0.1]\n","    @param lr (float): learning rate [default: 0.001]\n","    @param lr_decay (float): learning rate decay [default: 0.5]\n","    @param max_epoch (int): max epoch [default: 30]\n","    @param patience_lr_decay (int): wait for how many iterations to decay learning rate [default: 5]\n","    @param max_num_trial (int): terminate training after how many trials [default: 5]\n","    @param continue_training (bool): whether to start a new training process or continue training from the previously saved model [default=False]\n","    \"\"\"    \n","    train_data_src = read_corpus(train_src, source='src')\n","    train_data_tgt = read_corpus(train_tgt, source='tgt')\n","\n","    dev_data_src = read_corpus(dev_src, source='src')\n","    dev_data_tgt = read_corpus(dev_tgt, source='tgt')\n","\n","    train_data = list(zip(train_data_src, train_data_tgt))\n","    dev_data = list(zip(dev_data_src, dev_data_tgt))\n","\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=lr) #create the optimizer object\n","    if continue_training == True:\n","        #load the best saved model\n","        print('load the previously best saved model', file=sys.stderr)\n","        model = NMT.load(model_save_path)\n","        # load the optimizer's state\n","        print('restore parameters of the optimizer', file=sys.stderr)\n","        with open(model_save_path+'_optim.json', 'r') as f:\n","            config = json.load(f)\n","            optimizer.from_config(config)\n","    else:\n","        model = NMT(200, 100, VOCABULARY) #create a new model to train from scratch\n","        if np.abs(uniform_init) > 0.:\n","            print('uniformly initialize parameters [-%f, +%f]' % (uniform_init, uniform_init), file=sys.stderr)\n","            model.uniformly_initialize_layers(uniform_init)\n","\n","\n","    vocab = model.vocab\n","    model.training = True\n","\n","    vocab_mask = tf.Variable(tf.ones([len(vocab.tgt)]))\n","    vocab_mask[vocab.tgt['<pad>']].assign(0)\n","\n","    num_trial = 0\n","    train_iter = patience = cum_loss = report_loss = cum_tgt_words = report_tgt_words = 0\n","    cum_examples = report_examples = epoch = valid_num = 0\n","    hist_valid_scores = []\n","    train_time = begin_time = time.time()\n","    print('Begin Maximum Likelihood Training:')\n","    print('use device: %s' % model.device, file=sys.stderr)\n","\n","    while True:\n","        epoch += 1\n","\n","        for src_sents, tgt_sents in batch_iter(train_data, batch_size=train_batch_size, shuffle=True):\n","            train_iter += 1\n","\n","            batch_size = len(src_sents)\n","\n","            with tf.device(model.device):\n","                with tf.GradientTape() as t:\n","                    example_losses = -model(src_sents, tgt_sents) # (batch_size,)\n","                    batch_loss = tf.reduce_sum(example_losses)\n","                    loss = batch_loss / batch_size\n","\n","                grads = t.gradient(loss, model.trainable_variables)\n","\n","                # clip gradient\n","                #grads = [tf.clip_by_norm(g, clip_grad) for g in grads] #this makes more sense to me\n","                grads, _ = tf.clip_by_global_norm(grads, clip_grad) #tf recommends this however\n","\n","                optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","            batch_losses_val = batch_loss.numpy()\n","            report_loss += batch_losses_val\n","            cum_loss += batch_losses_val\n","\n","            tgt_words_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n","            report_tgt_words += tgt_words_num_to_predict\n","            cum_tgt_words += tgt_words_num_to_predict\n","            report_examples += batch_size\n","            cum_examples += batch_size\n","\n","            if train_iter % log_every == 0:\n","                print('epoch %d, iter %d, avg. loss %.2f, avg. ppl %.2f ' \\\n","                      'cum. examples %d, speed %.2f words/sec, time elapsed %.2f sec' % (epoch, train_iter,\n","                                                                                         report_loss / report_examples,\n","                                                                                         math.exp(report_loss / report_tgt_words),\n","                                                                                         cum_examples,\n","                                                                                         report_tgt_words / (time.time() - train_time),\n","                                                                                         time.time() - begin_time), file=sys.stderr)\n","\n","                train_time = time.time()\n","                report_loss = report_tgt_words = report_examples = 0.\n","\n","            # perform validation\n","            if train_iter % valid_niter == 0:\n","                print('epoch %d, iter %d, cum. loss %.2f, cum. ppl %.2f cum. examples %d' % (epoch, train_iter,\n","                                                                                         cum_loss / cum_examples,\n","                                                                                         np.exp(cum_loss / cum_tgt_words),\n","                                                                                         cum_examples), file=sys.stderr)\n","\n","                cum_loss = cum_examples = cum_tgt_words = 0.\n","                valid_num += 1\n","\n","                print('begin validation ...', file=sys.stderr)\n","\n","                # compute dev. ppl and bleu\n","                dev_ppl = evaluate_ppl(model, dev_data, batch_size=128)   # dev batch size can be a bit larger\n","                valid_metric = -dev_ppl\n","\n","                print('validation: iter %d, dev. ppl %f' % (train_iter, dev_ppl), file=sys.stderr)\n","\n","                is_better = len(hist_valid_scores) == 0 or valid_metric > max(hist_valid_scores)\n","                hist_valid_scores.append(valid_metric)\n","\n","                if is_better:\n","                    patience = 0\n","                    # save the model\n","                    print('save currently the best model to [%s]' % model_save_path, file=sys.stderr)\n","                    model.save(model_save_path)\n","\n","                    # save the optimizer's state\n","                    print('save parameters of the optimizer', file=sys.stderr)\n","                    with open(model_save_path + '_optim.json', 'w') as f:\n","                        config = optimizer.get_config()\n","                        config = {k:(v.item() if type(config[k]) == np.float32 else v) for (k,v) in config.items()} #convert to native type for json serialization\n","                        json.dump(config, f)\n","\n","                elif patience < patience_lr_decay:\n","                    patience += 1\n","                    print('hit patience %d' % patience, file=sys.stderr)\n","\n","                    if patience == patience_lr_decay:\n","                        num_trial += 1\n","                        print('hit #%d trial' % num_trial, file=sys.stderr)\n","                        if num_trial == max_num_trial:\n","                            print('early stop!', file=sys.stderr)\n","                            return\n","\n","                        # decay lr, and restore from previously best checkpoint\n","                        lr = optimizer.learning_rate * lr_decay\n","                        print('load previously best model and decay learning rate to %f' % lr, file=sys.stderr)\n","\n","                        # load model\n","                        model = NMT.load(model_save_path)\n","                        # load the optimizer's state\n","                        print('restore parameters of the optimizer', file=sys.stderr)\n","                        with open(model_save_path+'_optim.json', 'r') as f:\n","                            config = json.load(f)\n","                            optimizer.from_config(config)\n","\n","                        # set new lr\n","                        optimizer.learning_rate.assign(lr)\n","\n","                        # reset patience\n","                        patience = 0\n","\n","                if epoch == max_epoch:\n","                    print('reached maximum number of epochs!', file=sys.stderr)\n","                    return\n","\n","\n","def decode(test_source_file, test_target_file=None, model=None, model_save_path='./saved_model/model', \n","           beam_size=3, max_decoding_time_step=50, output_file=None, val_size=500):\n","    \"\"\" Performs decoding on a test set, and save the best-scoring decoding results.\n","    If the target gold-standard sentences are given, the function also computes\n","    corpus-level BLEU score.\n","    @param test_source_file (string): path to open the file\n","    @param test_target_file (string): path to open the file\n","    @param model (NMT model): the trained NMT model\n","    @param model_save_path (string): saved model path\n","    @param beam_size (int): scope of the bean search\n","    @param max_decoding_time_step (int): as the name implies\n","    @param output_file (string): path to save the results\n","    @returns None\n","    \"\"\"\n","    print(\"load test source sentences from [{}]\".format(test_source_file))\n","    test_data_src = read_corpus(test_source_file, source='src')\n","    if test_target_file:\n","        print(\"load test target sentences from [{}]\".format(test_target_file))\n","        test_data_tgt = read_corpus(test_target_file, source='tgt')\n","\n","    if model == None:\n","        #load the best saved model\n","        print(\"load model from {}\".format(model_save_path), file=sys.stderr)\n","        model = NMT.load(model_save_path)\n","\n","    with tf.device(model.device):\n","        hypotheses = beam_search(model, test_data_src, beam_size, max_decoding_time_step)\n","\n","    if test_target_file:\n","        top_hypotheses = [hyps[0] for hyps in hypotheses]\n","        bleu_score = compute_corpus_level_bleu_score(test_data_tgt[0:val_size], top_hypotheses[0:val_size])\n","        print('Corpus BLEU: {}'.format(bleu_score * 100))\n","\n","    if output_file:\n","        with open(output_file, 'w') as f:\n","            for src_sent, hyps in zip(test_data_src, hypotheses):\n","                top_hyp = hyps[0]\n","                hyp_sent = ' '.join(top_hyp.value)\n","                f.write(hyp_sent + '\\n')\n","\n","\n","def beam_search(model, test_data_src, beam_size, max_decoding_time_step, val_size=500):\n","    \"\"\" Run beam search to construct hypotheses for a list of src-language sentences.\n","    @param model (NMT): NMT Model\n","    @param test_data_src (List[List[str]]): List of sentences (words) in source language, from test set.\n","    @param beam_size (int): beam_size (# of hypotheses to hold for a translation at every step)\n","    @param max_decoding_time_step (int): maximum sentence length that Beam search can produce\n","    @returns hypotheses (List[List[Hypothesis]]): List of Hypothesis translations for every source sentence.\n","    \"\"\"\n","    model.training = False \n","\n","    hypotheses = []\n","    for src_sent in tqdm(test_data_src[0:val_size], desc='Decoding', file=sys.stdout):\n","        # example_hyps = model.beam_search(src_sent, beam_size, max_decoding_time_step) #do for each sentence\n","        example_hyps = model.no_beam_search(src_sent)\n","        hypotheses.append(example_hyps)\n","\n","    model.training = True\n","    return hypotheses\n","\n","def test_save_load_model(dev_src, dev_tgt):\n","    '''\n","    To make sure we get identical results when save/load the model\n","    '''\n","    model = NMT(50,50,VOCABULARY)\n","    dev_data_src = read_corpus(dev_src, source='src')[0:5]\n","    dev_data_tgt = read_corpus(dev_tgt, source='tgt')[0:5]\n","    dev_data = list(zip(dev_data_src, dev_data_tgt))\n","    model.uniformly_initialize_layers(0.5)\n","    \n","    print('Initial Results:')\n","    model.training = False\n","    res1 = model(dev_data_src, dev_data_tgt)\n","    ppl = evaluate_ppl(model, dev_data)\n","    #print('model call results {}'.format(res1))\n","    #print(ppl)\n","\n","    print('save the model')\n","    test_path = './saved_model/test_save_load_model_delete'\n","    model.save(test_path)\n","    time.sleep(2)\n","\n","    print('load model')\n","    model = NMT.load(test_path)\n","    time.sleep(2)\n","    model.training = False\n","    res2 = model(dev_data_src, dev_data_tgt)\n","    ppl = evaluate_ppl(model, dev_data)\n","    #print('model call results {}'.format(res2))\n","    #print(ppl)\n","\n","    print('Same results? {}'.format(np.allclose(res1.numpy(), res2.numpy())))\n","\n","\n","\n","def main_run():\n","    \"\"\" Main func.\n","    \"\"\"\n","\n","    # seed the random number generators\n","    seed = 1234\n","    tf.random.set_seed(seed)\n","    np.random.seed(seed * 13 // 7)\n","\n","    train_src='./a4/en_es_data/train.es'\n","    train_tgt = './a4/en_es_data/train.en'\n","    dev_src = './a4/en_es_data/dev.es'\n","    dev_tgt = './a4/en_es_data/dev.en'\n","\n","    #test_save_load_model(dev_src, dev_tgt)\n","\n","    #train(train_src, train_tgt, dev_src, dev_tgt, continue_training=True)\n","    decode(dev_src, test_target_file=dev_tgt, beam_size=10)\n","\n","main_run()\n","\n","'''\n","500 training data Bleu score\n","    beam_search: 36.0 (took 2 mins to run) beam_search=3\n","    amit_beam_search: 36.0 (took 6 mins to run) beam_search=3\n","    beam_search: 37.1 (took 3.5 mins to run) beam_search=10\n","    amit_beam_search: 37.1 (took 39 mins to run) beam_search=10\n","    no beam: 33.97\n","\n","500 validation data Bleu score\n","    beam_search: 26.0 (took 3 mins to run) beam_search=10\n","    no beam search: 25 (took 1.5 mins)\n","'''"],"execution_count":0,"outputs":[{"output_type":"stream","text":["load test source sentences from [./a4/en_es_data/dev.es]\n","load test target sentences from [./a4/en_es_data/dev.en]\n"],"name":"stdout"},{"output_type":"stream","text":["load model from ./saved_model/model\n","restore model hyper parameters\n"],"name":"stderr"},{"output_type":"stream","text":["\rDecoding:   0%|          | 0/500 [00:00<?, ?it/s]"],"name":"stdout"},{"output_type":"stream","text":["restore model weights\n"],"name":"stderr"},{"output_type":"stream","text":["Decoding: 100%|██████████| 500/500 [01:26<00:00,  5.87it/s]\n","Corpus BLEU: 24.79346637453408\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'research\\n500 training data Bleu score\\n    beam_search: 36.0 (took 2 mins to run) beam_search=3\\n    amit_beam_search: 36.0 (took 6 mins to run) beam_search=3\\n    beam_search: 37.1 (took 3.5 mins to run) beam_search=10\\n    amit_beam_search: 37.1 (took 39 mins to run) beam_search=10\\n    no beam: 33.97\\n\\n500 validation data Bleu score\\n    beam_search: 26.0 (took 3 mins to run) beam_search=10\\n'"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"hPKkWeQV9uax","colab_type":"text"},"source":["##PLAYGROUND"]},{"cell_type":"code","metadata":{"id":"B-Sv78hW-2pu","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import torch\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9yoGoQPf_jBZ","colab_type":"code","outputId":"b1cd41c3-08e1-490b-a6a6-443fd1e8b92a","executionInfo":{"status":"ok","timestamp":1576352297879,"user_tz":480,"elapsed":374,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["a = torch.zeros(4)\n","a = a.unsqueeze(1)\n","a = a.view(2,-1)\n","a\n","a = torch.tensor(0.1)\n","a.item()\n","#(hyp_scores.unsqueeze(1).expand_as(log_p_t) + log_p_t).view(-1)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.10000000149011612"]},"metadata":{"tags":[]},"execution_count":34}]}]}