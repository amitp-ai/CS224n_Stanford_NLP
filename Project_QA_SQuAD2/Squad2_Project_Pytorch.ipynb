{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Squad2_Project_Pytorch.ipynb","provenance":[],"collapsed_sections":["rjdqwkxEvaGA","lAX8090-wTSr","vDB2enoP_UIS","uJI1e8qW6Oq7","VulEnRZy5Re9","n7--UU6W5ax_","WvbimWbY9rUu","TpaH-op4HbzM","TgyY3EXRbctS","1gBQU7_6kYar","aiR0StVieoSz","1UcOEIE3XYrO","Db4yZbe0IRv5","WExcMjJ42e0d","fACquozO52cO"],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"OYluXaK24zkr"},"source":["#CS224N Final Project on Question-Answering using SQUAD2.0 Dataset (using Pytorch 1.6.0)\n","\n","See OneNote Notebook for details"]},{"cell_type":"markdown","metadata":{"id":"sgn674u0_OHm"},"source":["##1. Basic Things"]},{"cell_type":"code","metadata":{"id":"nIeOcS46_CuJ","executionInfo":{"status":"ok","timestamp":1605478144467,"user_tz":480,"elapsed":1138,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"}},"outputId":"73849ea3-0a41-41f7-f647-de4fea622145","colab":{"base_uri":"https://localhost:8080/"}},"source":["'''\n","Basic Info:\n","1. UNKNOWN TOKEN IS 1 (i.e. word not in dictionary/vocabulory)\n","2. PAD TOKEN IS 0 (padded with 0 for sentences shorter than the max limit)\n","3. The input data from *.npz file has all the examples shorter than max limit padded with 0.\n","    The limits are as follows:\n","    paragraph max limit is 400\n","    question maximum limit is 50\n","    answer maximum limit is xx (see in the args section)\n","4. Colab tips: https://amitness.com/2020/06/google-colaboratory-tips/\n","'''\n","\n","!nvidia-smi\n","!lscpu\n","!free"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Sun Nov 15 22:09:03 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 455.32.00    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   36C    P0    24W / 300W |      0MiB / 16130MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","Architecture:        x86_64\n","CPU op-mode(s):      32-bit, 64-bit\n","Byte Order:          Little Endian\n","CPU(s):              4\n","On-line CPU(s) list: 0-3\n","Thread(s) per core:  2\n","Core(s) per socket:  2\n","Socket(s):           1\n","NUMA node(s):        1\n","Vendor ID:           GenuineIntel\n","CPU family:          6\n","Model:               85\n","Model name:          Intel(R) Xeon(R) CPU @ 2.00GHz\n","Stepping:            3\n","CPU MHz:             2000.176\n","BogoMIPS:            4000.35\n","Hypervisor vendor:   KVM\n","Virtualization type: full\n","L1d cache:           32K\n","L1i cache:           32K\n","L2 cache:            1024K\n","L3 cache:            39424K\n","NUMA node0 CPU(s):   0-3\n","Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n","              total        used        free      shared  buff/cache   available\n","Mem:       26751716      619284    24083172         956     2049260    25770372\n","Swap:             0           0           0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rjdqwkxEvaGA"},"source":["###To Get More RAM (not needed anymore)"]},{"cell_type":"code","metadata":{"id":"F90RTUsalhDC"},"source":["def get_more_RAM():\n","    # a = [1.0]*int(1e10) #this is faster\n","    a = [1.0]\n","    counter = 1\n","    while True:\n","        a = a*10\n","        counter += 1\n","# get_more_RAM()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pjqKRMuavxC4"},"source":["###Install and Import Necessary Packages"]},{"cell_type":"code","metadata":{"id":"ilHsR_HKGE1Y","executionInfo":{"status":"ok","timestamp":1605478154583,"user_tz":480,"elapsed":4423,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"}},"outputId":"1a884799-975f-490c-a18b-5e0974b9c00e","colab":{"base_uri":"https://localhost:8080/"}},"source":["!python --version #3.6.9\n","# !pip install https://download.pytorch.org/whl/cu100/torch-1.3.1%2Bcu100-cp36-cp36m-linux_x86_64.whl\n","!pip install ujson"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Python 3.6.9\n","Collecting ujson\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/84/e039c6ffc6603f2dfe966972d345d4f650a4ffd74b18c852ece645de12ac/ujson-4.0.1-cp36-cp36m-manylinux1_x86_64.whl (179kB)\n","\u001b[K     |████████████████████████████████| 184kB 13.7MB/s \n","\u001b[?25hInstalling collected packages: ujson\n","Successfully installed ujson-4.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w1jlNoTc43Tu","executionInfo":{"status":"ok","timestamp":1605478160719,"user_tz":480,"elapsed":4735,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"}},"outputId":"0a831128-9c41-4017-905c-b1cee0048e85","colab":{"base_uri":"https://localhost:8080/"}},"source":["import torch #version 1.3.1 or 1.6.0 or 1.7.0\n","import numpy as np\n","import random\n","print(torch.__version__) #use Torch version 1.3.1"],"execution_count":3,"outputs":[{"output_type":"stream","text":["1.7.0+cu101\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lAX8090-wTSr"},"source":["###PLAYGROUND"]},{"cell_type":"code","metadata":{"id":"lJmBlYBHsF5z","executionInfo":{"elapsed":11023,"status":"ok","timestamp":1603659293232,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"03b46ca1-c59d-49f3-c0c1-605828442934","colab":{"base_uri":"https://localhost:8080/"}},"source":["torch.manual_seed(0)\n","x = torch.rand(2,2)\n","device = torch.device('cuda', 0)\n","# torch.cuda.set_device(1)\n","# device = torch.cuda.device(0)\n","# x = x.to('cuda') #SEE IF CAN USE TPU\n","x = x.to(device=device)\n","print(x, x.type())\n","\n","# x = x.type(torch.float16) or\n","x1 = x.to(dtype=torch.float16)\n","print(x1, x1.type())\n","print(x1 is x) #false for tensors (but not for nn.Module)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[0.4963, 0.7682],\n","        [0.0885, 0.1320]], device='cuda:0') torch.cuda.FloatTensor\n","tensor([[0.4963, 0.7681],\n","        [0.0885, 0.1321]], device='cuda:0', dtype=torch.float16) torch.cuda.HalfTensor\n","False\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X7MdvxftsF0r","executionInfo":{"elapsed":1917,"status":"ok","timestamp":1600042670486,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"2b32f054-04a8-4b41-efb1-3a46f0811682","colab":{"base_uri":"https://localhost:8080/","height":199}},"source":["#DELETE THIS\n","from functools import reduce\n","import time\n","\n","device = torch.device('cuda', 0)\n","torch.manual_seed(0)\n","class DataSet(torch.utils.data.Dataset):\n","    \"\"\"docstring for DataSet\"\"\"\n","    def __init__(self, data):\n","        super(DataSet, self).__init__()\n","        self.data = data\n","\n","    def __getitem__(self, index):\n","        return self.data[0][index], self.data[1][index]\n","    def __len__(self):\n","        return len(self.data[0])\n","\n","in_dim, hid_dim, out_dim = 20, 40, 1\n","data_size, batch_size, num_epochs = 50, 4, 30\n","X = torch.rand(data_size, in_dim)\n","Y = torch.rand(data_size, out_dim)\n","# Y[:,0] = (X*X).sum(-1)\n","# Y[:,0] = (torch.sin(X)).sum(-1)\n","# Y[:,1] = X.sum(-1)**3\n","\n","dataset = DataSet([X,Y])\n","data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n","\n","\n","class BasicNN(torch.nn.Module):\n","    \"\"\"docstring for ClassName\"\"\"\n","    def __init__(self, in_dim, hid_dim, out_dim):\n","        super(BasicNN, self).__init__()\n","        self.in_dim = in_dim\n","        self.hid_dim = hid_dim\n","        self.out_dim = out_dim\n","        self.m1 = torch.nn.Linear(in_dim, hid_dim)\n","        self.lyr_norm1 = torch.nn.LayerNorm([hid_dim], False)\n","        self.m2 = torch.nn.Linear(hid_dim, hid_dim)\n","        self.lyr_norm2 = torch.nn.LayerNorm([hid_dim], False)\n","        self.m3 = torch.nn.Linear(hid_dim, out_dim)\n","        self.conv = torch.nn.Conv1d(10,10,3,stride=1, groups=2)\n","        self.reset()\n","\n","    def forward(self, x):\n","        y = self.m1(x)\n","        y = torch.nn.functional.relu(y)\n","        y = self.lyr_norm1(y)\n","        y = self.m2(y)\n","        y = torch.nn.functional.relu(y)\n","        y = self.lyr_norm2(y)\n","        y = self.m3(y)\n","        return y\n","\n","\n","    def reset(self):\n","        '''\n","        Initialize with Glorot initialization\n","        '''\n","        # z = list(self.children()) #not recursive\n","        # print(list(z[0].modules()), print(type(self.lyr_norm1)))\n","        with torch.no_grad():\n","            for m in self.modules(): #self.modules() will recursively return all the modules in the network\n","                if type(m) in [torch.nn.modules.linear.Linear, torch.nn.modules.normalization.LayerNorm]:\n","                    dims = reduce(lambda x,y: x+y, m.weight.shape)\n","                    rng = torch.sqrt(torch.tensor(6.0/(dims)))\n","                    m.weight.uniform_(-rng, rng)\n","                    # print(dims, type(m), m.weight.shape)\n","                elif type(m) == torch.nn.modules.conv.Conv1d:\n","                    dims = reduce(lambda x,y: x*y, m.weight.shape[1:]) + m.weight.shape[0]\n","                    rng = torch.sqrt(torch.tensor(6.0/(dims)))\n","                    m.weight.uniform_(-rng, rng)\n","                    # print(dims, type(m), m.weight.shape)\n","                # print(m, type(m))\n","\n","\n","def Training_Func(data_loader):\n","    model = BasicNN(in_dim, hid_dim, out_dim)\n","    model = model.to(device)\n","    \n","    # optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","    # optimizer = torch.optim.Adadelta(model.parameters(), lr=0.2)\n","    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda e: 1.0/min(max(2, e/15*2)-1, 2)) #start reducing after 10 epochs then reduce by a maximum of 3x\n","\n","    model.train()\n","    with torch.enable_grad():\n","        strt = time.time()\n","        losses = []\n","        scaler = torch.cuda.amp.GradScaler()\n","        for e in range(num_epochs):\n","            tot_loss = 0\n","            for x,y in data_loader:\n","                x = x.to(device)\n","                y = y.to(device)\n","                optimizer.zero_grad()\n","\n","                '''\n","                y_pred = model(x)\n","                loss = ((y_pred-y)**2).sum(-1).mean()\n","                loss.backward()\n","                optimizer.step()\n","                '''\n","                \n","                with torch.cuda.amp.autocast():\n","                    y_pred = model(x)\n","                    loss = ((y_pred-y)**2).sum(-1).mean()\n","\n","                scaler.scale(loss).backward()\n","                scaler.step(optimizer)\n","                scaler.update()\n","\n","\n","                tot_loss += loss.item()\n","            losses.append(tot_loss)\n","            #print(e, tot_loss)\n","            #print(f\"lr is {optimizer.state_dict()['param_groups'][0]['lr']}\")\n","            scheduler.step()\n","            # break\n","        end = time.time()\n","        '''\n","        plt.plot(losses)\n","        plt.yscale('log')\n","        plt.text(10,3, str(end-strt))\n","        plt.grid(True)\n","        plt.show()\n","        '''\n","\n","    with torch.no_grad():\n","        print(((model(X.to(device))-Y.to(device))**2).sum())\n","        #print(model(X), Y)\n","        print(model)\n","    return model\n","\n","strt = time.time()\n","Training_Func(data_loader)\n","print(f'Time taken: {time.time()-strt}')\n","None"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor(0.8883, device='cuda:0')\n","BasicNN(\n","  (m1): Linear(in_features=20, out_features=40, bias=True)\n","  (lyr_norm1): LayerNorm((40,), eps=False, elementwise_affine=True)\n","  (m2): Linear(in_features=40, out_features=40, bias=True)\n","  (lyr_norm2): LayerNorm((40,), eps=False, elementwise_affine=True)\n","  (m3): Linear(in_features=40, out_features=1, bias=True)\n","  (conv): Conv1d(10, 10, kernel_size=(3,), stride=(1,), groups=2)\n",")\n","Time taken: 1.2718801498413086\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PmmU5qda5GcR"},"source":["###Mount Google Drive"]},{"cell_type":"code","metadata":{"id":"MzZpCHwZ4_Ae","executionInfo":{"status":"ok","timestamp":1605478195687,"user_tz":480,"elapsed":30724,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"}},"outputId":"07933ef4-e422-4cff-a27a-7319b9189b1c","colab":{"base_uri":"https://localhost:8080/"}},"source":["#Create/readin the dataset file\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","#To ensure that the Colab Python interpreter can load Python files from within\n","import sys\n","import os\n","FOLDER_NAME = '/content/gdrive/My Drive/Colab Notebooks/Deep_Learning/CS224N_2019/Project_Squad2/project_squad_final'\n","sys.path.append(os.path.join(FOLDER_NAME, 'src'))\n","print(sys.path)\n","\n","# for auto-reloading external modules\n","# version 1 (automatically reloads all the time):\n","# # see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","# %load_ext autoreload\n","# %autoreload 2\n","\n","# version 2 (reloads when you tell it to i.e. whenever you execute import <module>):\n","import importlib\n","\n","\n","\n","os.chdir(FOLDER_NAME)\n","!pwd\n","!ls\n","print()\n","print('NOTE: ALL THE PATHS ARE REFERENCED FROM THE DIRECTORY WHERE THE COLAB NOTEBOOK IS SAVED BCSE OS.CHDIR() \\\n","IS MADE TO POINT THERE AND ALL .py FILES INSIDE SRC DIRECTORY ARE ONLY CALLED FROM THE COLAB NOTEBOOK AND ALSO BCSE \\\n","ALL THE PATH TO ALL THE SRC/*.PY FILES IS ADDED TO SYS.PATH.')\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n","['', '/env/python', '/usr/lib/python36.zip', '/usr/lib/python3.6', '/usr/lib/python3.6/lib-dynload', '/usr/local/lib/python3.6/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.6/dist-packages/IPython/extensions', '/root/.ipython', '/content/gdrive/My Drive/Colab Notebooks/Deep_Learning/CS224N_2019/Project_Squad2/project_squad_final/src']\n","/content/gdrive/My Drive/Colab Notebooks/Deep_Learning/CS224N_2019/Project_Squad2/project_squad_final\n","data\t\t LICENSE    Report.gslides\t\t  src\n","environment.yml  Readings   save\n","handouts\t README.md  Squad2_Project_Pytorch.ipynb\n","\n","NOTE: ALL THE PATHS ARE REFERENCED FROM THE DIRECTORY WHERE THE COLAB NOTEBOOK IS SAVED BCSE OS.CHDIR() IS MADE TO POINT THERE AND ALL .py FILES INSIDE SRC DIRECTORY ARE ONLY CALLED FROM THE COLAB NOTEBOOK AND ALSO BCSE ALL THE PATH TO ALL THE SRC/*.PY FILES IS ADDED TO SYS.PATH.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vDB2enoP_UIS"},"source":["###Git Version Control"]},{"cell_type":"markdown","metadata":{"id":"9fcIC0AMHb-3"},"source":["Don't have to do version control on this colab file as well as the report presentation as they have version control availablity by default thru Google Drive."]},{"cell_type":"code","metadata":{"id":"zYECPNHo_Ys3","executionInfo":{"elapsed":1954,"status":"ok","timestamp":1604600868667,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":480},"outputId":"06ed5382-6184-47ab-d040-d61ff04ef94f","colab":{"base_uri":"https://localhost:8080/"}},"source":["!git config --global user.name “[Amit Patel]”\n","!git config --global user.email “[amitpatel.gt@gmail.com]”\n","!git config --global color.ui auto\n","!git config -l\n","\n","initialize_code_to_git = False #only set to true the first time\n","if initialize_code_to_git:\n","    !git init\n","    !git status\n","    !git add --all #./src/*.py and .gitignore files only (per .gitignore)\n","    !git commit -m 'Initial Commit on 07/20/20'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["user.name=“[Amit\n","user.email=“[amitpatel.gt@gmail.com]”\n","color.ui=auto\n","core.repositoryformatversion=0\n","core.filemode=true\n","core.bare=false\n","core.logallrefupdates=true\n","core.symlinks=false\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UH4ug4lX_fiB","executionInfo":{"elapsed":26315,"status":"ok","timestamp":1604512227417,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":480},"outputId":"52beaafe-472d-49fa-c658-07cb46799006","colab":{"base_uri":"https://localhost:8080/"}},"source":["perform_new_commit = True #only set to true when want to commit new changes\n","if perform_new_commit:\n","    !git status\n","    !git add --all #and .gitignore files only (per .gitignore)\n","    !git commit -m '11/04/20: Added data shuffling an other things' #PUT A DESCRIPTIVE MESSAGE"],"execution_count":null,"outputs":[{"output_type":"stream","text":["On branch master\n","Changes not staged for commit:\n","  (use \"git add <file>...\" to update what will be committed)\n","  (use \"git checkout -- <file>...\" to discard changes in working directory)\n","\n","\t\u001b[31mmodified:   src/args.py\u001b[m\n","\t\u001b[31mmodified:   src/train.py\u001b[m\n","\t\u001b[31mmodified:   src/train_utils.py\u001b[m\n","\t\u001b[31mmodified:   src/util.py\u001b[m\n","\n","Untracked files:\n","  (use \"git add <file>...\" to include in what will be committed)\n","\n","\t\u001b[31mDelete_Squad2_Project_Pytorch.ipynb\u001b[m\n","\t\u001b[31ma1b2x5.ipynb\u001b[m\n","\n","no changes added to commit (use \"git add\" and/or \"git commit -a\")\n","[master b223d21] 11/04/20: Added data shuffling an other things\n"," 6 files changed, 49 insertions(+), 15 deletions(-)\n"," create mode 100644 Delete_Squad2_Project_Pytorch.ipynb\n"," create mode 100644 a1b2x5.ipynb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-1WoSMmuDo3v"},"source":["#change the commmit message\n","# !git add src/BiDAF.py\n","# !git commit --amend -m '08/23/20: Added reset() in Bidaf.py and QANet.py'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ZynTxn2DfNL"},"source":["# #get config info\n","# !git config --list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ky6e7odDte_"},"source":["#Note: don't add anything other than source files\n","!git status\n","print('='*100,'\\n')\n","!git log #shows commits for current branch only\n","print('='*100,'\\n')\n","!git log --stat -M #shows commits for current branch only"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nqClVKjlQQ3_"},"source":["# !git checkout master"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8at8wYAaSP8k"},"source":["# # view diff between different commits\n","# !git reset HEAD * #to undo any local changes\n","# !git diff --cache #diff on staged (i.e. git added) changes\n","# !git diff #diff on unstaged changes\n","# !git difftool --tool-help #can use these tools with diff\n","# !git diff a5b031cb:src/QANet.py f94e67c3:src/QANet.py\n","# !git difftool --tool=vimdiff 574ce300:src/QANet.py f94e67c3:src/QANet.py\n","# !git diff a5b031cb:src/BiDAF.py f94e67c3:src/models.py\n","# !git diff a5b031cb:src/QANet.py src/QANet.py\n","# !git show\n","# !git diff ./src/QANet_xfmr_based.py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xjfBCamywJS5","executionInfo":{"elapsed":6257,"status":"ok","timestamp":1597105651776,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"633f5d63-3d49-4ef7-897c-1be84bdea2f8","colab":{"base_uri":"https://localhost:8080/","height":254}},"source":["# !git branch #lists all the branches\n","# !git checkout 455ddd41\n","\n","# #Create a new branch from an older commit above\n","# !git status\n","# !git branch previous_version_of_QANet\n","# !git checkout previous_version_of_QANet\n","# !git add src/QANet.py\n","# !git commit -m \"Went back to previous version of QANet\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["* \u001b[32mmaster\u001b[m\n","Note: checking out '455ddd41'.\n","\n","You are in 'detached HEAD' state. You can look around, make experimental\n","changes and commit them, and you can discard any commits you make in this\n","state without impacting any branches by performing another checkout.\n","\n","If you want to create a new branch to retain commits you create, you may\n","do so (now or later) by using -b with the checkout command again. Example:\n","\n","  git checkout -b <new-branch-name>\n","\n","HEAD is now at 455ddd4 08/08/20 - 2: Updated Author Info\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CUEYHXPhxZeM","executionInfo":{"elapsed":605,"status":"ok","timestamp":1598562957936,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"f2311642-f1bf-4ca6-c5b1-f693099524b3","colab":{"base_uri":"https://localhost:8080/","height":44}},"source":["# !git branch \"without_cmd_line_args\"\n","# !git checkout \"without_cmd_line_args\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Switched to branch 'without_cmd_line_args'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tLWpFTiG-JCv"},"source":["# #https://stackoverflow.com/questions/11628074/how-to-make-git-ignore-my-changes\n","# !git checkout . #to ignore any uncommited/unstaged changes\n","# #If you want to keep those changes for future use, you have 2 options:\n","# !git stash #this will save those changes in a stack. You can apply the changes to your working copy later using git stash pop\n","# !git diff > changes.patch: This will save those changes to the file changes.patch. If you want to apply them to your working copy you can do so: git apply changes.patch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y7s0EDUPGo7D","executionInfo":{"elapsed":390,"status":"ok","timestamp":1598563104481,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"4318a280-de37-47d8-96f5-e7d8ef39565c","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# !git branch playground\n","# !git checkout playground\n","\n","!git checkout master\n","# !git checkout previous_version_of_QANet"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Switched to branch 'master'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DarsuBLV0BwZ","executionInfo":{"elapsed":503,"status":"ok","timestamp":1604602039280,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":480},"outputId":"880412b2-9961-4447-b25e-a43dab1ded0c","colab":{"base_uri":"https://localhost:8080/"}},"source":["!git status\n","!git branch --list"],"execution_count":null,"outputs":[{"output_type":"stream","text":["On branch master\n","Changes not staged for commit:\n","  (use \"git add <file>...\" to update what will be committed)\n","  (use \"git checkout -- <file>...\" to discard changes in working directory)\n","\n","\t\u001b[31mmodified:   src/QANet_xfmr_based.py\u001b[m\n","\n","no changes added to commit (use \"git add\" and/or \"git commit -a\")\n","* \u001b[32mmaster\u001b[m\n","  playground\u001b[m\n","  previous_version_of_QANet\u001b[m\n","  without_cmd_line_args\u001b[m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3ElQpwO_r5rn","executionInfo":{"elapsed":8234,"status":"ok","timestamp":1604601409374,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":480},"outputId":"d8ef1f1d-8225-4506-a5e6-c9ddae8422f1","colab":{"base_uri":"https://localhost:8080/"}},"source":["##How to make git forget about a file that is tracked\n","##First add the file to .gitignore\n","##https://stackoverflow.com/questions/1274057/how-to-make-git-forget-about-a-file-that-was-tracked-but-is-now-in-gitignore#:~:text=27%20Answers&text=answer%20was%20accepted%E2%80%A6-,.,that%20are%20already%20being%20tracked.&text=The%20removal%20of%20the%20file,happen%20on%20the%20next%20commit.\n","\n","# !git rm -r --cached . \n","# !git add .\n","# !git commit -am \"Remove ignored files i.e. *.ipynb\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["rm '.gitignore'\n","rm 'Delete_Squad2_Project_Pytorch.ipynb'\n","rm 'LICENSE'\n","rm 'README.md'\n","rm 'a1b2x5.ipynb'\n","rm 'environment.yml'\n","rm 'src/BiDAF.py'\n","rm 'src/QANet.py'\n","rm 'src/QANet_xfmr_based.py'\n","rm 'src/args.py'\n","rm 'src/inference.py'\n","rm 'src/layers.py'\n","rm 'src/setup.py'\n","rm 'src/test.py'\n","rm 'src/train.py'\n","rm 'src/train_amp.py'\n","rm 'src/train_utils.py'\n","rm 'src/util.py'\n","[master 8b46047] Remove ignored files i.e. *.ipynb\n"," 4 files changed, 31 insertions(+), 28 deletions(-)\n"," delete mode 100644 Delete_Squad2_Project_Pytorch.ipynb\n"," delete mode 100644 a1b2x5.ipynb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UymL2MMN8nZd","executionInfo":{"elapsed":13372,"status":"ok","timestamp":1604601481302,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":480},"outputId":"8cf36f50-e71c-47e8-ed13-4ded605b94d1","colab":{"base_uri":"https://localhost:8080/"}},"source":["# #Shows a tree like-view of everything in the repo\n","# !git log --graph --pretty=oneline --abbrev-commit\n","!git log --oneline --decorate --all --graph"],"execution_count":null,"outputs":[{"output_type":"stream","text":["* \u001b[33m8b46047\u001b[m\u001b[33m (\u001b[m\u001b[1;36mHEAD -> \u001b[m\u001b[1;32mmaster\u001b[m\u001b[33m)\u001b[m Remove ignored files i.e. *.ipynb\n","* \u001b[33mb223d21\u001b[m 11/04/20: Added data shuffling an other things\n","* \u001b[33mbc2198a\u001b[m 10/27/20: Some minor modifications\n","* \u001b[33me0014e3\u001b[m 10/10/20: Added hooks to understand training process\n","* \u001b[33maa2d046\u001b[m 10/02/20: Added train_utils.py\n","* \u001b[33mba8a53f\u001b[m 09/15/20: Added train_amp.py and modified some files to that end\n","* \u001b[33m1032f58\u001b[m 09/11/20: Modified train.py and QANet_xfmr_based.py\n","* \u001b[33me0ad4b7\u001b[m 08/27/20: Modified to allow parameters to be passed thru the command line interface for train.py and test.py\n","* \u001b[33mc418707\u001b[m\u001b[33m (\u001b[m\u001b[1;32mwithout_cmd_line_args\u001b[m\u001b[33m)\u001b[m 08/27/20: Modified qanet_xfmr_based.py\n","* \u001b[33m034ea87\u001b[m 08/27/20: Modified .gitignore and Added readme, license, and environment.yml\n","* \u001b[33m9f80376\u001b[m modified qanet.py, train.py, and added qanet_xfmr.py\n","* \u001b[33m5a0f096\u001b[m 08/23/20: Added reset() in Bidaf.py and QANet.py\n","* \u001b[33m3128800\u001b[m 08/21/20: Modified .gitignore to ignore squad2_project_pytorch*.ipynb\n","* \u001b[33m599f16c\u001b[m 08/21/20: Minor modifications in train.py\n","* \u001b[33m6561020\u001b[m 08/18/20: Changed to Adam optimizer and added lr scheduler in train.py\n","* \u001b[33maa158fb\u001b[m 08/13/20: Minor changes on save and load paths in train.py\n","* \u001b[33m16a4cb7\u001b[m 08/13/20: Commented out dropout after q_emb, c_emb, q_enc, and c_enc in QANet.py\n","\u001b[31m|\u001b[m * \u001b[33m33c28c4\u001b[m\u001b[33m (\u001b[m\u001b[1;32mplayground\u001b[m\u001b[33m)\u001b[m 08/13/20: Undo prev. change of chr_drp_prob. Then, commented out dropout after q_emb, c_emb, q_enc, and c_enc in QANet.py\n","\u001b[31m|\u001b[m * \u001b[33mdcba87a\u001b[m 08/12/20: Change location to save checkpoints to a dummy location in train.py\n","\u001b[31m|\u001b[m * \u001b[33ma4c2a32\u001b[m 08/12/20: made chr_drop_prob be same as wrd_drp_prob in QANet.py\n","\u001b[31m|\u001b[m\u001b[31m/\u001b[m  \n","* \u001b[33m4bdc8ce\u001b[m 08/10/20: Minor Modifications\n","* \u001b[33mf806499\u001b[m 08/09/20: Modified QANet Model to better match with the paper\n","\u001b[32m|\u001b[m * \u001b[33md8bb830\u001b[m\u001b[33m (\u001b[m\u001b[1;32mprevious_version_of_QANet\u001b[m\u001b[33m)\u001b[m 08/12/20: Minor Modifications to train.py\n","\u001b[32m|\u001b[m * \u001b[33m3d32dc1\u001b[m Went back to previous version of QANet\n","\u001b[32m|\u001b[m\u001b[32m/\u001b[m  \n","* \u001b[33m455ddd4\u001b[m 08/08/20 - 2: Updated Author Info\n","* \u001b[33m196f036\u001b[m 08/08/20: Made minor modifications\n","* \u001b[33ma5b031c\u001b[m renamed models.py to BiDAF.py and changed wherever it was imported\n","* \u001b[33ma9563dd\u001b[m changed gitignore to .gitignore end of 07/04/20\n","* \u001b[33mf8bcd93\u001b[m minor changes made at the end of 07/04/20\n","* \u001b[33mf94e67c\u001b[m 08/04/20: Modified QANet as per https://github.com/NLPLearn/QANet\n","* \u001b[33m574ce30\u001b[m Initial Commit on 07/20/20\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7PjTM4Gc4lvO"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uJI1e8qW6Oq7"},"source":["###Move files from gdrive to GCP machine (no need to do this as it's not any faster)"]},{"cell_type":"code","metadata":{"id":"xNTv43M26a7J","executionInfo":{"elapsed":477,"status":"ok","timestamp":1598382501975,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"60e615fb-de3d-4257-e599-b5cfc10890b3","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["os.chdir('/content')\n","os.mkdir('temp_amit_prj')\n","!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["gdrive\tsample_data  temp_amit_prj\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cxr_POrn6a4E","executionInfo":{"elapsed":548,"status":"ok","timestamp":1598382777199,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"fefc5b66-1fff-44cb-83db-f2cda05b6e50","colab":{"base_uri":"https://localhost:8080/","height":74}},"source":["os.chdir(FOLDER_NAME)\n","!pwd\n","!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Colab Notebooks/Deep_Learning/CS224N_2019/Project_Squad2/project_squad_final\n","data  Readings\tReport.gslides\tsave  Squad2_Project_Pytorch.ipynb  src\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZSoFexpO-I1T"},"source":["# !cp -r src '/content/temp_amit_prj/'\n","!cp -r data '/content/temp_amit_prj/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NWw65JRW-Ix4","executionInfo":{"elapsed":406,"status":"ok","timestamp":1598383336566,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"ea284570-1d46-47bd-863d-1ae40164d34b","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["os.chdir('/content/temp_amit_prj/')\n","!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["data  src\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0mXaVDVM6a1X","executionInfo":{"elapsed":351,"status":"ok","timestamp":1598383431973,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"c7533135-6914-4ad0-a59c-2b4435d7b8a9","colab":{"base_uri":"https://localhost:8080/","height":219}},"source":["sys.path"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['',\n"," '/env/python',\n"," '/usr/lib/python36.zip',\n"," '/usr/lib/python3.6',\n"," '/usr/lib/python3.6/lib-dynload',\n"," '/usr/local/lib/python3.6/dist-packages',\n"," '/usr/lib/python3/dist-packages',\n"," '/usr/local/lib/python3.6/dist-packages/IPython/extensions',\n"," '/root/.ipython',\n"," '/content/gdrive/My Drive/Colab Notebooks/Deep_Learning/CS224N_2019/Project_Squad2/project_squad_final/src']"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"code","metadata":{"id":"l1r1F2X96ay9","executionInfo":{"elapsed":336,"status":"ok","timestamp":1598383439519,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"57a2ff1a-e3a9-4b6a-dd01-3e26e7013e5e","colab":{"base_uri":"https://localhost:8080/","height":181}},"source":["sys.path.remove(os.path.join(FOLDER_NAME, 'src'))\n","sys.path"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['',\n"," '/env/python',\n"," '/usr/lib/python36.zip',\n"," '/usr/lib/python3.6',\n"," '/usr/lib/python3.6/lib-dynload',\n"," '/usr/local/lib/python3.6/dist-packages',\n"," '/usr/lib/python3/dist-packages',\n"," '/usr/local/lib/python3.6/dist-packages/IPython/extensions',\n"," '/root/.ipython']"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"code","metadata":{"id":"jFHA-vBsE_ZI","executionInfo":{"elapsed":570,"status":"ok","timestamp":1598383530204,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"b308e8af-49bf-47f3-afb8-100e21c9d16d","colab":{"base_uri":"https://localhost:8080/","height":199}},"source":["sys.path.append('/content/temp_amit_prj/src')\n","sys.path"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['',\n"," '/env/python',\n"," '/usr/lib/python36.zip',\n"," '/usr/lib/python3.6',\n"," '/usr/lib/python3.6/lib-dynload',\n"," '/usr/local/lib/python3.6/dist-packages',\n"," '/usr/lib/python3/dist-packages',\n"," '/usr/local/lib/python3.6/dist-packages/IPython/extensions',\n"," '/root/.ipython',\n"," '/content/temp_amit_prj/src']"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"code","metadata":{"id":"AgBh-UOhE_dD","executionInfo":{"elapsed":446,"status":"ok","timestamp":1598389859623,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"672b1677-856f-4933-ef64-87ccdfecf75c","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["!pwd"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/temp_amit_prj\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KwsLajqqE_WA","executionInfo":{"elapsed":902,"status":"ok","timestamp":1598389973958,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"4c0f9796-afec-495d-c376-7e323a408f95","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["os.chdir('/content')\n","!pwd\n","!rm -r temp_amit_prj/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"P7GtFHyTeHFC","executionInfo":{"elapsed":380,"status":"ok","timestamp":1598390042636,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"c7c38c68-2b3f-4b31-e056-8993357415ba","colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["os.chdir(FOLDER_NAME)\n","!pwd"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Colab Notebooks/Deep_Learning/CS224N_2019/Project_Squad2/project_squad_final\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WX_g8ssqeHBo"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VulEnRZy5Re9"},"source":["##2. Arguments/inputs to various functions"]},{"cell_type":"code","metadata":{"id":"n29yHRJv5VzI","executionInfo":{"elapsed":322,"status":"ok","timestamp":1598390110575,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"38c0ed36-7b7a-47d0-f7f4-7b7caf1366aa","colab":{"base_uri":"https://localhost:8080/","height":508}},"source":["import args\n","importlib.reload(args)\n","None\n","args.get_setup_args()\n","# args.get_train_args()\n","# args.get_test_args()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'ans_limit': 30,\n"," 'answer_file': './data/answer.json',\n"," 'char2idx_file': './data/char2idx.json',\n"," 'char_dim': 64,\n"," 'char_emb_file': './data/char_emb.json',\n"," 'char_limit': 16,\n"," 'dev_eval_file': './data/dev_eval.json',\n"," 'dev_meta_file': './data/dev_meta.json',\n"," 'dev_record_file': './data/dev.npz',\n"," 'dev_url': 'https://github.com/chrischute/squad/data/dev-v2.0.json',\n"," 'glove_dim': 300,\n"," 'glove_num_vecs': 2196017,\n"," 'glove_url': 'http://nlp.stanford.edu/data/glove.840B.300d.zip',\n"," 'include_test_examples': True,\n"," 'para_limit': 400,\n"," 'ques_limit': 50,\n"," 'test_eval_file': './data/test_eval.json',\n"," 'test_meta_file': './data/test_meta.json',\n"," 'test_para_limit': 1000,\n"," 'test_ques_limit': 100,\n"," 'test_record_file': './data/test.npz',\n"," 'test_url': 'https://github.com/chrischute/squad/data/test-v2.0.json',\n"," 'train_eval_file': './data/train_eval.json',\n"," 'train_record_file': './data/train.npz',\n"," 'train_url': 'https://github.com/chrischute/squad/data/train-v2.0.json',\n"," 'word2idx_file': './data/word2idx.json',\n"," 'word_emb_file': './data/word_emb.json'}"]},"metadata":{"tags":[]},"execution_count":84}]},{"cell_type":"markdown","metadata":{"id":"n7--UU6W5ax_"},"source":["##3. Setup"]},{"cell_type":"code","metadata":{"id":"4gqe1qWJ5aGG"},"source":["import spacy\n","import setup\n","importlib.reload(setup)\n","\n","\n","def setup_main():\n","    # Get command-line args\n","    args_ = args.get_setup_args()\n","\n","    # Download resources\n","    setup.download(args_)\n","\n","    # Import spacy language model\n","    nlp = spacy.blank(\"en\")\n","\n","    # Preprocess dataset\n","    args_.train_file = setup.url_to_data_path(args_.train_url)\n","    args_.dev_file = setup.url_to_data_path(args_.dev_url)\n","    if args_.include_test_examples:\n","       args_.test_file = setup.url_to_data_path(args_.test_url)\n","    glove_dir = setup.url_to_data_path(args_.glove_url.replace('.zip', ''))\n","    glove_ext = f'.txt' if glove_dir.endswith('d') else f'.{args_.glove_dim}d.txt'\n","    args_.glove_file = os.path.join(glove_dir, os.path.basename(glove_dir) + glove_ext)\n","    setup.pre_process(args_, nlp)\n","\n","# setup_main()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FA5bK2Kgb_Tk","executionInfo":{"elapsed":2122,"status":"ok","timestamp":1595132683215,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"e57e3df1-1811-46a5-8de0-17a6ee6526e6","colab":{"base_uri":"https://localhost:8080/","height":363}},"source":["os.chdir('{}/data'.format(FOLDER_NAME))\n","!ls -l --block-size=M\n","os.chdir(FOLDER_NAME)\n","!pwd"],"execution_count":null,"outputs":[{"output_type":"stream","text":["total 7047M\n","-rw------- 1 root root    1M May 11 23:51 char2idx.json\n","-rw------- 1 root root    2M May 11 23:51 char_emb.json\n","-rw------- 1 root root   16M May 11 23:51 dev_eval.json\n","-rw------- 1 root root    1M May 11 23:51 dev_meta.json\n","-rw------- 1 root root  174M May 11 23:51 dev.npz\n","-rw------- 1 root root    3M May 11 23:42 dev-v2.0.json\n","drwx------ 2 root root    1M May 11 23:32 glove.840B.300d\n","-rw------- 1 root root 2076M Jan  7  2020 glove.840B.300d.zip\n","-rw------- 1 root root   14M May 11 23:51 test_eval.json\n","-rw------- 1 root root    1M May 11 23:51 test_meta.json\n","-rw------- 1 root root  423M May 11 23:51 test.npz\n","-rw------- 1 root root    2M May 11 23:42 test-v2.0.json\n","-rw------- 1 root root  287M May 11 23:51 train_eval.json\n","-rw------- 1 root root 3795M May 11 23:49 train.npz\n","-rw------- 1 root root   41M May 11 23:27 train-v2.0.json\n","-rw------- 1 root root    2M May 11 23:51 word2idx.json\n","-rw------- 1 root root  219M May 11 23:51 word_emb.json\n","/content/gdrive/My Drive/Colab Notebooks/Deep_Learning/CS224N_2019/Project_Squad2/project_squad_final\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"E_WK9YwBy6Dh"},"source":["# os.chdir('/content/sample_data')\n","# !ls\n","# print()\n","# !cat 'README.md'\n","# print()\n","# \n","# os.chdir('/content')\n","# !ls\n","# print()\n","# os.chdir(FOLDER_NAME)\n","# !ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WvbimWbY9rUu"},"source":["##4. Utility"]},{"cell_type":"code","metadata":{"id":"ELEpnPCrv4Pr","executionInfo":{"elapsed":2285,"status":"ok","timestamp":1603772389854,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"0a43495f-575d-4657-a32b-4c0f545baf0d","colab":{"base_uri":"https://localhost:8080/","height":381}},"source":["import util\n","importlib.reload(util)\n","\n","def utils_playground():\n","    train_data_path = './data/dev.npz'\n","    train_dataset = util.SQuAD(train_data_path)\n","    train_dataset.memory_size()\n","    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=5, shuffle=True, \n","                                               num_workers=0, collate_fn=util.collate_fn)\n","    print()\n","    a = train_dataset[0:7]\n","    print(len(a), a[1].shape)\n","    print()\n","\n","    for a in train_dataloader:\n","        for i in range(len(a)):\n","            print(a[i].shape)\n","        #print(a[0][0,:])\n","        #print(a[1][0,:,:])\n","        break\n","\n","    del train_dataset\n","    del train_dataloader\n","\n","utils_playground()\n","import gc\n","gc.collect()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["context_idxs: 19.090808MB\n","context_char_idxs: 305.452928MB\n","question_idxs: 2.428008MB\n","question_char_idxs: 38.848128MB\n","y1s: 0.047608MB\n","y2s: 0.047608MB\n","ids: 0.047608MB\n","valid_idxs: 7.2e-05MB\n","Total size: 365.962768MB\n","\n","7 torch.Size([7, 401, 16])\n","\n","torch.Size([5, 253])\n","torch.Size([5, 253, 16])\n","torch.Size([5, 17])\n","torch.Size([5, 17, 16])\n","torch.Size([5])\n","torch.Size([5])\n","torch.Size([5])\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["133"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"TpaH-op4HbzM"},"source":["##5. Layers"]},{"cell_type":"code","metadata":{"id":"NSlc_DAbHnd4"},"source":["import layers\n","importlib.reload(layers)\n","None"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TgyY3EXRbctS"},"source":["##6. Models"]},{"cell_type":"code","metadata":{"id":"OtsWqkZ4bfMJ","executionInfo":{"elapsed":6247,"status":"ok","timestamp":1603851603875,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"27a67649-f995-42ce-d7b5-0803880e8320","colab":{"base_uri":"https://localhost:8080/","height":145}},"source":["import QANet\n","importlib.reload(QANet)\n","import QANet_xfmr_based\n","importlib.reload(QANet_xfmr_based)\n","import util\n","importlib.reload(util)\n","None\n","\n","#debug\n","word_vectors = util.torch_from_json(\"./data/word_emb.json\")\n","print(word_vectors.shape)\n","mod = QANet.QANet(word_vectors, hidden_size=96)\n","\n","util.get_model_info(mod)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([88749, 300])\n","emb: 561048 parameters\n","emb_enc: 87648 parameters\n","att: 289 parameters\n","mod_enc_input_prj: 36960 parameters\n","mod_enc: 467040 parameters\n","out: 384 parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ePAHHIFOuZQe"},"source":["# !pip install line_profiler\n","# !kernprof -l -v ./src/QANet_xfmr_based.py\n","!python ./src/QANet.py"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZEiAleNacSgM"},"source":["##7. Training"]},{"cell_type":"code","metadata":{"id":"Jord0OI4yCy3"},"source":["%matplotlib inline\n","from IPython.display import display, Image\n","import glob\n","for f in glob.glob('./save/*.png'):\n","    print(f)\n","    display(Image(f))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ePMhsWLmf_en","executionInfo":{"status":"ok","timestamp":1605509832995,"user_tz":480,"elapsed":11516928,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"}},"outputId":"cda1f74f-354a-496f-c555-13186ba84823","colab":{"base_uri":"https://localhost:8080/"}},"source":["# import train\n","# importlib.reload(train)\n","\n","'''\n","CHANGES MADE:\n","1. REMOVED GRAD CLIPPING IN TRAIN.PY (LINE 134). Did not help\n","2. Using lr=3e-4, it devF1 starts improving after 20 epochs when the lr starts to decrease to 1e-4 (see zzz-02)\n","3. Using lr=1e-4 (zzz-03): didn't help much.\n","4. Added positional encoding inside the encoder (zzz-04) and 5e-4: didn't help much\n","\n","TODO:\n","4. Try adding l2_wd and put positional encoding insider the encoder like qanet and not outside like orig_xmfr\n","5. try different see\n","'''\n","\n","# ema_decay = 0.999\n","name='QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched' #'train-Baseline'\n","model_type='QANet_xfmr_based.QANet_xfmr' #QANet.QANet\n","#load_path = os.path.join('./save/train', 'QANET_xfmr_datashuffle_lrwarmup_posenc_inside_encoder_0p1dropprob-01', 'best.pth.tar')  #None\n","\n","'''\n","From QANet Paper:\n","lr = 1.e-3, l2_wd = 3.e-7, batch = 32, drop_prob = 0.1\n","'''\n","\n","!python ./src/train.py --num_epochs=40 --lr=1.0e-3 --l2_wd=0.0 --batch_size=32 \\\n","--drop_prob=0.05 --hidden_size=96 --seed=224 --name=$name --model_type=$model_type\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Training the Model...\n","2020-11-16 03:45:19.608859: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","[11.16.20 03:45:22] Args: {\n","    \"batch_size\": 32,\n","    \"char_emb_file\": \"./data/char_emb.json\",\n","    \"dev_eval_file\": \"./data/dev_eval.json\",\n","    \"dev_record_file\": \"./data/dev.npz\",\n","    \"drop_prob\": 0.05,\n","    \"ema_decay\": 0.999,\n","    \"eval_steps\": 50000,\n","    \"gpu_ids\": [\n","        0\n","    ],\n","    \"hidden_size\": 96,\n","    \"l2_wd\": 0.0,\n","    \"load_path\": null,\n","    \"lr\": 0.001,\n","    \"max_ans_len\": 15,\n","    \"max_checkpoints\": 5,\n","    \"max_grad_norm\": 5.0,\n","    \"maximize_metric\": true,\n","    \"metric_name\": \"F1\",\n","    \"model_type\": \"QANet_xfmr_based.QANet_xfmr\",\n","    \"name\": \"QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched\",\n","    \"num_epochs\": 40,\n","    \"num_visuals\": 10,\n","    \"num_workers\": 4,\n","    \"save_dir\": \"./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03\",\n","    \"seed\": 224,\n","    \"test_eval_file\": \"./data/test_eval.json\",\n","    \"test_record_file\": \"./data/test.npz\",\n","    \"train_eval_file\": \"./data/train_eval.json\",\n","    \"train_record_file\": \"./data/train.npz\",\n","    \"use_squad_v2\": true,\n","    \"word_emb_file\": \"./data/word_emb.json\"\n","}\n","[11.16.20 03:45:22] Using random seed 224...\n","[11.16.20 03:45:22] Loading embeddings...\n","[11.16.20 03:45:34] Building model...\n","[11.16.20 03:45:39] Saver will maximize F1...\n","[11.16.20 03:45:39] Building dataset...\n","tcmalloc: large alloc 3326009344 bytes == 0x10ea1c000 @  0x7f29b977f1e7 0x7f29b72a55e1 0x7f29b7309c78 0x7f29b72a8e61 0x551555 0x5a9dac 0x50a433 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x508cd5 0x594a01 0x59fd0e 0x5576d8 0x50c19e 0x507be4 0x508f37 0x594a01 0x549e8f 0x5515c1 0x5a9dac 0x50a433 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x507be4 0x50ad03 0x634e72\n","tcmalloc: large alloc 6652010496 bytes == 0x7f2711826000 @  0x7f29b9761b6b 0x7f29b9781379 0x7f295e1d774e 0x7f295e1d97b6 0x7f2998c43d53 0x7f299862e8cf 0x7f2998945cac 0x7f29988f131b 0x7f2998910135 0x7f29988ebb4b 0x7f29988f131b 0x7f2998910135 0x7f29989da2be 0x7f2999eecd6e 0x7f29988f131b 0x7f2998910135 0x7f29988ebb4b 0x7f29988f131b 0x7f2998910135 0x7f29989da2be 0x7f299861a800 0x7f2998b477ea 0x7f2998199081 0x7f2998c33c76 0x7f2998c17810 0x7f29a858023d 0x7f29a868ad33 0x7f29a86937a0 0x50a4a5 0x50beb4 0x507be4\n","tcmalloc: large alloc 6668640256 bytes == 0x7f2584070000 @  0x7f29b9761b6b 0x7f29b9781379 0x7f295e1d774e 0x7f295e1d97b6 0x7f2998c43d53 0x7f29985be54a 0x7f2998918c0a 0x7f2998940803 0x7f2998ac6b14 0x7f2998c034ee 0x7f299865c976 0x7f299865db30 0x7f299891ab09 0x7f2998199249 0x7f2998ab3ae8 0x7f29989bf8a5 0x7f299865f41b 0x7f2998b4f7d8 0x7f2998199249 0x7f2998ab3ae8 0x7f29989bf9f5 0x7f2999f93997 0x7f2998199249 0x7f2998ab3ae8 0x7f29989bf9f5 0x7f29a87d930e 0x50a4a5 0x50cc96 0x507be4 0x508f37 0x594a01\n","[11.16.20 03:47:34] Shuffling dataset...\n","tcmalloc: large alloc 6668640256 bytes == 0x7f23f68ba000 @  0x7f29b9761b6b 0x7f29b9781379 0x7f295e1d774e 0x7f295e1d97b6 0x7f299862dfa2 0x7f2998918bd3 0x7f29988f0207 0x7f299890b2dc 0x7f29988e778a 0x7f29988f0207 0x7f299890b2dc 0x7f29989d70dd 0x7f299863f09f 0x7f29986416b6 0x7f2998641bad 0x7f29985fa23f 0x7f2998918db9 0x7f299893f283 0x7f2998acd01d 0x7f29989e2a57 0x7f2999f523dc 0x7f299893f283 0x7f2998acd01d 0x7f2998c063f7 0x7f29a8a019e8 0x7f29a8a041dd 0x50c19e 0x507be4 0x509900 0x50a2fd 0x50beb4\n","[11.16.20 03:47:42] Training...\n","[11.16.20 03:47:42] Starting epoch 1 out of 40 epochs...\n","[11.16.20 03:53:43] Evaluating at step 50016...\n"," 38% 50016/129922 [06:00<10:28, 127.11it/s, NLL=6.86, epoch=1, iteration=1562, lr=0.0001]HBox(children=(FloatProgress(value=0.0, max=5951.0), HTML(value='')))\n","\n","[11.16.20 03:53:58] Saved checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_50016.pth.tar\n","[11.16.20 03:53:59] New best checkpoint at step 50016...\n","[11.16.20 03:53:59] Dev NLL: 05.50, F1: 52.19, EM: 52.19, AvNA: 52.14\n","[11.16.20 03:53:59] Visualizing in TensorBoard...\n","[11.16.20 03:59:58] Evaluating at step 100032...\n"," 77% 100032/129922 [12:15<03:37, 137.43it/s, NLL=5.36, epoch=1, iteration=3125, lr=0.0001]./src/train.py:204: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  tqdm.tqdm_notebook(total=len(data_loader.dataset)) as progress_bar:\n","HBox(children=(FloatProgress(value=0.0, max=5951.0), HTML(value='')))\n","\n","[11.16.20 04:00:13] Saved checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_100032.pth.tar\n","[11.16.20 04:00:13] Dev NLL: 05.24, F1: 52.19, EM: 52.19, AvNA: 52.14\n","[11.16.20 04:00:13] Visualizing in TensorBoard...\n","100% 129922/129922 [16:05<00:00, 134.55it/s, NLL=4.9, epoch=1, iteration=4060, lr=0.0001]\n","[11.16.20 04:03:48] Starting epoch 2 out of 40 epochs...\n","[11.16.20 04:06:13] Evaluating at step 150050...\n"," 15% 20128/129922 [02:25<13:53, 131.75it/s, NLL=6.58, epoch=2, iteration=4689, lr=0.0001]HBox(children=(FloatProgress(value=0.0, max=5951.0), HTML(value='')))\n","\n","[11.16.20 04:06:29] Saved checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_150050.pth.tar\n","[11.16.20 04:06:29] Dev NLL: 05.15, F1: 52.19, EM: 52.19, AvNA: 52.14\n","[11.16.20 04:06:29] Visualizing in TensorBoard...\n","[11.16.20 04:12:29] Evaluating at step 200066...\n"," 54% 70144/129922 [08:41<07:16, 137.00it/s, NLL=6.52, epoch=2, iteration=6252, lr=0.000708]HBox(children=(FloatProgress(value=0.0, max=5951.0), HTML(value='')))\n","\n","[11.16.20 04:12:44] Saved checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_200066.pth.tar\n","[11.16.20 04:12:44] Dev NLL: 05.06, F1: 52.19, EM: 52.19, AvNA: 52.14\n","[11.16.20 04:12:44] Visualizing in TensorBoard...\n","[11.16.20 04:18:41] Evaluating at step 250082...\n"," 92% 120160/129922 [14:53<01:07, 144.98it/s, NLL=7.83, epoch=2, iteration=7815, lr=0.000981]HBox(children=(FloatProgress(value=0.0, max=5951.0), HTML(value='')))\n","\n","[11.16.20 04:18:56] Saved checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_250082.pth.tar\n","[11.16.20 04:18:56] Dev NLL: 05.11, F1: 52.19, EM: 52.19, AvNA: 52.14\n","[11.16.20 04:18:56] Visualizing in TensorBoard...\n","100% 129922/129922 [16:20<00:00, 132.51it/s, NLL=5.09, epoch=2, iteration=8121, lr=0.000925]\n","[11.16.20 04:20:08] Starting epoch 3 out of 40 epochs...\n","[11.16.20 04:24:55] Evaluating at step 300100...\n"," 31% 40256/129922 [04:47<09:51, 151.60it/s, NLL=4.72, epoch=3, iteration=9379, lr=0.000382]HBox(children=(FloatProgress(value=0.0, max=5951.0), HTML(value='')))\n"," 31% 40256/129922 [05:00<09:51, 151.60it/s, NLL=4.72, epoch=3, iteration=9379, lr=0.000382]\n","[11.16.20 04:25:10] Saved checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_300100.pth.tar\n","[11.16.20 04:25:10] Removed checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_100032.pth.tar\n","[11.16.20 04:25:10] Dev NLL: 05.09, F1: 52.19, EM: 52.19, AvNA: 52.14\n","[11.16.20 04:25:10] Visualizing in TensorBoard...\n","[11.16.20 04:31:07] Evaluating at step 350116...\n"," 69% 90272/129922 [10:59<04:39, 141.68it/s, NLL=6.72, epoch=3, iteration=10942, lr=0.0001]HBox(children=(FloatProgress(value=0.0, max=5951.0), HTML(value='')))\n"," 69% 90272/129922 [11:10<04:39, 141.68it/s, NLL=6.72, epoch=3, iteration=10942, lr=0.0001]\n","[11.16.20 04:31:22] Saved checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_350116.pth.tar\n","[11.16.20 04:31:23] New best checkpoint at step 350116...\n","[11.16.20 04:31:23] Removed checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_150050.pth.tar\n","[11.16.20 04:31:23] Dev NLL: 05.04, F1: 52.24, EM: 52.24, AvNA: 52.23\n","[11.16.20 04:31:23] Visualizing in TensorBoard...\n","100% 129922/129922 [15:58<00:00, 135.57it/s, NLL=6.6, epoch=3, iteration=12182, lr=0.0001]\n","[11.16.20 04:36:06] Starting epoch 4 out of 40 epochs...\n","[11.16.20 04:37:22] Evaluating at step 400134...\n","  8% 10368/129922 [01:15<13:44, 145.02it/s, NLL=6.58, epoch=4, iteration=12506, lr=0.0001]HBox(children=(FloatProgress(value=0.0, max=5951.0), HTML(value='')))\n","\n","[11.16.20 04:37:36] Saved checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_400134.pth.tar\n","[11.16.20 04:37:36] Removed checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_400134.pth.tar\n","[11.16.20 04:37:36] Dev NLL: 05.00, F1: 52.05, EM: 52.04, AvNA: 52.16\n","[11.16.20 04:37:36] Visualizing in TensorBoard...\n","[11.16.20 04:43:31] Evaluating at step 450150...\n"," 46% 60384/129922 [07:24<07:40, 150.95it/s, NLL=5.97, epoch=4, iteration=14069, lr=0.0001]HBox(children=(FloatProgress(value=0.0, max=5951.0), HTML(value='')))\n","\n","[11.16.20 04:43:46] Saved checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_450150.pth.tar\n","[11.16.20 04:43:46] Removed checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_450150.pth.tar\n","[11.16.20 04:43:46] Dev NLL: 04.97, F1: 52.04, EM: 52.01, AvNA: 52.18\n","[11.16.20 04:43:46] Visualizing in TensorBoard...\n","[11.16.20 04:49:44] Evaluating at step 500166...\n"," 85% 110400/129922 [13:37<02:32, 128.09it/s, NLL=7.2, epoch=4, iteration=15632, lr=0.000384]HBox(children=(FloatProgress(value=0.0, max=5951.0), HTML(value='')))\n"," 85% 110400/129922 [13:51<02:32, 128.09it/s, NLL=7.2, epoch=4, iteration=15632, lr=0.000384]\n","[11.16.20 04:49:59] Saved checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_500166.pth.tar\n","[11.16.20 04:49:59] Removed checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_500166.pth.tar\n","[11.16.20 04:49:59] Dev NLL: 04.94, F1: 51.91, EM: 51.82, AvNA: 52.56\n","[11.16.20 04:49:59] Visualizing in TensorBoard...\n","100% 129922/129922 [16:14<00:00, 133.37it/s, NLL=6.62, epoch=4, iteration=16243, lr=0.000701]\n","[11.16.20 04:52:21] Starting epoch 5 out of 40 epochs...\n","[11.16.20 04:55:59] Evaluating at step 550184...\n"," 23% 30496/129922 [03:38<11:22, 145.61it/s, NLL=4.8, epoch=5, iteration=17196, lr=0.000981]HBox(children=(FloatProgress(value=0.0, max=5951.0), HTML(value='')))\n"," 23% 30496/129922 [03:50<11:22, 145.61it/s, NLL=4.8, epoch=5, iteration=17196, lr=0.000981]\n","[11.16.20 04:56:14] Saved checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_550184.pth.tar\n","[11.16.20 04:56:14] Removed checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_550184.pth.tar\n","[11.16.20 04:56:14] Dev NLL: 04.94, F1: 51.70, EM: 51.64, AvNA: 52.09\n","[11.16.20 04:56:14] Visualizing in TensorBoard...\n","[11.16.20 05:02:13] Evaluating at step 600200...\n"," 62% 80512/129922 [09:52<05:50, 141.12it/s, NLL=6.64, epoch=5, iteration=18759, lr=0.000706]HBox(children=(FloatProgress(value=0.0, max=5951.0), HTML(value='')))\n","\n","[11.16.20 05:02:28] Saved checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_600200.pth.tar\n","[11.16.20 05:02:28] Removed checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_600200.pth.tar\n","[11.16.20 05:02:28] Dev NLL: 04.88, F1: 52.14, EM: 52.09, AvNA: 52.26\n","[11.16.20 05:02:28] Visualizing in TensorBoard...\n","100% 129922/129922 [15:59<00:00, 135.34it/s, NLL=6.64, epoch=5, iteration=20304, lr=0.0001]\n","[11.16.20 05:08:21] Starting epoch 6 out of 40 epochs...\n","[11.16.20 05:08:26] Evaluating at step 650218...\n","  0% 608/129922 [00:05<14:47, 145.68it/s, NLL=5.66, epoch=6, iteration=20323, lr=0.0001]HBox(children=(FloatProgress(value=0.0, max=5951.0), HTML(value='')))\n","\n","[11.16.20 05:08:40] Saved checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_650218.pth.tar\n","[11.16.20 05:08:40] Removed checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_650218.pth.tar\n","[11.16.20 05:08:40] Dev NLL: 04.77, F1: 51.79, EM: 51.72, AvNA: 52.08\n","[11.16.20 05:08:40] Visualizing in TensorBoard...\n","[11.16.20 05:14:36] Evaluating at step 700234...\n"," 39% 50624/129922 [06:15<09:07, 144.90it/s, NLL=5.26, epoch=6, iteration=21886, lr=0.0001]HBox(children=(FloatProgress(value=0.0, max=5951.0), HTML(value='')))\n","\n","[11.16.20 05:14:51] Saved checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_700234.pth.tar\n","[11.16.20 05:14:51] Removed checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_700234.pth.tar\n","[11.16.20 05:14:51] Dev NLL: 04.71, F1: 51.74, EM: 51.66, AvNA: 52.19\n","[11.16.20 05:14:51] Visualizing in TensorBoard...\n","[11.16.20 05:20:48] Evaluating at step 750250...\n"," 77% 100640/129922 [12:27<03:50, 126.94it/s, NLL=6.72, epoch=6, iteration=23449, lr=0.0001]HBox(children=(FloatProgress(value=0.0, max=5951.0), HTML(value='')))\n","\n","[11.16.20 05:21:03] Saved checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_750250.pth.tar\n","[11.16.20 05:21:03] Removed checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_750250.pth.tar\n","[11.16.20 05:21:03] Dev NLL: 04.67, F1: 51.61, EM: 51.47, AvNA: 52.63\n","[11.16.20 05:21:03] Visualizing in TensorBoard...\n","100% 129922/129922 [16:13<00:00, 133.39it/s, NLL=4.23, epoch=6, iteration=24365, lr=0.0001]\n","[11.16.20 05:24:35] Starting epoch 7 out of 40 epochs...\n","[11.16.20 05:27:05] Evaluating at step 800268...\n"," 16% 20736/129922 [02:30<11:55, 152.61it/s, NLL=5.16, epoch=7, iteration=25013, lr=0.0001]HBox(children=(FloatProgress(value=0.0, max=5951.0), HTML(value='')))\n","\n","[11.16.20 05:27:20] Saved checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_800268.pth.tar\n","[11.16.20 05:27:20] Removed checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_800268.pth.tar\n","[11.16.20 05:27:20] Dev NLL: 04.64, F1: 51.67, EM: 51.47, AvNA: 52.95\n","[11.16.20 05:27:20] Visualizing in TensorBoard...\n","[11.16.20 05:33:19] Evaluating at step 850284...\n"," 54% 70752/129922 [08:44<06:18, 156.50it/s, NLL=6.12, epoch=7, iteration=26576, lr=0.000832]HBox(children=(FloatProgress(value=0.0, max=5951.0), HTML(value='')))\n","\n","[11.16.20 05:33:34] Saved checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_850284.pth.tar\n","[11.16.20 05:33:34] Removed checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_850284.pth.tar\n","[11.16.20 05:33:34] Dev NLL: 04.62, F1: 51.34, EM: 51.08, AvNA: 52.87\n","[11.16.20 05:33:34] Visualizing in TensorBoard...\n","[11.16.20 05:39:33] Evaluating at step 900300...\n"," 93% 120768/129922 [14:58<01:05, 139.13it/s, NLL=5.45, epoch=7, iteration=28139, lr=0.000923]HBox(children=(FloatProgress(value=0.0, max=5951.0), HTML(value='')))\n"," 93% 120768/129922 [15:10<01:05, 139.13it/s, NLL=5.45, epoch=7, iteration=28139, lr=0.000923]\n","[11.16.20 05:39:48] Saved checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_900300.pth.tar\n","[11.16.20 05:39:48] Removed checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_900300.pth.tar\n","[11.16.20 05:39:48] Dev NLL: 04.64, F1: 50.92, EM: 50.68, AvNA: 52.50\n","[11.16.20 05:39:48] Visualizing in TensorBoard...\n","100% 129922/129922 [16:20<00:00, 132.56it/s, NLL=6.48, epoch=7, iteration=28426, lr=0.00084]\n","[11.16.20 05:40:55] Starting epoch 8 out of 40 epochs...\n","[11.16.20 05:45:46] Evaluating at step 950318...\n"," 31% 40864/129922 [04:51<10:51, 136.73it/s, NLL=5.85, epoch=8, iteration=29703, lr=0.000193]HBox(children=(FloatProgress(value=0.0, max=5951.0), HTML(value='')))\n","\n","[11.16.20 05:46:01] Saved checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_950318.pth.tar\n","[11.16.20 05:46:01] Removed checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_950318.pth.tar\n","[11.16.20 05:46:01] Dev NLL: 04.64, F1: 50.87, EM: 50.71, AvNA: 52.51\n","[11.16.20 05:46:01] Visualizing in TensorBoard...\n","[11.16.20 05:51:57] Evaluating at step 1000334...\n"," 70% 90880/129922 [11:02<04:17, 151.42it/s, NLL=5.87, epoch=8, iteration=31266, lr=0.0001]HBox(children=(FloatProgress(value=0.0, max=5951.0), HTML(value='')))\n","\n","[11.16.20 05:52:11] Saved checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_1000334.pth.tar\n","[11.16.20 05:52:11] Removed checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_1000334.pth.tar\n","[11.16.20 05:52:11] Dev NLL: 04.57, F1: 51.41, EM: 51.18, AvNA: 53.10\n","[11.16.20 05:52:11] Visualizing in TensorBoard...\n","100% 129922/129922 [15:55<00:00, 136.02it/s, NLL=8.05, epoch=8, iteration=32487, lr=0.0001]\n","[11.16.20 05:56:50] Starting epoch 9 out of 40 epochs...\n","[11.16.20 05:58:08] Evaluating at step 1050352...\n","  8% 10976/129922 [01:18<15:57, 124.28it/s, NLL=5.24, epoch=9, iteration=32830, lr=0.0001]HBox(children=(FloatProgress(value=0.0, max=5951.0), HTML(value='')))\n","  8% 10976/129922 [01:30<15:57, 124.28it/s, NLL=5.24, epoch=9, iteration=32830, lr=0.0001]\n","[11.16.20 05:58:23] Saved checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_1050352.pth.tar\n","[11.16.20 05:58:23] Removed checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_1050352.pth.tar\n","[11.16.20 05:58:23] Dev NLL: 04.55, F1: 51.12, EM: 50.90, AvNA: 53.15\n","[11.16.20 05:58:23] Visualizing in TensorBoard...\n","[11.16.20 06:04:19] Evaluating at step 1100368...\n"," 47% 60992/129922 [07:29<07:33, 151.93it/s, NLL=5.38, epoch=9, iteration=34393, lr=0.0001]HBox(children=(FloatProgress(value=0.0, max=5951.0), HTML(value='')))\n"," 47% 60992/129922 [07:40<07:33, 151.93it/s, NLL=5.38, epoch=9, iteration=34393, lr=0.0001]\n","[11.16.20 06:04:34] Saved checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_1100368.pth.tar\n","[11.16.20 06:04:34] Removed checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_1100368.pth.tar\n","[11.16.20 06:04:34] Dev NLL: 04.53, F1: 51.16, EM: 50.95, AvNA: 53.23\n","[11.16.20 06:04:34] Visualizing in TensorBoard...\n","[11.16.20 06:10:29] Evaluating at step 1150384...\n"," 85% 111008/129922 [13:39<02:22, 132.96it/s, NLL=5.76, epoch=9, iteration=35956, lr=0.000557]HBox(children=(FloatProgress(value=0.0, max=5951.0), HTML(value='')))\n"," 85% 111008/129922 [13:50<02:22, 132.96it/s, NLL=5.76, epoch=9, iteration=35956, lr=0.000557]\n","[11.16.20 06:10:44] Saved checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_1150384.pth.tar\n","[11.16.20 06:10:44] Removed checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_1150384.pth.tar\n","[11.16.20 06:10:44] Dev NLL: 04.52, F1: 51.17, EM: 50.93, AvNA: 53.54\n","[11.16.20 06:10:44] Visualizing in TensorBoard...\n","100% 129922/129922 [16:11<00:00, 133.75it/s, NLL=7.88, epoch=9, iteration=36548, lr=0.000821]\n","[11.16.20 06:13:01] Starting epoch 10 out of 40 epochs...\n","[11.16.20 06:16:44] Evaluating at step 1200402...\n"," 24% 31104/129922 [03:42<11:53, 138.55it/s, NLL=6.65, epoch=10, iteration=37520, lr=0.001]HBox(children=(FloatProgress(value=0.0, max=5951.0), HTML(value='')))\n","\n","[11.16.20 06:16:59] Saved checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_1200402.pth.tar\n","[11.16.20 06:16:59] Removed checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_1200402.pth.tar\n","[11.16.20 06:16:59] Dev NLL: 04.56, F1: 51.24, EM: 51.07, AvNA: 52.97\n","[11.16.20 06:16:59] Visualizing in TensorBoard...\n","[11.16.20 06:22:55] Evaluating at step 1250418...\n"," 62% 81120/129922 [09:53<06:04, 134.05it/s, NLL=5.58, epoch=10, iteration=39083, lr=0.000554]HBox(children=(FloatProgress(value=0.0, max=5951.0), HTML(value='')))\n","\n","[11.16.20 06:23:09] Saved checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_1250418.pth.tar\n","[11.16.20 06:23:09] Removed checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_1250418.pth.tar\n","[11.16.20 06:23:09] Dev NLL: 04.61, F1: 51.30, EM: 51.18, AvNA: 52.56\n","[11.16.20 06:23:09] Visualizing in TensorBoard...\n","100% 129922/129922 [15:56<00:00, 135.83it/s, NLL=6.28, epoch=10, iteration=40609, lr=0.0001]\n","[11.16.20 06:28:58] Starting epoch 11 out of 40 epochs...\n","[11.16.20 06:29:07] Evaluating at step 1300436...\n","  1% 1216/129922 [00:09<14:11, 151.08it/s, NLL=5.72, epoch=11, iteration=40647, lr=0.0001]HBox(children=(FloatProgress(value=0.0, max=5951.0), HTML(value='')))\n","  1% 1216/129922 [00:20<14:11, 151.08it/s, NLL=5.72, epoch=11, iteration=40647, lr=0.0001]\n","[11.16.20 06:29:22] Saved checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_1300436.pth.tar\n","[11.16.20 06:29:22] Removed checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_1300436.pth.tar\n","[11.16.20 06:29:22] Dev NLL: 04.54, F1: 51.08, EM: 50.92, AvNA: 52.80\n","[11.16.20 06:29:22] Visualizing in TensorBoard...\n","[11.16.20 06:35:23] Evaluating at step 1350452...\n"," 39% 51232/129922 [06:25<09:25, 139.03it/s, NLL=5.6, epoch=11, iteration=42210, lr=0.0001]HBox(children=(FloatProgress(value=0.0, max=5951.0), HTML(value='')))\n","\n","[11.16.20 06:35:37] Saved checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_1350452.pth.tar\n","[11.16.20 06:35:37] Removed checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_1350452.pth.tar\n","[11.16.20 06:35:37] Dev NLL: 04.50, F1: 51.37, EM: 51.24, AvNA: 53.03\n","[11.16.20 06:35:37] Visualizing in TensorBoard...\n","[11.16.20 06:41:33] Evaluating at step 1400468...\n"," 78% 101248/129922 [12:35<03:15, 146.94it/s, NLL=5.26, epoch=11, iteration=43773, lr=0.0001]HBox(children=(FloatProgress(value=0.0, max=5951.0), HTML(value='')))\n","\n","[11.16.20 06:41:48] Saved checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_1400468.pth.tar\n","[11.16.20 06:41:48] Removed checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_1400468.pth.tar\n","[11.16.20 06:41:48] Dev NLL: 04.48, F1: 51.20, EM: 51.05, AvNA: 53.25\n","[11.16.20 06:41:48] Visualizing in TensorBoard...\n","100% 129922/129922 [16:13<00:00, 133.43it/s, NLL=8.27, epoch=11, iteration=44670, lr=0.0001]\n","[11.16.20 06:45:11] Starting epoch 12 out of 40 epochs...\n","[11.16.20 06:47:42] Evaluating at step 1450486...\n"," 16% 21344/129922 [02:30<12:05, 149.65it/s, NLL=6.31, epoch=12, iteration=45337, lr=0.000198]HBox(children=(FloatProgress(value=0.0, max=5951.0), HTML(value='')))\n","\n","[11.16.20 06:47:57] Saved checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_1450486.pth.tar\n","[11.16.20 06:47:57] Removed checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_1450486.pth.tar\n","[11.16.20 06:47:57] Dev NLL: 04.47, F1: 51.25, EM: 51.07, AvNA: 53.50\n","[11.16.20 06:47:57] Visualizing in TensorBoard...\n","[11.16.20 06:53:53] Evaluating at step 1500502...\n"," 55% 71360/129922 [08:41<07:40, 127.05it/s, NLL=6.28, epoch=12, iteration=46900, lr=0.000925]HBox(children=(FloatProgress(value=0.0, max=5951.0), HTML(value='')))\n","\n","[11.16.20 06:54:08] Saved checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_1500502.pth.tar\n","[11.16.20 06:54:08] Removed checkpoint: ./save/train/QANET_xfmr_lrwarmup_posenc_inside_encoder_0p05dropprob_lr1e-3_sinsched-03/step_1500502.pth.tar\n","[11.16.20 06:54:08] Dev NLL: 04.49, F1: 50.97, EM: 50.81, AvNA: 53.03\n","[11.16.20 06:54:08] Visualizing in TensorBoard...\n"," 74% 96736/129922 [11:58<04:06, 134.66it/s, NLL=6.31, epoch=12, iteration=47693, lr=0.000994]\n","Traceback (most recent call last):\n","  File \"./src/train.py\", line 309, in <module>\n","    main(train_args, model_type)\n","  File \"./src/train.py\", line 132, in main\n","    optimizer.zero_grad()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/optim/optimizer.py\", line 192, in zero_grad\n","    p.grad.zero_()\n","KeyboardInterrupt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"05SdAbx0PyhS"},"source":["%load_ext tensorboard\n","%tensorboard --logdir=./save/train/QANET_xfmr_NOdatashuffle_lrwarmup_posenc_inside_encoder_0p05dropprob-01"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1gBQU7_6kYar"},"source":["###Debug QANet"]},{"cell_type":"code","metadata":{"id":"5JWBzzJZECGO","executionInfo":{"elapsed":338,"status":"ok","timestamp":1598671581866,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"01f6d75f-0c13-4aed-9ec0-455cf56d4c19","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import util\n","importlib.reload(util)\n","\n","import QANet_xfmr_based\n","importlib.reload(QANet_xfmr_based)\n","\n","import QANet\n","importlib.reload(QANet)\n","\n","import BiDAF\n","importlib.reload(BiDAF)\n","\n","##############################\n","vocab_size = int(1e4)\n","char_vocab_size = 100\n","device = util.get_available_devices()[0]\n","word_vocab = torch.rand(vocab_size, 300).to(device)\n","\n","# model = QANet_xfmr_based.QANet_xfmr(word_vocab, 96)\n","model = QANet.QANet(word_vocab, 96)\n","# model = BiDAF.BiDAF_with_char_embed(word_vocab, 200)\n","model = model.to(device)\n","util.get_model_info(model)\n","\n","# Total size 1211161, Total size 4.844643999999997MB and Tot Num parameters 108 #QANet_xfmr 96dim\n","# Total size 1152601, Total size 4.610404000000002MB and Tot Num parameters 184 #QANet 96dim\n","\n","#Total size 13703835, Total size 54.815340000000205MB and Tot Num parameters 376 (QANet)\n","#Total size 6234605, Total size 24.93842MB and Tot Num parameters 57 (BiDaf with chan embed)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["emb.char_embed.weight, torch.Size([1378, 200]), param total dim: 275600, 1.1024MB\n","emb.char_conv.weight, torch.Size([200, 200, 5]), param total dim: 200000, 0.8MB\n","emb.char_conv.bias, torch.Size([200]), param total dim: 200, 0.0008MB\n","emb.proj_combine.weight, torch.Size([96, 500]), param total dim: 48000, 0.192MB\n","emb.hwy.transforms.0.weight, torch.Size([96, 96]), param total dim: 9216, 0.036864MB\n","emb.hwy.transforms.0.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","emb.hwy.transforms.1.weight, torch.Size([96, 96]), param total dim: 9216, 0.036864MB\n","emb.hwy.transforms.1.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","emb.hwy.gates.0.weight, torch.Size([96, 96]), param total dim: 9216, 0.036864MB\n","emb.hwy.gates.0.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","emb.hwy.gates.1.weight, torch.Size([96, 96]), param total dim: 9216, 0.036864MB\n","emb.hwy.gates.1.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","emb_enc.encoder_blocks_list.0.conv_norm_layers.0.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","emb_enc.encoder_blocks_list.0.conv_norm_layers.0.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","emb_enc.encoder_blocks_list.0.conv_norm_layers.1.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","emb_enc.encoder_blocks_list.0.conv_norm_layers.1.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","emb_enc.encoder_blocks_list.0.conv_norm_layers.2.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","emb_enc.encoder_blocks_list.0.conv_norm_layers.2.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","emb_enc.encoder_blocks_list.0.conv_norm_layers.3.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","emb_enc.encoder_blocks_list.0.conv_norm_layers.3.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","emb_enc.encoder_blocks_list.0.conv_layers.0.separable_layer.weight, torch.Size([96, 1, 7]), param total dim: 672, 0.002688MB\n","emb_enc.encoder_blocks_list.0.conv_layers.0.separable_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","emb_enc.encoder_blocks_list.0.conv_layers.0.depthwise_layer.weight, torch.Size([96, 96, 1]), param total dim: 9216, 0.036864MB\n","emb_enc.encoder_blocks_list.0.conv_layers.0.depthwise_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","emb_enc.encoder_blocks_list.0.conv_layers.1.separable_layer.weight, torch.Size([96, 1, 7]), param total dim: 672, 0.002688MB\n","emb_enc.encoder_blocks_list.0.conv_layers.1.separable_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","emb_enc.encoder_blocks_list.0.conv_layers.1.depthwise_layer.weight, torch.Size([96, 96, 1]), param total dim: 9216, 0.036864MB\n","emb_enc.encoder_blocks_list.0.conv_layers.1.depthwise_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","emb_enc.encoder_blocks_list.0.conv_layers.2.separable_layer.weight, torch.Size([96, 1, 7]), param total dim: 672, 0.002688MB\n","emb_enc.encoder_blocks_list.0.conv_layers.2.separable_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","emb_enc.encoder_blocks_list.0.conv_layers.2.depthwise_layer.weight, torch.Size([96, 96, 1]), param total dim: 9216, 0.036864MB\n","emb_enc.encoder_blocks_list.0.conv_layers.2.depthwise_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","emb_enc.encoder_blocks_list.0.conv_layers.3.separable_layer.weight, torch.Size([96, 1, 7]), param total dim: 672, 0.002688MB\n","emb_enc.encoder_blocks_list.0.conv_layers.3.separable_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","emb_enc.encoder_blocks_list.0.conv_layers.3.depthwise_layer.weight, torch.Size([96, 96, 1]), param total dim: 9216, 0.036864MB\n","emb_enc.encoder_blocks_list.0.conv_layers.3.depthwise_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","emb_enc.encoder_blocks_list.0.self_atten_layer_norm.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","emb_enc.encoder_blocks_list.0.self_atten_layer_norm.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","emb_enc.encoder_blocks_list.0.self_attention.mha.in_proj_weight, torch.Size([288, 96]), param total dim: 27648, 0.110592MB\n","emb_enc.encoder_blocks_list.0.self_attention.mha.out_proj.weight, torch.Size([96, 96]), param total dim: 9216, 0.036864MB\n","emb_enc.encoder_blocks_list.0.feed_fwd_layer_norm.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","emb_enc.encoder_blocks_list.0.feed_fwd_layer_norm.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","emb_enc.encoder_blocks_list.0.feed_fwd.weight, torch.Size([96, 96]), param total dim: 9216, 0.036864MB\n","att.c_weight, torch.Size([96, 1]), param total dim: 96, 0.000384MB\n","att.q_weight, torch.Size([96, 1]), param total dim: 96, 0.000384MB\n","att.cq_weight, torch.Size([1, 1, 96]), param total dim: 96, 0.000384MB\n","att.bias, torch.Size([1]), param total dim: 1, 4e-06MB\n","mod_enc_input_prj.weight, torch.Size([96, 384]), param total dim: 36864, 0.147456MB\n","mod_enc_input_prj.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.0.conv_norm_layers.0.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.0.conv_norm_layers.0.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.0.conv_norm_layers.1.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.0.conv_norm_layers.1.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.0.conv_layers.0.separable_layer.weight, torch.Size([96, 1, 5]), param total dim: 480, 0.00192MB\n","mod_enc.encoder_blocks_list.0.conv_layers.0.separable_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.0.conv_layers.0.depthwise_layer.weight, torch.Size([96, 96, 1]), param total dim: 9216, 0.036864MB\n","mod_enc.encoder_blocks_list.0.conv_layers.0.depthwise_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.0.conv_layers.1.separable_layer.weight, torch.Size([96, 1, 5]), param total dim: 480, 0.00192MB\n","mod_enc.encoder_blocks_list.0.conv_layers.1.separable_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.0.conv_layers.1.depthwise_layer.weight, torch.Size([96, 96, 1]), param total dim: 9216, 0.036864MB\n","mod_enc.encoder_blocks_list.0.conv_layers.1.depthwise_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.0.self_atten_layer_norm.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.0.self_atten_layer_norm.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.0.self_attention.mha.in_proj_weight, torch.Size([288, 96]), param total dim: 27648, 0.110592MB\n","mod_enc.encoder_blocks_list.0.self_attention.mha.out_proj.weight, torch.Size([96, 96]), param total dim: 9216, 0.036864MB\n","mod_enc.encoder_blocks_list.0.feed_fwd_layer_norm.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.0.feed_fwd_layer_norm.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.0.feed_fwd.weight, torch.Size([96, 96]), param total dim: 9216, 0.036864MB\n","mod_enc.encoder_blocks_list.1.conv_norm_layers.0.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.1.conv_norm_layers.0.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.1.conv_norm_layers.1.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.1.conv_norm_layers.1.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.1.conv_layers.0.separable_layer.weight, torch.Size([96, 1, 5]), param total dim: 480, 0.00192MB\n","mod_enc.encoder_blocks_list.1.conv_layers.0.separable_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.1.conv_layers.0.depthwise_layer.weight, torch.Size([96, 96, 1]), param total dim: 9216, 0.036864MB\n","mod_enc.encoder_blocks_list.1.conv_layers.0.depthwise_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.1.conv_layers.1.separable_layer.weight, torch.Size([96, 1, 5]), param total dim: 480, 0.00192MB\n","mod_enc.encoder_blocks_list.1.conv_layers.1.separable_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.1.conv_layers.1.depthwise_layer.weight, torch.Size([96, 96, 1]), param total dim: 9216, 0.036864MB\n","mod_enc.encoder_blocks_list.1.conv_layers.1.depthwise_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.1.self_atten_layer_norm.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.1.self_atten_layer_norm.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.1.self_attention.mha.in_proj_weight, torch.Size([288, 96]), param total dim: 27648, 0.110592MB\n","mod_enc.encoder_blocks_list.1.self_attention.mha.out_proj.weight, torch.Size([96, 96]), param total dim: 9216, 0.036864MB\n","mod_enc.encoder_blocks_list.1.feed_fwd_layer_norm.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.1.feed_fwd_layer_norm.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.1.feed_fwd.weight, torch.Size([96, 96]), param total dim: 9216, 0.036864MB\n","mod_enc.encoder_blocks_list.2.conv_norm_layers.0.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.2.conv_norm_layers.0.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.2.conv_norm_layers.1.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.2.conv_norm_layers.1.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.2.conv_layers.0.separable_layer.weight, torch.Size([96, 1, 5]), param total dim: 480, 0.00192MB\n","mod_enc.encoder_blocks_list.2.conv_layers.0.separable_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.2.conv_layers.0.depthwise_layer.weight, torch.Size([96, 96, 1]), param total dim: 9216, 0.036864MB\n","mod_enc.encoder_blocks_list.2.conv_layers.0.depthwise_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.2.conv_layers.1.separable_layer.weight, torch.Size([96, 1, 5]), param total dim: 480, 0.00192MB\n","mod_enc.encoder_blocks_list.2.conv_layers.1.separable_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.2.conv_layers.1.depthwise_layer.weight, torch.Size([96, 96, 1]), param total dim: 9216, 0.036864MB\n","mod_enc.encoder_blocks_list.2.conv_layers.1.depthwise_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.2.self_atten_layer_norm.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.2.self_atten_layer_norm.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.2.self_attention.mha.in_proj_weight, torch.Size([288, 96]), param total dim: 27648, 0.110592MB\n","mod_enc.encoder_blocks_list.2.self_attention.mha.out_proj.weight, torch.Size([96, 96]), param total dim: 9216, 0.036864MB\n","mod_enc.encoder_blocks_list.2.feed_fwd_layer_norm.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.2.feed_fwd_layer_norm.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.2.feed_fwd.weight, torch.Size([96, 96]), param total dim: 9216, 0.036864MB\n","mod_enc.encoder_blocks_list.3.conv_norm_layers.0.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.3.conv_norm_layers.0.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.3.conv_norm_layers.1.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.3.conv_norm_layers.1.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.3.conv_layers.0.separable_layer.weight, torch.Size([96, 1, 5]), param total dim: 480, 0.00192MB\n","mod_enc.encoder_blocks_list.3.conv_layers.0.separable_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.3.conv_layers.0.depthwise_layer.weight, torch.Size([96, 96, 1]), param total dim: 9216, 0.036864MB\n","mod_enc.encoder_blocks_list.3.conv_layers.0.depthwise_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.3.conv_layers.1.separable_layer.weight, torch.Size([96, 1, 5]), param total dim: 480, 0.00192MB\n","mod_enc.encoder_blocks_list.3.conv_layers.1.separable_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.3.conv_layers.1.depthwise_layer.weight, torch.Size([96, 96, 1]), param total dim: 9216, 0.036864MB\n","mod_enc.encoder_blocks_list.3.conv_layers.1.depthwise_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.3.self_atten_layer_norm.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.3.self_atten_layer_norm.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.3.self_attention.mha.in_proj_weight, torch.Size([288, 96]), param total dim: 27648, 0.110592MB\n","mod_enc.encoder_blocks_list.3.self_attention.mha.out_proj.weight, torch.Size([96, 96]), param total dim: 9216, 0.036864MB\n","mod_enc.encoder_blocks_list.3.feed_fwd_layer_norm.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.3.feed_fwd_layer_norm.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.3.feed_fwd.weight, torch.Size([96, 96]), param total dim: 9216, 0.036864MB\n","mod_enc.encoder_blocks_list.4.conv_norm_layers.0.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.4.conv_norm_layers.0.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.4.conv_norm_layers.1.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.4.conv_norm_layers.1.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.4.conv_layers.0.separable_layer.weight, torch.Size([96, 1, 5]), param total dim: 480, 0.00192MB\n","mod_enc.encoder_blocks_list.4.conv_layers.0.separable_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.4.conv_layers.0.depthwise_layer.weight, torch.Size([96, 96, 1]), param total dim: 9216, 0.036864MB\n","mod_enc.encoder_blocks_list.4.conv_layers.0.depthwise_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.4.conv_layers.1.separable_layer.weight, torch.Size([96, 1, 5]), param total dim: 480, 0.00192MB\n","mod_enc.encoder_blocks_list.4.conv_layers.1.separable_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.4.conv_layers.1.depthwise_layer.weight, torch.Size([96, 96, 1]), param total dim: 9216, 0.036864MB\n","mod_enc.encoder_blocks_list.4.conv_layers.1.depthwise_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.4.self_atten_layer_norm.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.4.self_atten_layer_norm.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.4.self_attention.mha.in_proj_weight, torch.Size([288, 96]), param total dim: 27648, 0.110592MB\n","mod_enc.encoder_blocks_list.4.self_attention.mha.out_proj.weight, torch.Size([96, 96]), param total dim: 9216, 0.036864MB\n","mod_enc.encoder_blocks_list.4.feed_fwd_layer_norm.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.4.feed_fwd_layer_norm.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.4.feed_fwd.weight, torch.Size([96, 96]), param total dim: 9216, 0.036864MB\n","mod_enc.encoder_blocks_list.5.conv_norm_layers.0.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.5.conv_norm_layers.0.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.5.conv_norm_layers.1.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.5.conv_norm_layers.1.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.5.conv_layers.0.separable_layer.weight, torch.Size([96, 1, 5]), param total dim: 480, 0.00192MB\n","mod_enc.encoder_blocks_list.5.conv_layers.0.separable_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.5.conv_layers.0.depthwise_layer.weight, torch.Size([96, 96, 1]), param total dim: 9216, 0.036864MB\n","mod_enc.encoder_blocks_list.5.conv_layers.0.depthwise_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.5.conv_layers.1.separable_layer.weight, torch.Size([96, 1, 5]), param total dim: 480, 0.00192MB\n","mod_enc.encoder_blocks_list.5.conv_layers.1.separable_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.5.conv_layers.1.depthwise_layer.weight, torch.Size([96, 96, 1]), param total dim: 9216, 0.036864MB\n","mod_enc.encoder_blocks_list.5.conv_layers.1.depthwise_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.5.self_atten_layer_norm.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.5.self_atten_layer_norm.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.5.self_attention.mha.in_proj_weight, torch.Size([288, 96]), param total dim: 27648, 0.110592MB\n","mod_enc.encoder_blocks_list.5.self_attention.mha.out_proj.weight, torch.Size([96, 96]), param total dim: 9216, 0.036864MB\n","mod_enc.encoder_blocks_list.5.feed_fwd_layer_norm.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.5.feed_fwd_layer_norm.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.5.feed_fwd.weight, torch.Size([96, 96]), param total dim: 9216, 0.036864MB\n","mod_enc.encoder_blocks_list.6.conv_norm_layers.0.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.6.conv_norm_layers.0.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.6.conv_norm_layers.1.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.6.conv_norm_layers.1.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.6.conv_layers.0.separable_layer.weight, torch.Size([96, 1, 5]), param total dim: 480, 0.00192MB\n","mod_enc.encoder_blocks_list.6.conv_layers.0.separable_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.6.conv_layers.0.depthwise_layer.weight, torch.Size([96, 96, 1]), param total dim: 9216, 0.036864MB\n","mod_enc.encoder_blocks_list.6.conv_layers.0.depthwise_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.6.conv_layers.1.separable_layer.weight, torch.Size([96, 1, 5]), param total dim: 480, 0.00192MB\n","mod_enc.encoder_blocks_list.6.conv_layers.1.separable_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.6.conv_layers.1.depthwise_layer.weight, torch.Size([96, 96, 1]), param total dim: 9216, 0.036864MB\n","mod_enc.encoder_blocks_list.6.conv_layers.1.depthwise_layer.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.6.self_atten_layer_norm.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.6.self_atten_layer_norm.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.6.self_attention.mha.in_proj_weight, torch.Size([288, 96]), param total dim: 27648, 0.110592MB\n","mod_enc.encoder_blocks_list.6.self_attention.mha.out_proj.weight, torch.Size([96, 96]), param total dim: 9216, 0.036864MB\n","mod_enc.encoder_blocks_list.6.feed_fwd_layer_norm.weight, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.6.feed_fwd_layer_norm.bias, torch.Size([96]), param total dim: 96, 0.000384MB\n","mod_enc.encoder_blocks_list.6.feed_fwd.weight, torch.Size([96, 96]), param total dim: 9216, 0.036864MB\n","out.att_linear_1.weight, torch.Size([1, 192]), param total dim: 192, 0.000768MB\n","out.att_linear_2.weight, torch.Size([1, 192]), param total dim: 192, 0.000768MB\n","Total size 1152601, Total size 4.610404000000002MB and Tot Num parameters 184\n","\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w9I4ASRHUSvJ","executionInfo":{"elapsed":7400,"status":"ok","timestamp":1595123164577,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"9a8e7769-95f8-4ced-dcfc-df15d11cb3af","colab":{"base_uri":"https://localhost:8080/","height":199}},"source":["import time\n","strt = time.time()\n","!python ./src/QANet.py\n","print(time.time()-strt, 'seconds')\n","QANet.GPU_Memory_Usage()\n","#12.2 sec and 85% (my version) vs 9.2 sec and 79% (pytorch)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["EPOCH: 0\n","AFTER LOSS.BACKWARD\n","memory used: 13.1484375 (GiB) i.e. 82.698%\n","***************\n","EPOCH: 1\n","AFTER LOSS.BACKWARD\n","memory used: 13.154296875 (GiB) i.e. 82.735%\n","***************\n","7.043651103973389 seconds\n","memory used: 0.7626953125 (GiB) i.e. 4.797%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c5YZRY8OPk1V","executionInfo":{"elapsed":11925,"status":"ok","timestamp":1598638145265,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"1167639c-cf88-46f9-ea95-58f2e34e2b21","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python -m torch.utils.bottleneck ./src/QANet_xfmr_based.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["`bottleneck` is a tool that can be used as an initial step for debugging\n","bottlenecks in your program.\n","\n","It summarizes runs of your script with the Python profiler and PyTorch's\n","autograd profiler. Because your script will be profiled, please ensure that it\n","exits in a finite amount of time.\n","\n","For more complicated uses of the profilers, please see\n","https://docs.python.org/3/library/profile.html and\n","https://pytorch.org/docs/master/autograd.html#profiler for more information.\n","Running environment analysis...\n","Running your script with cProfile\n","Profiling QANet_xfmr's forward call\n","         4850 function calls (4607 primitive calls) in 0.260 seconds\n","\n","   Ordered by: internal time\n","\n","   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n","       87    0.119    0.001    0.119    0.001 {built-in method equal}\n","      129    0.042    0.000    0.042    0.000 {method 'matmul' of 'torch._C._TensorBase' objects}\n","        5    0.019    0.004    0.019    0.004 {method 'to' of 'torch._C._TensorBase' objects}\n","       61    0.009    0.000    0.009    0.000 {built-in method bmm}\n","       29    0.008    0.000    0.167    0.006 functional.py:3124(multi_head_attention_forward)\n","       29    0.008    0.000    0.008    0.000 {method 'masked_fill' of 'torch._C._TensorBase' objects}\n","        2    0.007    0.004    0.007    0.004 {built-in method conv1d}\n","       29    0.007    0.000    0.193    0.007 QANet_xfmr_based.py:222(forward)\n","       58    0.006    0.000    0.006    0.000 {built-in method layer_norm}\n","      116    0.006    0.000    0.006    0.000 {method 'contiguous' of 'torch._C._TensorBase' objects}\n","       31    0.005    0.000    0.005    0.000 {method 'sum' of 'torch._C._TensorBase' objects}\n","       58    0.003    0.000    0.003    0.000 QANet.py:251(residual_layer_dropout)\n","       29    0.002    0.000    0.171    0.006 QANet.py:430(forward)\n","        5    0.002    0.000    0.002    0.000 {built-in method cat}\n","    239/1    0.002    0.000    0.260    0.260 module.py:531(__call__)\n","      209    0.002    0.000    0.002    0.000 {method 'view' of 'torch._C._TensorBase' objects}\n","      129    0.001    0.000    0.001    0.000 {method 't' of 'torch._C._TensorBase' objects}\n","        4    0.001    0.000    0.001    0.000 {built-in method embedding}\n","       33    0.001    0.000    0.001    0.000 {built-in method relu}\n","        2    0.001    0.000    0.002    0.001 layers.py:138(forward)\n","       31    0.001    0.000    0.001    0.000 {method 'softmax' of 'torch._C._TensorBase' objects}\n","      150    0.001    0.000    0.001    0.000 {method 'transpose' of 'torch._C._TensorBase' objects}\n","      129    0.001    0.000    0.043    0.000 functional.py:1355(linear)\n","       29    0.001    0.000    0.001    0.000 {method 'logical_not' of 'torch._C._TensorBase' objects}\n","       58    0.001    0.000    0.001    0.000 {built-in method rand}\n","      669    0.001    0.000    0.001    0.000 module.py:571(__getattr__)\n","       29    0.000    0.000    0.000    0.000 {method 'chunk' of 'torch._C._TensorBase' objects}\n","       95    0.000    0.000    0.001    0.000 functional.py:788(dropout)\n","       58    0.000    0.000    0.000    0.000 {method 'permute' of 'torch._C._TensorBase' objects}\n","        8    0.000    0.000    0.000    0.000 {built-in method rsub}\n","        5    0.000    0.000    0.214    0.043 QANet_xfmr_based.py:188(forward)\n","      239    0.000    0.000    0.000    0.000 {built-in method torch._C._get_tracing_state}\n","       29    0.000    0.000    0.167    0.006 activation.py:729(forward)\n","       95    0.000    0.000    0.000    0.000 {built-in method dropout}\n","        4    0.000    0.000    0.000    0.000 {built-in method sigmoid}\n","        5    0.000    0.000    0.020    0.004 QANet.py:407(forward)\n","       68    0.000    0.000    0.000    0.000 {method 'unsqueeze' of 'torch._C._TensorBase' objects}\n","       31    0.000    0.000    0.000    0.000 {method 'squeeze' of 'torch._C._TensorBase' objects}\n","        4    0.000    0.000    0.001    0.000 util.py:410(masked_softmax)\n","      270    0.000    0.000    0.000    0.000 {method 'size' of 'torch._C._TensorBase' objects}\n","        2    0.000    0.000    0.000    0.000 {built-in method max}\n","       58    0.000    0.000    0.007    0.000 normalization.py:151(forward)\n","        1    0.000    0.000    0.260    0.260 QANet_xfmr_based.py:116(forward)\n","        5    0.000    0.000    0.000    0.000 {built-in method zeros}\n","       71    0.000    0.000    0.035    0.000 linear.py:86(forward)\n","        5    0.000    0.000    0.000    0.000 {method 'clone' of 'torch._C._TensorBase' objects}\n","       58    0.000    0.000    0.007    0.000 functional.py:1689(layer_norm)\n","        3    0.000    0.000    0.000    0.000 {built-in method matmul}\n","        2    0.000    0.000    0.000    0.000 {built-in method zeros_like}\n","      487    0.000    0.000    0.000    0.000 {method 'values' of 'collections.OrderedDict' objects}\n","        1    0.000    0.000    0.000    0.000 layers.py:244(get_similarity_matrix)\n","        2    0.000    0.000    0.044    0.022 QANet.py:200(forward)\n","       95    0.000    0.000    0.000    0.000 _VF.py:11(__getattr__)\n","       95    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n","        4    0.000    0.000    0.000    0.000 {method 'type' of 'torch._C._TensorBase' objects}\n","        2    0.000    0.000    0.000    0.000 {method 'log_softmax' of 'torch._C._TensorBase' objects}\n","  249/244    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n","       58    0.000    0.000    0.000    0.000 __init__.py:31(__get__)\n","      129    0.000    0.000    0.000    0.000 {method 'dim' of 'torch._C._TensorBase' objects}\n","       31    0.000    0.000    0.001    0.000 functional.py:1202(softmax)\n","        1    0.000    0.000    0.001    0.001 layers.py:226(forward)\n","       33    0.000    0.000    0.001    0.000 functional.py:904(relu)\n","        2    0.000    0.000    0.000    0.000 {method 'expand' of 'torch._C._TensorBase' objects}\n","       58    0.000    0.000    0.000    0.000 {built-in method torch._C._get_cudnn_enabled}\n","        1    0.000    0.000    0.001    0.001 QANet.py:554(forward)\n","        8    0.000    0.000    0.000    0.000 tensor.py:361(__rsub__)\n","        9    0.000    0.000    0.000    0.000 container.py:157(__iter__)\n","       58    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n","        4    0.000    0.000    0.001    0.000 sparse.py:111(forward)\n","        2    0.000    0.000    0.007    0.004 conv.py:195(forward)\n","        5    0.000    0.000    0.000    0.000 {method 'detach' of 'torch._C._TensorBase' objects}\n","        2    0.000    0.000    0.000    0.000 functional.py:1297(log_softmax)\n","        5    0.000    0.000    0.000    0.000 container.py:154(__len__)\n","        4    0.000    0.000    0.001    0.000 functional.py:1406(embedding)\n","        9    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n","        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n","\n","\n","EPOCH: 0\n","AFTER LOSS.BACKWARD\n","memory used: 11.9970703125 (GiB) i.e. 75.457%\n","***************\n","EPOCH: 1\n","AFTER LOSS.BACKWARD\n","memory used: 12.0009765625 (GiB) i.e. 75.481%\n","***************\n","Running your script with the autograd profiler...\n","Profiling QANet_xfmr's forward call\n","         4850 function calls (4607 primitive calls) in 0.231 seconds\n","\n","   Ordered by: internal time\n","\n","   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n","       87    0.133    0.002    0.133    0.002 {built-in method equal}\n","        5    0.056    0.011    0.056    0.011 {method 'to' of 'torch._C._TensorBase' objects}\n","      129    0.005    0.000    0.005    0.000 {method 'matmul' of 'torch._C._TensorBase' objects}\n","       29    0.005    0.000    0.170    0.006 QANet_xfmr_based.py:222(forward)\n","       58    0.004    0.000    0.004    0.000 {built-in method layer_norm}\n","       29    0.004    0.000    0.149    0.005 functional.py:3124(multi_head_attention_forward)\n","       58    0.003    0.000    0.003    0.000 QANet.py:251(residual_layer_dropout)\n","      116    0.002    0.000    0.002    0.000 {method 'contiguous' of 'torch._C._TensorBase' objects}\n","       29    0.002    0.000    0.002    0.000 {method 'masked_fill' of 'torch._C._TensorBase' objects}\n","      209    0.001    0.000    0.001    0.000 {method 'view' of 'torch._C._TensorBase' objects}\n","    239/1    0.001    0.000    0.231    0.231 module.py:531(__call__)\n","       61    0.001    0.000    0.001    0.000 {built-in method bmm}\n","       29    0.001    0.000    0.152    0.005 QANet.py:430(forward)\n","      129    0.001    0.000    0.001    0.000 {method 't' of 'torch._C._TensorBase' objects}\n","       31    0.001    0.000    0.001    0.000 {method 'sum' of 'torch._C._TensorBase' objects}\n","      150    0.001    0.000    0.001    0.000 {method 'transpose' of 'torch._C._TensorBase' objects}\n","       33    0.001    0.000    0.001    0.000 {built-in method relu}\n","       31    0.001    0.000    0.001    0.000 {method 'softmax' of 'torch._C._TensorBase' objects}\n","       58    0.001    0.000    0.001    0.000 {built-in method rand}\n","       29    0.001    0.000    0.001    0.000 {method 'logical_not' of 'torch._C._TensorBase' objects}\n","      129    0.001    0.000    0.007    0.000 functional.py:1355(linear)\n","      669    0.000    0.000    0.000    0.000 module.py:571(__getattr__)\n","       95    0.000    0.000    0.001    0.000 functional.py:788(dropout)\n","        2    0.000    0.000    0.000    0.000 {built-in method conv1d}\n","       29    0.000    0.000    0.000    0.000 {method 'chunk' of 'torch._C._TensorBase' objects}\n","        5    0.000    0.000    0.227    0.045 QANet_xfmr_based.py:188(forward)\n","       58    0.000    0.000    0.000    0.000 {method 'permute' of 'torch._C._TensorBase' objects}\n","       95    0.000    0.000    0.000    0.000 {built-in method dropout}\n","      239    0.000    0.000    0.000    0.000 {built-in method torch._C._get_tracing_state}\n","        5    0.000    0.000    0.056    0.011 QANet.py:407(forward)\n","       68    0.000    0.000    0.000    0.000 {method 'unsqueeze' of 'torch._C._TensorBase' objects}\n","        2    0.000    0.000    0.001    0.001 layers.py:138(forward)\n","        8    0.000    0.000    0.000    0.000 {built-in method rsub}\n","       29    0.000    0.000    0.149    0.005 activation.py:729(forward)\n","        5    0.000    0.000    0.000    0.000 {built-in method cat}\n","        4    0.000    0.000    0.001    0.000 util.py:410(masked_softmax)\n","       31    0.000    0.000    0.000    0.000 {method 'squeeze' of 'torch._C._TensorBase' objects}\n","       58    0.000    0.000    0.005    0.000 normalization.py:151(forward)\n","      270    0.000    0.000    0.000    0.000 {method 'size' of 'torch._C._TensorBase' objects}\n","       58    0.000    0.000    0.005    0.000 functional.py:1689(layer_norm)\n","        5    0.000    0.000    0.000    0.000 {method 'clone' of 'torch._C._TensorBase' objects}\n","       71    0.000    0.000    0.004    0.000 linear.py:86(forward)\n","        4    0.000    0.000    0.000    0.000 {built-in method sigmoid}\n","        3    0.000    0.000    0.000    0.000 {built-in method matmul}\n","        4    0.000    0.000    0.000    0.000 {built-in method embedding}\n","        1    0.000    0.000    0.231    0.231 QANet_xfmr_based.py:116(forward)\n","      487    0.000    0.000    0.000    0.000 {method 'values' of 'collections.OrderedDict' objects}\n","        5    0.000    0.000    0.000    0.000 {built-in method zeros}\n","        1    0.000    0.000    0.000    0.000 layers.py:244(get_similarity_matrix)\n","        2    0.000    0.000    0.003    0.001 QANet.py:200(forward)\n","       95    0.000    0.000    0.000    0.000 _VF.py:11(__getattr__)\n","        2    0.000    0.000    0.000    0.000 {built-in method max}\n","      129    0.000    0.000    0.000    0.000 {method 'dim' of 'torch._C._TensorBase' objects}\n","        4    0.000    0.000    0.000    0.000 {method 'type' of 'torch._C._TensorBase' objects}\n","       95    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n","        2    0.000    0.000    0.000    0.000 {built-in method zeros_like}\n","        2    0.000    0.000    0.000    0.000 {method 'log_softmax' of 'torch._C._TensorBase' objects}\n","       58    0.000    0.000    0.000    0.000 __init__.py:31(__get__)\n","  249/244    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n","        1    0.000    0.000    0.001    0.001 layers.py:226(forward)\n","       33    0.000    0.000    0.001    0.000 functional.py:904(relu)\n","       31    0.000    0.000    0.001    0.000 functional.py:1202(softmax)\n","       58    0.000    0.000    0.000    0.000 {built-in method torch._C._get_cudnn_enabled}\n","       58    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n","        1    0.000    0.000    0.001    0.001 QANet.py:554(forward)\n","        5    0.000    0.000    0.000    0.000 {method 'detach' of 'torch._C._TensorBase' objects}\n","        8    0.000    0.000    0.000    0.000 tensor.py:361(__rsub__)\n","        4    0.000    0.000    0.000    0.000 sparse.py:111(forward)\n","        2    0.000    0.000    0.000    0.000 {method 'expand' of 'torch._C._TensorBase' objects}\n","        2    0.000    0.000    0.000    0.000 conv.py:195(forward)\n","        9    0.000    0.000    0.000    0.000 container.py:157(__iter__)\n","        2    0.000    0.000    0.000    0.000 functional.py:1297(log_softmax)\n","        4    0.000    0.000    0.000    0.000 functional.py:1406(embedding)\n","        9    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n","        5    0.000    0.000    0.000    0.000 container.py:154(__len__)\n","        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n","\n","\n","EPOCH: 0\n","AFTER LOSS.BACKWARD\n","memory used: 12.0009765625 (GiB) i.e. 75.481%\n","***************\n","EPOCH: 1\n","AFTER LOSS.BACKWARD\n","memory used: 12.0009765625 (GiB) i.e. 75.481%\n","***************\n","Profiling QANet_xfmr's forward call\n","         4850 function calls (4607 primitive calls) in 0.246 seconds\n","\n","   Ordered by: internal time\n","\n","   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n","       87    0.135    0.002    0.135    0.002 {built-in method equal}\n","        5    0.056    0.011    0.056    0.011 {method 'to' of 'torch._C._TensorBase' objects}\n","      129    0.007    0.000    0.007    0.000 {method 'matmul' of 'torch._C._TensorBase' objects}\n","       58    0.006    0.000    0.006    0.000 {built-in method layer_norm}\n","       29    0.006    0.000    0.184    0.006 QANet_xfmr_based.py:222(forward)\n","       58    0.004    0.000    0.004    0.000 QANet.py:251(residual_layer_dropout)\n","       29    0.004    0.000    0.157    0.005 functional.py:3124(multi_head_attention_forward)\n","      150    0.002    0.000    0.002    0.000 {method 'transpose' of 'torch._C._TensorBase' objects}\n","      116    0.002    0.000    0.002    0.000 {method 'contiguous' of 'torch._C._TensorBase' objects}\n","      209    0.002    0.000    0.002    0.000 {method 'view' of 'torch._C._TensorBase' objects}\n","       29    0.002    0.000    0.002    0.000 {method 'masked_fill' of 'torch._C._TensorBase' objects}\n","      129    0.002    0.000    0.002    0.000 {method 't' of 'torch._C._TensorBase' objects}\n","       61    0.001    0.000    0.001    0.000 {built-in method bmm}\n","    239/1    0.001    0.000    0.246    0.246 module.py:531(__call__)\n","       29    0.001    0.000    0.161    0.006 QANet.py:430(forward)\n","       29    0.001    0.000    0.001    0.000 {method 'logical_not' of 'torch._C._TensorBase' objects}\n","       31    0.001    0.000    0.001    0.000 {method 'sum' of 'torch._C._TensorBase' objects}\n","       31    0.001    0.000    0.001    0.000 {method 'softmax' of 'torch._C._TensorBase' objects}\n","       33    0.001    0.000    0.001    0.000 {built-in method relu}\n","       95    0.001    0.000    0.001    0.000 {built-in method dropout}\n","       58    0.001    0.000    0.001    0.000 {built-in method rand}\n","       29    0.001    0.000    0.001    0.000 {method 'chunk' of 'torch._C._TensorBase' objects}\n","       58    0.001    0.000    0.001    0.000 {method 'permute' of 'torch._C._TensorBase' objects}\n","      129    0.001    0.000    0.010    0.000 functional.py:1355(linear)\n","       68    0.000    0.000    0.000    0.000 {method 'unsqueeze' of 'torch._C._TensorBase' objects}\n","        2    0.000    0.000    0.000    0.000 {built-in method conv1d}\n","      669    0.000    0.000    0.000    0.000 module.py:571(__getattr__)\n","        5    0.000    0.000    0.242    0.048 QANet_xfmr_based.py:188(forward)\n","       95    0.000    0.000    0.001    0.000 functional.py:788(dropout)\n","        5    0.000    0.000    0.057    0.011 QANet.py:407(forward)\n","      239    0.000    0.000    0.000    0.000 {built-in method torch._C._get_tracing_state}\n","       31    0.000    0.000    0.000    0.000 {method 'squeeze' of 'torch._C._TensorBase' objects}\n","        4    0.000    0.000    0.001    0.000 util.py:410(masked_softmax)\n","        2    0.000    0.000    0.001    0.001 layers.py:138(forward)\n","        8    0.000    0.000    0.000    0.000 {built-in method rsub}\n","        5    0.000    0.000    0.000    0.000 {built-in method cat}\n","       29    0.000    0.000    0.158    0.005 activation.py:729(forward)\n","        3    0.000    0.000    0.000    0.000 {built-in method matmul}\n","      270    0.000    0.000    0.000    0.000 {method 'size' of 'torch._C._TensorBase' objects}\n","        4    0.000    0.000    0.000    0.000 {built-in method embedding}\n","        5    0.000    0.000    0.000    0.000 {method 'clone' of 'torch._C._TensorBase' objects}\n","       58    0.000    0.000    0.006    0.000 normalization.py:151(forward)\n","       71    0.000    0.000    0.005    0.000 linear.py:86(forward)\n","       58    0.000    0.000    0.006    0.000 functional.py:1689(layer_norm)\n","        1    0.000    0.000    0.246    0.246 QANet_xfmr_based.py:116(forward)\n","        1    0.000    0.000    0.000    0.000 layers.py:244(get_similarity_matrix)\n","        5    0.000    0.000    0.000    0.000 {built-in method zeros}\n","        4    0.000    0.000    0.000    0.000 {built-in method sigmoid}\n","        4    0.000    0.000    0.000    0.000 {method 'type' of 'torch._C._TensorBase' objects}\n","       95    0.000    0.000    0.000    0.000 _VF.py:11(__getattr__)\n","        2    0.000    0.000    0.000    0.000 {method 'log_softmax' of 'torch._C._TensorBase' objects}\n","      487    0.000    0.000    0.000    0.000 {method 'values' of 'collections.OrderedDict' objects}\n","  249/244    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n","        2    0.000    0.000    0.000    0.000 {built-in method max}\n","        2    0.000    0.000    0.003    0.001 QANet.py:200(forward)\n","        1    0.000    0.000    0.001    0.001 layers.py:226(forward)\n","       95    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n","        2    0.000    0.000    0.000    0.000 {built-in method zeros_like}\n","       31    0.000    0.000    0.001    0.000 functional.py:1202(softmax)\n","       58    0.000    0.000    0.000    0.000 __init__.py:31(__get__)\n","       33    0.000    0.000    0.001    0.000 functional.py:904(relu)\n","        5    0.000    0.000    0.000    0.000 {method 'detach' of 'torch._C._TensorBase' objects}\n","       58    0.000    0.000    0.000    0.000 {built-in method torch._C._get_cudnn_enabled}\n","      129    0.000    0.000    0.000    0.000 {method 'dim' of 'torch._C._TensorBase' objects}\n","        1    0.000    0.000    0.001    0.001 QANet.py:554(forward)\n","        2    0.000    0.000    0.000    0.000 {method 'expand' of 'torch._C._TensorBase' objects}\n","        9    0.000    0.000    0.000    0.000 container.py:157(__iter__)\n","       58    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n","        8    0.000    0.000    0.000    0.000 tensor.py:361(__rsub__)\n","        4    0.000    0.000    0.000    0.000 sparse.py:111(forward)\n","        2    0.000    0.000    0.001    0.000 conv.py:195(forward)\n","        2    0.000    0.000    0.000    0.000 functional.py:1297(log_softmax)\n","        4    0.000    0.000    0.000    0.000 functional.py:1406(embedding)\n","        5    0.000    0.000    0.000    0.000 container.py:154(__len__)\n","        9    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n","        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n","\n","\n","EPOCH: 0\n","AFTER LOSS.BACKWARD\n","memory used: 12.0009765625 (GiB) i.e. 75.481%\n","***************\n","EPOCH: 1\n","AFTER LOSS.BACKWARD\n","memory used: 12.0009765625 (GiB) i.e. 75.481%\n","***************\n","--------------------------------------------------------------------------------\n","  Environment Summary\n","--------------------------------------------------------------------------------\n","PyTorch 1.3.1+cu100 compiled w/ CUDA 10.0.130\n","Running with Python 3.6 and CUDA 10.1.243\n","\n","`pip3 list` truncated output:\n","numpy==1.18.5\n","torch==1.3.1+cu100\n","torchsummary==1.5.1\n","torchtext==0.3.1\n","torchvision==0.7.0+cu101\n","--------------------------------------------------------------------------------\n","  cProfile output\n","--------------------------------------------------------------------------------\n","         21439 function calls (20151 primitive calls) in 4.884 seconds\n","\n","   Ordered by: internal time\n","   List reduced from 313 to 15 due to restriction <15>\n","\n","   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n","      115    3.255    0.028    3.255    0.028 {method 'to' of 'torch._C._TensorBase' objects}\n","        1    1.535    1.535    1.535    1.535 {method 'enable' of '_lsprof.Profiler' objects}\n","        1    0.020    0.020    0.020    0.020 {built-in method rand}\n","      126    0.014    0.000    0.014    0.000 {method 'uniform_' of 'torch._C._TensorBase' objects}\n","     1524    0.014    0.000    0.015    0.000 /usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:587(__setattr__)\n","        4    0.006    0.001    0.006    0.001 {built-in method randint}\n","        2    0.004    0.002    0.010    0.005 ./src/QANet.py:395(__init__)\n","        6    0.003    0.000    0.003    0.000 {built-in method zeros}\n","       50    0.002    0.000    0.002    0.000 {built-in method posix.stat}\n","        1    0.002    0.002    0.002    0.002 {method 'normal_' of 'torch._C._TensorBase' objects}\n","        2    0.002    0.001    0.002    0.001 {built-in method sin}\n","        9    0.002    0.000    0.003    0.000 <frozen importlib._bootstrap_external>:830(get_data)\n","    112/1    0.002    0.000    3.256    3.256 /usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:200(_apply)\n","        3    0.001    0.000    0.002    0.001 {built-in method ujson.load}\n","        9    0.001    0.000    0.001    0.000 {built-in method marshal.loads}\n","\n","\n","--------------------------------------------------------------------------------\n","  autograd profiler output (CPU mode)\n","--------------------------------------------------------------------------------\n","        top 15 events sorted by cpu_time_total\n","\n","----------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------  \n","Name                          Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     CUDA total %     CUDA total       CUDA time avg    Number of Calls  Input Shapes                         \n","----------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------  \n","EmbeddingBackward             8.78%            137.234ms        8.78%            137.234ms        137.234ms        NaN              0.000us          0.000us          1                []                                   \n","embedding_backward            8.78%            137.230ms        8.78%            137.230ms        137.230ms        NaN              0.000us          0.000us          1                []                                   \n","embedding_dense_backward      8.78%            137.227ms        8.78%            137.227ms        137.227ms        NaN              0.000us          0.000us          1                []                                   \n","EmbeddingBackward             8.07%            126.160ms        8.07%            126.160ms        126.160ms        NaN              0.000us          0.000us          1                []                                   \n","embedding_backward            8.07%            126.155ms        8.07%            126.155ms        126.155ms        NaN              0.000us          0.000us          1                []                                   \n","embedding_dense_backward      8.07%            126.152ms        8.07%            126.152ms        126.152ms        NaN              0.000us          0.000us          1                []                                   \n","EmbeddingBackward             6.99%            109.256ms        6.99%            109.256ms        109.256ms        NaN              0.000us          0.000us          1                []                                   \n","embedding_backward            6.99%            109.252ms        6.99%            109.252ms        109.252ms        NaN              0.000us          0.000us          1                []                                   \n","embedding_dense_backward      6.99%            109.250ms        6.99%            109.250ms        109.250ms        NaN              0.000us          0.000us          1                []                                   \n","EmbeddingBackward             6.98%            109.083ms        6.98%            109.083ms        109.083ms        NaN              0.000us          0.000us          1                []                                   \n","embedding_backward            6.98%            109.079ms        6.98%            109.079ms        109.079ms        NaN              0.000us          0.000us          1                []                                   \n","embedding_dense_backward      6.98%            109.076ms        6.98%            109.076ms        109.076ms        NaN              0.000us          0.000us          1                []                                   \n","to                            2.63%            41.087ms         2.63%            41.087ms         41.087ms         NaN              0.000us          0.000us          1                []                                   \n","to                            2.48%            38.760ms         2.48%            38.760ms         38.760ms         NaN              0.000us          0.000us          1                []                                   \n","to                            2.42%            37.869ms         2.42%            37.869ms         37.869ms         NaN              0.000us          0.000us          1                []                                   \n","----------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------  \n","Self CPU time total: 1.563s\n","CUDA time total: 0.000us\n","\n","--------------------------------------------------------------------------------\n","  autograd profiler output (CUDA mode)\n","--------------------------------------------------------------------------------\n","        top 15 events sorted by cpu_time_total\n","\n","\tBecause the autograd profiler uses the CUDA event API,\n","\tthe CUDA time column reports approximately max(cuda_time, cpu_time).\n","\tPlease ignore this output if your code does not use CUDA.\n","\n","----------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------  \n","Name                          Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     CUDA total %     CUDA total       CUDA time avg    Number of Calls  Input Shapes                         \n","----------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------  \n","EmbeddingBackward             11.07%           109.663ms        11.07%           109.663ms        109.663ms        14.10%           4.875ms          4.875ms          1                []                                   \n","embedding_backward            11.07%           109.656ms        11.07%           109.656ms        109.656ms        14.08%           4.871ms          4.871ms          1                []                                   \n","embedding_dense_backward      11.07%           109.649ms        11.07%           109.649ms        109.649ms        14.07%           4.865ms          4.865ms          1                []                                   \n","EmbeddingBackward             11.06%           109.541ms        11.06%           109.541ms        109.541ms        14.12%           4.883ms          4.883ms          1                []                                   \n","embedding_backward            11.06%           109.533ms        11.06%           109.533ms        109.533ms        14.11%           4.879ms          4.879ms          1                []                                   \n","embedding_dense_backward      11.06%           109.526ms        11.06%           109.526ms        109.526ms        14.09%           4.875ms          4.875ms          1                []                                   \n","to                            4.09%            40.567ms         4.09%            40.567ms         40.567ms         0.12%            39.938us         39.938us         1                []                                   \n","to                            3.98%            39.379ms         3.98%            39.379ms         39.379ms         0.12%            40.812us         40.812us         1                []                                   \n","to                            3.87%            38.351ms         3.87%            38.351ms         38.351ms         0.10%            33.344us         33.344us         1                []                                   \n","EmbeddingBackward             3.63%            35.953ms         3.63%            35.953ms         35.953ms         2.53%            873.938us        873.938us        1                []                                   \n","embedding_backward            3.63%            35.945ms         3.63%            35.945ms         35.945ms         2.51%            869.750us        869.750us        1                []                                   \n","embedding_dense_backward      3.63%            35.936ms         3.63%            35.936ms         35.936ms         2.50%            865.312us        865.312us        1                []                                   \n","EmbeddingBackward             3.60%            35.658ms         3.60%            35.658ms         35.658ms         2.53%            875.375us        875.375us        1                []                                   \n","embedding_backward            3.60%            35.651ms         3.60%            35.651ms         35.651ms         2.52%            871.125us        871.125us        1                []                                   \n","embedding_dense_backward      3.60%            35.644ms         3.60%            35.644ms         35.644ms         2.51%            867.000us        867.000us        1                []                                   \n","----------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------  \n","Self CPU time total: 990.653ms\n","CUDA time total: 34.585ms\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6TdvCWH7ECJb"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aiR0StVieoSz"},"source":["##Playground"]},{"cell_type":"code","metadata":{"id":"EOeIhM12-tXg","executionInfo":{"elapsed":1049,"status":"ok","timestamp":1590537477297,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"ec07f111-1aa8-4a83-c1b6-8998492b7992","colab":{"base_uri":"https://localhost:8080/","height":201}},"source":["logits = torch.tensor([[1.0,2.0,3], [-1,2,3]])\n","mask = torch.tensor([[1,1,0],[1,0,0]])\n","res = util.masked_softmax(logits, mask, log_softmax=False)\n","print(res)\n","\n","\n","res = util.masked_softmax(logits, mask, log_softmax=True)\n","print(res)\n","\n","logits = torch.tensor([[1.0,2.0,-1e30], [-1,-1e30,-1e30]])\n","mask = torch.tensor([[1,1,0],[1,0,0]])\n","\n","res = torch.nn.functional.softmax(logits,-1)\n","print(res)\n","res = torch.nn.functional.log_softmax(logits,-1)\n","print(res)\n","print(res*mask)\n","res.sum(-1,keepdim=True)\n","res.dim()\n","torch.ones(2,3)\n","1e30 * 0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[0.2689, 0.7311, 0.0000],\n","        [1.0000, 0.0000, 0.0000]])\n","tensor([[-1.3133e+00, -3.1326e-01, -1.0000e+30],\n","        [ 0.0000e+00, -1.0000e+30, -1.0000e+30]])\n","tensor([[0.2689, 0.7311, 0.0000],\n","        [1.0000, 0.0000, 0.0000]])\n","tensor([[-1.3133e+00, -3.1326e-01, -1.0000e+30],\n","        [ 0.0000e+00, -1.0000e+30, -1.0000e+30]])\n","tensor([[-1.3133, -0.3133, -0.0000],\n","        [ 0.0000, -0.0000, -0.0000]])\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0.0"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"code","metadata":{"id":"hxxm5HEUMvhb","executionInfo":{"elapsed":1098,"status":"ok","timestamp":1590540219347,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"bb8e1e2d-0305-4341-8bd0-e42374269779","colab":{"base_uri":"https://localhost:8080/","height":150}},"source":["#************soft labelling debug (also check memory consumption)\n","\n","import time\n","\n","def soft_labelling_loss(log_p, y, cw_idxs):\n","    '''\n","    log_p shape (B,C)\n","    y shape (B,)\n","    \n","    for soft labelling, y_soft = (1-eps)*y + eps*1/C\n","    Use KLDiv loss as it is similar to the cross entropy loss within a constant\n","    References: \n","    1. https://towardsdatascience.com/label-smoothing-making-model-robust-to-incorrect-labels-2fae037ffbd0\n","    2. https://arxiv.org/pdf/1906.02629.pdf\n","    '''\n","\n","    eps = 0.2\n","    c_mask = torch.zeros_like(cw_idxs) != cw_idxs #(batch_size, max_c_len)\n","    c_len = c_mask.sum(dim=-1, keepdim=True) #(batch_size, 1)\n","\n","\n","    B,C = log_p.shape\n","    y_soft = torch.ones(B,C).cuda() #(B,C)\n","    y_soft = y_soft * eps / c_len #y=0 case\n","    y_soft[torch.arange(B), y] = ((1-eps) + torch.div(eps,c_len)).squeeze() #y=1 case\n","    y_soft *= c_mask #handle masking\n","    print('ysoft3', y_soft)\n","\n","    loss = torch.nn.functional.kl_div(log_p, y_soft, reduction='batchmean') #doesn't work. the loss explodes.\n","    print(loss)\n","    #dumy = y_soft.element_size() * y_soft.nelement() / 1e6\n","\n","    # loss = torch.nn.functional.nll_loss(log_p, y)\n","    # print(loss)\n","\n","    loss = (-y_soft * log_p).sum() / B\n","\n","    return loss\n","\n","\n","# B, C = 400,400\n","# logits = torch.rand(B,C).cuda()\n","# log_smax = torch.nn.functional.log_softmax(logits, dim=1)\n","# y = torch.randint(0,B,(B,)).cuda()\n","\n","logits = torch.tensor([[1.0,2.0,-1e30], [-1,-1e30,-1e30]]).cuda()\n","mask = torch.tensor([[1,1,0],[1,0,0]]).cuda()\n","# mask = torch.tensor([[1,1,1],[1,1,1]]).cuda() #for debug to disable masking\n","\n","y = torch.tensor([1,0]).cuda()\n","log_smax = torch.nn.functional.log_softmax(logits,dim=-1)\n","print(log_smax)\n","t1 = time.time()\n","nll = torch.nn.functional.nll_loss(log_smax, y)\n","print(time.time()-t1)\n","t1 = time.time()\n","myloss = soft_labelling_loss(log_smax, y, mask)\n","print(time.time()-t1)\n","\n","print(nll, myloss)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[-1.3133e+00, -3.1326e-01, -1.0000e+30],\n","        [ 0.0000e+00, -1.0000e+30, -1.0000e+30]], device='cuda:0')\n","0.00041604042053222656\n","ysoft3 tensor([[0.1000, 0.9000, 0.0000],\n","        [1.0000, 0.0000, 0.0000]], device='cuda:0')\n","tensor(0.0441, device='cuda:0')\n","0.0033414363861083984\n","tensor(0.1566, device='cuda:0') tensor(0.2066, device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"In_pO4QLM_S0","executionInfo":{"elapsed":384,"status":"ok","timestamp":1590449376118,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"5da66b43-48b3-4b8e-ea0b-c95378c23e76","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["N = int(1e1)\n","B, C = 400, 100\n","tot = torch.tensor(0.0).requires_grad_()\n","for _ in range(N):\n","    a = torch.rand(B,C)\n","    tot = tot + a.sum()\n","a.element_size() * a.nelement() / 1e6"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.16"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"LtdUTEREM_X0"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cFquc4gEXWhL","executionInfo":{"elapsed":479833,"status":"ok","timestamp":1590455779499,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"41c5c6e9-7a78-4e20-bf9f-e843655a8049","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import torch.nn as nn\n","import torch\n","\n","class testNet(nn.Module):\n","    def __init__(self):\n","        super(testNet, self).__init__()\n","        self.rnn = nn.RNN(input_size=200, hidden_size=1000, num_layers=1)\n","        self.linear = nn.Linear(1000, 100)\n","\n","    def forward(self, x, init):\n","        x = self.rnn(x, init)[0]\n","        y = self.linear(x.view(x.size(0)*x.size(1), x.size(2)))\n","        return y.view(x.size(0), x.size(1), y.size(1))\n","\n","net = testNet()\n","init = torch.zeros(1, 4, 1000)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n","\n","total_loss = 0.0\n","for i in range(250): #10000 mini-batch\n","    input = torch.randn(1000, 4, 200) #Seqlen = 1000, batch_size = 4, feature = 200\n","    target = torch.zeros((4, 1000), dtype=torch.long)\n","\n","    optimizer.zero_grad()\n","    output = net(input, init)\n","    loss = criterion(output.view(-1, output.size(2)), target.view(-1))\n","    loss.backward()\n","    optimizer.step()\n","    total_loss += loss\n","\n","print(total_loss)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor(48.2064, grad_fn=<AddBackward0>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"naJg33lPXWkF"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vJoC16YEXWnQ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eo33ucq7Mvmc"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1UcOEIE3XYrO"},"source":["##8. Testing (Evaluation)"]},{"cell_type":"code","metadata":{"id":"GYrGKpp2XfQ8","executionInfo":{"elapsed":26698,"status":"ok","timestamp":1598590498456,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"fd506af1-7104-49b2-995d-51635372c211","colab":{"base_uri":"https://localhost:8080/","height":828}},"source":["# import src.test as test #need to do this as there is a module named test in the std library\n","# importlib.reload(test)\n","\n","load_path = './save/train/train-With-Char-Embed_200hiddensize-01/best.pth.tar' #'./save/train/train-Baseline-01/best.pth.tar'\n","num_visuals = 20 #10 #number of examples to write on tensorboard\n","\n","!python ./src/test.py --split='dev' --load_path=$load_path --name='dev' \\\n","--num_visuals=$num_visuals --hidden_size=200 --model_type=BiDAF.BiDAF_with_char_embed\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[08.28.20 04:54:32] Args: {\n","    \"batch_size\": 64,\n","    \"char_emb_file\": \"./data/char_emb.json\",\n","    \"dev_eval_file\": \"./data/dev_eval.json\",\n","    \"dev_record_file\": \"./data/dev.npz\",\n","    \"hidden_size\": 200,\n","    \"load_path\": \"./save/train/train-With-Char-Embed_200hiddensize-01/best.pth.tar\",\n","    \"max_ans_len\": 15,\n","    \"model_type\": \"BiDAF.BiDAF_with_char_embed\",\n","    \"name\": \"dev\",\n","    \"num_visuals\": 20,\n","    \"num_workers\": 4,\n","    \"save_dir\": \"./save/test/dev-05\",\n","    \"split\": \"dev\",\n","    \"sub_file\": \"submission.csv\",\n","    \"test_eval_file\": \"./data/test_eval.json\",\n","    \"test_record_file\": \"./data/test.npz\",\n","    \"train_eval_file\": \"./data/train_eval.json\",\n","    \"train_record_file\": \"./data/train.npz\",\n","    \"use_squad_v2\": true,\n","    \"word_emb_file\": \"./data/word_emb.json\"\n","}\n","[08.28.20 04:54:32] Loading embeddings...\n","[08.28.20 04:54:38] Building model...\n","[08.28.20 04:54:42] Loading checkpoint from ./save/train/train-With-Char-Embed_200hiddensize-01/best.pth.tar...\n","[08.28.20 04:54:42] Building dataset...\n","[08.28.20 04:54:43] Evaluating on dev split...\n","100% 5951/5951 [00:08<00:00, 683.66it/s, NLL=2.65]\n","[08.28.20 04:54:53] Dev NLL: 02.65, F1: 64.11, EM: 60.98, AvNA: 69.82\n","2020-08-28 04:54:53.526898: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","[08.28.20 04:54:57] Writing submission file to ./save/test/dev-05/dev_submission.csv...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_2o77l8hoGlu"},"source":["%load_ext tensorboard\n","%tensorboard --logdir=./save/test/dev-01"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"waqG54Yir0XA"},"source":["##9. Inference"]},{"cell_type":"code","metadata":{"id":"Z4Cv_gwf9H2o"},"source":["from ujson import load as json_load\n","import util\n","# import BiDAF\n","import QANet_xfmr_based\n","\n","\n","word_emb_file = \"./data/word_emb.json\"\n","# load_path = \"./save/train/train-With-Char-Embed-01/best.pth.tar\"\n","load_path = \"./save/train/QANET_xfmr_datashuffle_lrwarmup_posenc_inside_encoder_0p05dropprob-06/best.pth.tar\"\n","hidden_size = 96 #100\n","record_file = './data/dev.npz'\n","eval_file = './data/dev_eval.json'\n","use_squad_v2 = True\n","max_ans_len = 15\n","have_labels = True\n","batch_size = 2 #64\n","device, gpu_ids = util.get_available_devices() #from util.py\n","batch_size *= max(1, len(gpu_ids))\n","\n","#get embeddings\n","word_vectors = util.torch_from_json(word_emb_file) #from util.py\n","\n","#load model\n","model = QANet_xfmr_based.QANet_xfmr(word_vectors=word_vectors, hidden_size=hidden_size) #from .py\n","model = torch.nn.DataParallel(model, gpu_ids)\n","model = util.load_model(model, load_path, gpu_ids, return_step=False) #from util.py\n","model = model.to(device)\n","model.eval()\n","\n","# Get data loader\n","dataset = util.SQuAD(record_file, use_squad_v2) #from util.py\n","data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False,num_workers=4, collate_fn=util.collate_fn)  #collate_fn from util.py\n","\n","\n","pred_dict = {}  # Predictions for viewing/printing\n","with open(eval_file, 'r') as fh:\n","    gold_dict = json_load(fh)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lj786PLHGadN","executionInfo":{"status":"ok","timestamp":1605222072837,"user_tz":480,"elapsed":818,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"}},"outputId":"2e501d9b-f04a-49d4-916a-aae4ff6898a2","colab":{"base_uri":"https://localhost:8080/"}},"source":["import inference\n","importlib.reload(inference)\n","\n","#Dataset\n","contexts = ['God is God and omnipresent , with no comparison . God is the Best .', 'Life without God is pointless . God is the key .']\n","questions = ['Who is God ?', 'who is most important ?']\n","# contexts = ['Alice is a student . Students are people . Alice is from Phoenix . Phoenix is a hot city . Cities are places . If it is snow then it is cold . If a person is from a hot place and it is cold, then she is not happy . Alice is happy .', \n","            # 'Alice is a student .']\n","# questions = ['Is it snow ?', 'Is Bob a student ?']\n","c_widxs, c_cidxs = inference.get_word_and_char_idxs(contexts)\n","q_widxs, q_cidxs = inference.get_word_and_char_idxs(questions)\n","y1s = torch.tensor([[2], [2]])\n","y2s = torch.tensor([[2], [3]])\n","ids = torch.tensor([0, 1])\n","dataset_inference = (c_widxs, c_cidxs, q_widxs, q_cidxs, y1s, y2s, ids)\n","# inference.SQuAD_inference(dataset_inference)[0]\n","\n","dataset_inference = inference.SQuAD_inference(dataset_inference)\n","data_loader_inference = torch.utils.data.DataLoader(dataset_inference, batch_size=1, shuffle=False,num_workers=1, collate_fn=util.collate_fn)\n","\n","# model_untrained = BiDAF(word_vectors=word_vectors, hidden_size=hidden_size) #from BiDAF.py\n","# model_untrained = nn.DataParallel(model_untrained, gpu_ids)\n","# model_untrained = model_untrained.to(device)\n","# model_untrained.eval()\n","\n","# model_untrained = BiDAF.BiDAF_with_char_embed(word_vectors=word_vectors, hidden_size=hidden_size) #from BiDAF.py\n","# model_untrained = torch.nn.DataParallel(model_untrained, gpu_ids)\n","# model_untrained = model_untrained.to(device)\n","# model_untrained.eval()\n","\n","print(inference.idx2word_dict[1])\n","print(dataset_inference[0][0].grad_fn)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--OOV--\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lO_iMks8ckGF"},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","class AddHooks(object):\n","    '''\n","    This is used to analyze the attention matrix\n","    '''\n","    def __init__(self, model):\n","        self.model = model\n","        self.hooks = {}\n","        self.sim_mtx = {}\n","\n","        def func(n):\n","            def f(m_,i_,o_):\n","                # print(n, '\\n', len(o_), o_[0].shape, o_[1].shape)\n","                self.sim_mtx[n].append(o_[1])\n","            return f\n","\n","        for n,m in model.named_modules():\n","            if 'mha' in n and 'mha.out_proj' not in n:\n","                # print(n)\n","                self.sim_mtx[n] = []\n","                self.hooks[n] = m.register_forward_hook(func(n))\n","    \n","    def remove_hooks(self):\n","        for k,v in self.hooks.items():\n","            v.remove()\n","        self.model = None\n","        self.hooks = None\n","        self.sim_mtx = None\n","    \n","\n","    def plot_heatmap(self, data, xlabels, ylabels, cmap=None):\n","        fig, ax = plt.subplots()\n","        im = ax.imshow(data, cmap=cmap)\n","\n","        # We want to show all ticks...\n","        ax.set_xticks(np.arange(len(xlabels)))\n","        ax.set_yticks(np.arange(len(ylabels)))\n","        # ... and label them with the respective list entries\n","        ax.set_xticklabels(xlabels)\n","        ax.set_yticklabels(ylabels)\n","\n","        # Rotate the tick labels and set their alignment.\n","        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n","                rotation_mode=\"anchor\")\n","\n","        # Loop over data dimensions and create text annotations.\n","        for i in range(len(ylabels)):\n","            for j in range(len(xlabels)):\n","                text = ax.text(j, i, round(data[i, j], 1), size=5,\n","                            ha=\"center\", va=\"center\", color=\"w\")\n","\n","\n","        # ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n","        # ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n","        # ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n","        # ax.tick_params(which=\"minor\", bottom=False, left=False)\n","\n","        ax.set_title(\"Attention Matrix\")\n","        # fig.tight_layout()\n","        # fig.colorbar(im, ax=ax)\n","        # plt.show()\n","        # plt.clf()\n","\n","\n","    def plot(self, xlabels, ylabels):\n","        for e,(k,v) in enumerate(self.sim_mtx.items()):\n","            if k == 'module.emb_enc.encoder_blocks_list.1.self_attention.mha':\n","                for i in range(len(v)):\n","                    # print(v[i][0].shape, len(xlabels[i]))\n","                    data = v[i][0].cpu().numpy()\n","                    self.plot_heatmap(data, xlabels[i], ylabels[i])\n","                    name = f'./save/atten_{e}_{i}.png'\n","                    plt.savefig(name, dpi=500)\n","                    plt.clf()\n","        self.remove_hooks()\n","\n","\n","h = AddHooks(model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_e_nm5XpW5u0","executionInfo":{"status":"ok","timestamp":1605222074039,"user_tz":480,"elapsed":948,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"}},"outputId":"1681e0d0-5f9e-458d-a6ef-7836784de232","colab":{"base_uri":"https://localhost:8080/"}},"source":["# inference.inference(model_untrained, dataset_inference, data_loader_inference)\n","inference.inference(model, dataset_inference, data_loader_inference, device, max_ans_len)\n","# inference.inference(model, dataset, data_loader)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 2/2 [00:00<00:00,  6.70it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Printing Inference\n","**Context:** God is God and omnipresent , with no comparison . God is the Best . \n","**Question:** Who is God ?\n","**GT Answer:** Ground truth answer not available\n","**Prediction:** God and omnipresent\n","\n","\n","Printing Inference\n","**Context:** Life without God is pointless . God is the key . \n","**Question:** who is most important ?\n","**GT Answer:** Ground truth answer not available\n","**Prediction:** N/A\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"hJx3OEVackAR","executionInfo":{"status":"ok","timestamp":1605222080129,"user_tz":480,"elapsed":5358,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"}},"outputId":"051d72f2-1b05-413b-9b99-dc7393983286","colab":{"base_uri":"https://localhost:8080/","height":90}},"source":["labels = zip(contexts, questions)\n","lbls = []; \n","for c,q in labels:\n","    tmp = [w for w in c.split(' ')]\n","    tmp = ['--OOV--'] + tmp\n","    lbls.append(tmp)\n","    tmp = [w for w in q.split(' ')]\n","    tmp = ['--OOV--'] + tmp\n","    lbls.append(tmp)\n","h.plot(lbls, lbls)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"KgoL65BiY6Xs"},"source":["%matplotlib inline\n","from IPython.display import display, Image\n","import glob\n","for f in glob.glob('./save/*.png'):\n","    print(f)\n","    display(Image(f))\n","# h.sim_mtx['module.emb_enc.encoder_blocks_list.0.self_attention.mha'][0].shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4_c3VyxAG3Xs"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Db4yZbe0IRv5"},"source":["##10. Hugging Face Transformers"]},{"cell_type":"markdown","metadata":{"id":"baFcZu2LI0yX"},"source":["1. https://pytorch.org/hub/huggingface_pytorch-transformers/\n","2. https://github.com/huggingface/pytorch-transformers.git\n","3. https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/huggingface_pytorch-transformers.ipynb"]},{"cell_type":"code","metadata":{"id":"IYrJLlkwIZ-m","executionInfo":{"elapsed":10552,"status":"ok","timestamp":1591055710584,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"5595fe0e-8587-4afc-8a68-672ad5832d1c","colab":{"base_uri":"https://localhost:8080/","height":572}},"source":["!pip install transformers\n","from transformers import *"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/b5/ac41e3e95205ebf53439e4dd087c58e9fd371fd8e3724f2b9b4cdb8282e5/transformers-2.10.0-py3-none-any.whl (660kB)\n","\u001b[K     |████████████████████████████████| 665kB 7.4MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 21.6MB/s \n","\u001b[?25hCollecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 45.7MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Collecting tokenizers==0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 46.1MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=ca8a846468f3748675bccc9d67da4dccea2c3ade10d03792cbf3c898a4cc097d\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.10.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"D2XNjYtC56In","executionInfo":{"elapsed":1630,"status":"ok","timestamp":1591056448718,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"71eab9a4-5158-40b0-994f-16d4f02f2347","colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["#Debug only for 'Tokenize the input'\n","tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased')\n","\n","text_1 = \"who was the person ?\"\n","text_2 = \"the person was me .\"\n","\n","indexed_tokens = tokenizer.encode_plus(text_1, add_special_tokens=False)\n","print(indexed_tokens)\n","\n","indexed_tokens = tokenizer.encode_plus(text_1, add_special_tokens=True)\n","print(indexed_tokens)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n"],"name":"stderr"},{"output_type":"stream","text":["{'input_ids': [1150, 1108, 1103, 1825, 136], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n","{'input_ids': [101, 1150, 1108, 1103, 1825, 136, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ecwYm1RsWjIr","executionInfo":{"elapsed":1660,"status":"ok","timestamp":1591058717799,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"62da1351-898c-46a8-eed4-c92d13a63a31","colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["# 1. Tokenize the input\n","import torch\n","tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased')\n","\n","text_1 = \"Who was Jim Henson ?\"\n","text_2 = \"Jim Henson was a puppeteer\" #words that are not in vocabulary seem to be plit into words that are (i.e. henson and puppeteer are split into 2 subwords)\n","\n","# Tokenized input with special tokens around it (for BERT: [CLS] at the beginning and [SEP] at the end and between sentences)\n","indexed_tokens = tokenizer.encode(text_1, text_2, add_special_tokens=True)\n","print(indexed_tokens, len(indexed_tokens))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n"],"name":"stderr"},{"output_type":"stream","text":["[101, 2627, 1108, 3104, 1124, 15703, 136, 102, 3104, 1124, 15703, 1108, 170, 16797, 8284, 102] 16\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LzPU0wh05--k","executionInfo":{"elapsed":4668,"status":"ok","timestamp":1591058723224,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"65668996-8bae-49e1-dbe0-b2fe48ac7da6","colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["# 2. USING BERTMODEL TO ENCODE THE INPUT SENTENCE INTO A SEQUENCE OF LAST LAYER HIDDEN-STATES\n","\n","# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n","segments_ids = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n","\n","# Convert inputs to PyTorch tensors\n","segments_tensors = torch.tensor([segments_ids])\n","tokens_tensor = torch.tensor([indexed_tokens])\n","\n","model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')\n","\n","with torch.no_grad():\n","    encoded_layers, _ = model(tokens_tensor, token_type_ids=segments_tensors)\n","encoded_layers.shape #the last dim is the size of hidden dim (or vocab size)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 16, 768])"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"code","metadata":{"id":"78er4aS7W4TY","executionInfo":{"elapsed":5722,"status":"ok","timestamp":1591059053655,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"02568937-5d48-4633-e648-e4b2043c0669","colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["# 3. USING MODELWITHLMHEAD TO PREDICT A MASKED TOKEN WITH BERT\n","# Mask a token that we will try to predict back with `BertForMaskedLM`\n","masked_index = 8\n","indexed_tokens[masked_index] = tokenizer.mask_token_id\n","tokens_tensor = torch.tensor([indexed_tokens])\n","\n","masked_lm__model = torch.hub.load('huggingface/pytorch-transformers', 'modelWithLMHead', 'bert-base-cased')\n","\n","with torch.no_grad():\n","    predictions = masked_lm__model(tokens_tensor, token_type_ids=segments_tensors)\n","\n","# Get the predicted token\n","predicted_index = torch.argmax(predictions[0][0], dim=1)[masked_index].item()\n","predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n","assert predicted_token == 'Jim'\n","print(f'A: {torch.argmax(predictions[0], dim=2)}, B: {predictions[0].shape}, C: {len(tokenizer.vocab)}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n"],"name":"stderr"},{"output_type":"stream","text":["A: tensor([[  119,  2627,  1108,  3104,  1124, 15703,   136,   119,  3104,  1124,\n","         15703,  1108,   170, 16797,  8284,   119]]), B: torch.Size([1, 16, 28996]), C: 28996\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X_m4xRUhbxL6","executionInfo":{"elapsed":12616,"status":"ok","timestamp":1591062516614,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"2b650f7f-b754-43e1-aa4a-5936611031a9","colab":{"base_uri":"https://localhost:8080/","height":117}},"source":["# 4. USING MODELFORQUESTIONANSWERING TO DO QUESTION ANSWERING WITH BERT\n","\n","question_answering_model = torch.hub.load('huggingface/pytorch-transformers', 'modelForQuestionAnswering', 'bert-large-uncased-whole-word-masking-finetuned-squad')\n","question_answering_tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-large-uncased-whole-word-masking-finetuned-squad')\n","\n","# The format is paragraph first and then question\n","text_1 = \"Jim Henson was a puppeteer\" #puppeteer is separated into two separate words\n","text_2 = \"Who was Jim Henson ?\"\n","indexed_tokens = question_answering_tokenizer.encode(text_1, text_2, add_special_tokens=True)\n","print(indexed_tokens, len(indexed_tokens))\n","segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n","segments_tensors = torch.tensor([segments_ids])\n","tokens_tensor = torch.tensor([indexed_tokens])\n","\n","# Predict the start and end positions logits\n","with torch.no_grad():\n","    start_logits, end_logits = question_answering_model(tokens_tensor, token_type_ids=segments_tensors)\n","\n","print(f'start_logits_shape: {start_logits.shape}, vocab: {len(question_answering_tokenizer.vocab)}')\n","# get the highest prediction\n","answer = question_answering_tokenizer.decode(indexed_tokens[torch.argmax(start_logits):torch.argmax(end_logits)+1])\n","assert answer == \"puppeteer\"\n","\n","print(f'Positions {torch.argmax(start_logits)} and {torch.argmax(end_logits)+1}')\n","# Or get the total loss which is the sum of the CrossEntropy loss for the start and end token positions (set model to train mode before if used for training)\n","start_positions, end_positions = torch.tensor([12]), torch.tensor([14])\n","multiple_choice_loss = question_answering_model(tokens_tensor, token_type_ids=segments_tensors, start_positions=start_positions, end_positions=end_positions)\n","\n","question_answering_tokenizer.decode(13997) #puppet\n","question_answering_tokenizer.decode(11510) #eer"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n","Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n"],"name":"stderr"},{"output_type":"stream","text":["[101, 3958, 27227, 2001, 1037, 13997, 11510, 102, 2040, 2001, 3958, 27227, 1029, 102] 14\n","start_logits_shape: torch.Size([1, 14]), vocab: 30522\n","Positions 5 and 7\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'# # e e r'"]},"metadata":{"tags":[]},"execution_count":81}]},{"cell_type":"code","metadata":{"id":"pQhpAcXXbxO1","executionInfo":{"elapsed":15619,"status":"ok","timestamp":1591062588203,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"d880ea57-df0d-47f4-bbcc-833be963068a","colab":{"base_uri":"https://localhost:8080/","height":213,"referenced_widgets":["2919f96bf4734c79a1522d2dc28b462f","daf229294e834f2fbf193dd1ed7a109c","b7f19eb83f064f1b9bc3ca9f11a21233"]}},"source":["# 5. USING MODELFORSEQUENCECLASSIFICATION TO DO PARAPHRASE CLASSIFICATION WITH BERT\n","\n","sequence_classification_model = torch.hub.load('huggingface/pytorch-transformers', 'modelForSequenceClassification', 'bert-base-cased-finetuned-mrpc')\n","sequence_classification_tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased-finetuned-mrpc')\n","\n","text_1 = \"Jim Henson was a puppeteer\"\n","text_2 = \"Who was Jim Henson ?\"\n","indexed_tokens = sequence_classification_tokenizer.encode(text_1, text_2, add_special_tokens=True)\n","segments_ids = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n","segments_tensors = torch.tensor([segments_ids])\n","tokens_tensor = torch.tensor([indexed_tokens])\n","\n","# Predict the sequence classification logits\n","with torch.no_grad():\n","    seq_classif_logits = sequence_classification_model(tokens_tensor, token_type_ids=segments_tensors)\n","\n","print(seq_classif_logits)\n","predicted_labels = torch.argmax(seq_classif_logits[0]).item()\n","\n","assert predicted_labels == 0  # In MRPC dataset this means the two sentences are not paraphrasing each other\n","\n","# Or get the sequence classification loss (set model to train mode before if used for training)\n","labels = torch.tensor([1])\n","seq_classif_loss = sequence_classification_model(tokens_tensor, token_type_ids=segments_tensors, labels=labels)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2919f96bf4734c79a1522d2dc28b462f","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"daf229294e834f2fbf193dd1ed7a109c","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433297515.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b7f19eb83f064f1b9bc3ca9f11a21233","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","(tensor([[ 0.9574, -0.2855]]),)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-AuRbrBywiUe","executionInfo":{"elapsed":2200,"status":"ok","timestamp":1591063234490,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":420},"outputId":"addbea68-9ee3-4385-8b8c-ba911f7448c8","colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["!pwd"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uWmHd7QnvOoj"},"source":["# 6. Huggingface-Transformers Quick Tour (without using torch.hub.load(...))\n","\n","import torch\n","from transformers import *\n","\n","# Transformers has a unified API\n","# for 10 transformer architectures and 30 pretrained weights.\n","#          Model          | Tokenizer          | Pretrained weights shortcut\n","MODELS = [(BertModel,       BertTokenizer,       'bert-base-uncased'),\n","          (OpenAIGPTModel,  OpenAIGPTTokenizer,  'openai-gpt'),\n","          (GPT2Model,       GPT2Tokenizer,       'gpt2'),\n","          (CTRLModel,       CTRLTokenizer,       'ctrl'),\n","          (TransfoXLModel,  TransfoXLTokenizer,  'transfo-xl-wt103'),\n","          (XLNetModel,      XLNetTokenizer,      'xlnet-base-cased'),\n","          (XLMModel,        XLMTokenizer,        'xlm-mlm-enfr-1024'),\n","          (DistilBertModel, DistilBertTokenizer, 'distilbert-base-cased'),\n","          (RobertaModel,    RobertaTokenizer,    'roberta-base'),\n","          (XLMRobertaModel, XLMRobertaTokenizer, 'xlm-roberta-base'),\n","         ]\n","\n","# To use TensorFlow 2.0 versions of the models, simply prefix the class names with 'TF', e.g. `TFRobertaModel` is the TF 2.0 counterpart of the PyTorch model `RobertaModel`\n","\n","# Let's encode some text in a sequence of hidden-states using each model:\n","for model_class, tokenizer_class, pretrained_weights in MODELS:\n","    # Load pretrained model/tokenizer\n","    tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n","    model = model_class.from_pretrained(pretrained_weights)\n","\n","    # Encode text\n","    input_ids = torch.tensor([tokenizer.encode(\"Here is some text to encode\", add_special_tokens=True)])  # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n","    with torch.no_grad():\n","        last_hidden_states = model(input_ids)[0]  # Models outputs are now tuples\n","\n","# Each architecture is provided with several class for fine-tuning on down-stream tasks, e.g.\n","BERT_MODEL_CLASSES = [BertModel, BertForPreTraining, BertForMaskedLM, BertForNextSentencePrediction,\n","                      BertForSequenceClassification, BertForTokenClassification, BertForQuestionAnswering]\n","\n","# All the classes for an architecture can be initiated from pretrained weights for this architecture\n","# Note that additional weights added for fine-tuning are only initialized\n","# and need to be trained on the down-stream task\n","pretrained_weights = 'bert-base-uncased'\n","tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n","for model_class in BERT_MODEL_CLASSES:\n","    # Load pretrained model/tokenizer\n","    model = model_class.from_pretrained(pretrained_weights)\n","\n","    # Models can return full list of hidden-states & attentions weights at each layer\n","    model = model_class.from_pretrained(pretrained_weights,\n","                                        output_hidden_states=True,\n","                                        output_attentions=True)\n","    input_ids = torch.tensor([tokenizer.encode(\"Let's see all hidden-states and attentions on this text\")])\n","    all_hidden_states, all_attentions = model(input_ids)[-2:]\n","\n","    # Models are compatible with Torchscript\n","    model = model_class.from_pretrained(pretrained_weights, torchscript=True)\n","    traced_model = torch.jit.trace(model, (input_ids,))\n","\n","    # Simple serialization for models and tokenizers\n","    model.save_pretrained('./directory/to/save/')  # save\n","    model = model_class.from_pretrained('./directory/to/save/')  # re-load\n","    tokenizer.save_pretrained('./directory/to/save/')  # save\n","    tokenizer = BertTokenizer.from_pretrained('./directory/to/save/')  # re-load\n","\n","    # SOTA examples for GLUE, SQUAD, text generation..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WExcMjJ42e0d"},"source":["##Debugging Data Logger"]},{"cell_type":"code","metadata":{"id":"SYOQaS0yF9v5"},"source":["for handler in logging.root.handlers[:]:\n","    logging.root.removeHandler(handler)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7DIx4Kc48j5T"},"source":["logger = get_logger('./save/test/dev-01', 'dev')\n","logger.info(f'Args: xxxxxxxzzzzzzzzzzzzzzzzzzzzzzzz')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y3yElb0NDHT6","executionInfo":{"elapsed":4183,"status":"ok","timestamp":1579660840775,"user":{"displayName":"Amit Patel","photoUrl":"","userId":"14428842836414406555"},"user_tz":480},"outputId":"facf01bd-8b9e-489b-b3c7-fb886213d5c3","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["os.chdir('/content/gdrive/My Drive/Colab Notebooks/Deep_Learning/CS224N_2019/Project_Squad2/save/test/dev-01')\n","!ls -a\n","!cat log.txt\n","os.chdir('/content/gdrive/My Drive/Colab Notebooks/Deep_Learning/CS224N_2019/Project_Squad2')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dev_submission.csv  events.out.tfevents.1579658622.05b922c2c9fc.138.2\n","cat: log.txt: No such file or directory\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fACquozO52cO"},"source":["##Playground"]},{"cell_type":"code","metadata":{"id":"AfxSHvhU0jaF"},"source":["# import csv\n","# !pwd\n","\n","# sub_path = './save/zzzz.csv'\n","# with open(sub_path, 'w', newline='', encoding='utf-8') as csv_fh:\n","#     csv_writer = csv.writer(csv_fh, delimiter=',')\n","#     csv_writer.writerow(['Id', 'Predicted'])\n","#     csv_writer.writerow(['Id111', 'Predicted1111'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NJtPkll1AERv"},"source":["# import tqdm\n","# import time\n","# for i in tqdm.tqdm(range(3)):\n","#     for j in tqdm.tqdm_notebook(range(5)):\n","#         time.sleep(0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5KYghSUg5urR"},"source":["#create data directory in colab virtual machine\n","# def copy_json_data_files_from_gdrive_to_colabGCP():\n","#     os.chdir('/content')\n","#     !mkdir 'data'\n","\n","#     !cp '/content/gdrive/My Drive/Colab Notebooks/Deep_Learning/CS224N_2019/Project_Squad2/data/train-v2.0.json' '/content/data/train-v2.0.json'\n","#     !cp '/content/gdrive/My Drive/Colab Notebooks/Deep_Learning/CS224N_2019/Project_Squad2/data/test-v2.0.json' '/content/data/test-v2.0.json'\n","#     !cp '/content/gdrive/My Drive/Colab Notebooks/Deep_Learning/CS224N_2019/Project_Squad2/data/dev-v2.0.json' '/content/data/dev-v2.0.json'\n","\n","#     os.chdir('/content/data')\n","#     !ls\n","# copy_json_data_files_from_gdrive_to_colabGCP()\n","\n","\n","# #check all the data files\n","# os.chdir('/content/data')\n","# !ls -l --block-size=M\n","\n","# import shutil\n","\n","# def copy_important_data_files_from_colabGCP_to_gdrive():\n","#     #copy important data files to gdrive\n","#     list_of_files = ['char2idx.json', 'char_emb.json', 'dev_eval.json', 'dev_meta.json', 'dev.npz', 'test_eval.json', 'test_meta.json', 'test.npz',\n","#                      'train_eval.json', 'train.npz', 'word2idx.json', 'word_emb.json']\n","#     for fname in list_of_files:\n","#         src = '/content/data/' + fname\n","#         dest = '/content/gdrive/My Drive/Colab Notebooks/Deep_Learning/CS224N_2019/Project_Squad2/data/' + fname\n","#         shutil.copyfile(src, dest)\n","#         #!cp src dest\n","\n","# # copy_important_data_files_from_colabGCP_to_gdrive()\n","\n","# #shutil.rmtree('/content/data') #remove non-empty directory (to save disk space)\n"],"execution_count":null,"outputs":[]}]}